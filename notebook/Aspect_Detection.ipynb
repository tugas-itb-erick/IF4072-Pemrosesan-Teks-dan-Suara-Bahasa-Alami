{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>physics</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't get why negroes always traveling to wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmao how funny that true does know where the c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@art_is_forever when did she publicly thank him?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Post a picture of Khloe already!!!!! Come on!!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@progreenlc no we don't. When did he become th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  physics  race  religion\n",
       "0  I don't get why negroes always traveling to wh...        0     1         0\n",
       "1  lmao how funny that true does know where the c...        0     0         0\n",
       "2   @art_is_forever when did she publicly thank him?        0     0         0\n",
       "3   Post a picture of Khloe already!!!!! Come on!!!!        0     0         0\n",
       "4  @progreenlc no we don't. When did he become th...        0     0         0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "data_physics = pd.read_csv('ad_physics.csv',index_col=0)\n",
    "data_race = pd.read_csv('ad_race.csv',index_col=0)\n",
    "data_religion = pd.read_csv('ad_religion.csv',index_col=0)\n",
    "data_physics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, nltk, string\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def expand_contractions(text) :\n",
    "    pattern = re.compile(\"({})\".format(\"|\".join(CONTRACTION_MAP.keys())),flags = re.DOTALL| re.IGNORECASE)\n",
    "    \n",
    "    def replace_text(t):\n",
    "        txt = t.group(0)\n",
    "        if txt.lower() in CONTRACTION_MAP.keys():\n",
    "            return CONTRACTION_MAP[txt.lower()]\n",
    "        \n",
    "    expand_text = pattern.sub(replace_text,text)\n",
    "    return expand_text\n",
    "\n",
    "def remove_repeated_characters(word):\n",
    "    pattern = re.compile(r\"(\\w*)(\\w)\\2(\\w*)\")\n",
    "    substitution_pattern = r\"\\1\\2\\3\"\n",
    "    while True:\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        new_word = pattern.sub(substitution_pattern,word)\n",
    "        if new_word != word:\n",
    "            word = new_word\n",
    "            continue\n",
    "        else:\n",
    "            return new_word\n",
    "\n",
    "def spelling_checker(word):\n",
    "    checker = suggest(word)\n",
    "    return checker[0][0]\n",
    "\n",
    "def cleanhtml(text):\n",
    "    cleanr = re.compile('&#[0-9]+;')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def clean_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "    \n",
    "def normalizer(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text.lower(), flags=re.MULTILINE) #remove url\n",
    "    text = re.sub('@[^\\s]+','',text) #remove username\n",
    "    text = clean_emoji(text)\n",
    "    text = cleanhtml(text)\n",
    "    expand = expand_contractions(text)\n",
    "    pattern = re.compile(\"[{}]\".format(re.escape(string.punctuation)))\n",
    "    filter_char =  filter(None,[pattern.sub('' ,expand)])\n",
    "    text_filter_char =  \" \".join(filter_char)\n",
    "    tokens = nltk.WhitespaceTokenizer().tokenize(text_filter_char)\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    stems = [stemmer.stem(t) for t in lemmas]\n",
    "    filtered_result = list(filter(lambda l: l not in stop_words, stems))\n",
    "    concate = ' '.join(filtered_result)\n",
    "    return concate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'peopl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    get whi negro alway travel white countri take ...\n",
       "1                  lmao funni true doe know camara lol\n",
       "2                                       publicli thank\n",
       "3                       post pictur khloe alreadi come\n",
       "4    becom poster child faith partner pleas got caught\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_physics.sentence = data_physics.sentence.apply(normalizer)\n",
    "data_race.sentence = data_race.sentence.apply(normalizer)\n",
    "data_religion.sentence = data_religion.sentence.apply(normalizer)\n",
    "data_physics.sentence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "\n",
    "x_train_physics, x_test_physics, y_train_physics, y_test_physics = train_test_split(data_physics.sentence, data_physics.physics, test_size=.2, random_state=SEED)\n",
    "x_train_race, x_test_race, y_train_race, y_test_race = train_test_split(data_race.sentence, data_race.race, test_size=.2, random_state=SEED)\n",
    "x_train_religion, x_test_religion, y_train_religion, y_test_religion = train_test_split(data_religion.sentence, data_religion.religion, test_size=.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def labelize_text(text,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(text.index, text):\n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "\n",
    "all_x = pd.concat([x_train_physics, x_test_physics])\n",
    "all_x_w2v = labelize_text(all_x, 'ALL')\n",
    "\n",
    "# physics\n",
    "x_train_physics = labelize_text(x_train_physics, 'TRAIN')\n",
    "x_test_physics = labelize_text(x_test_physics, 'TEST')\n",
    "\n",
    "# race\n",
    "x_train_race = labelize_text(x_train_race, 'TRAIN')\n",
    "x_test_race = labelize_text(x_test_race, 'TEST')\n",
    "\n",
    "# religion\n",
    "x_train_religion = labelize_text(x_train_religion, 'TRAIN')\n",
    "x_test_religion = labelize_text(x_test_religion, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4174/4174 [00:00<00:00, 1238733.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 4174/4174 [00:00<00:00, 475514.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8059, 26188)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "import numpy as np\n",
    "\n",
    "model_w2v = Word2Vec(size=200, min_count=20)\n",
    "model_w2v.build_vocab([x.words for x in tqdm(all_x_w2v)])\n",
    "model_w2v.train([x.words for x in tqdm(all_x_w2v)], total_examples=len(all_x_w2v), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('anoth', 0.25935670733451843),\n",
       " ('realli', 0.2524394392967224),\n",
       " ('would', 0.24629220366477966),\n",
       " ('hate', 0.244271919131279),\n",
       " ('girl', 0.23824137449264526),\n",
       " ('child', 0.22931347787380219),\n",
       " ('nice', 0.2237091064453125),\n",
       " ('dumb', 0.22180451452732086),\n",
       " ('littl', 0.22179366648197174),\n",
       " ('pretti', 0.21975255012512207)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.most_similar('cute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Document Vector using Average Word Vector With TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "matrix = vectorizer.fit_transform([x.words for x in all_x_w2v])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "def build_Word_Vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: \n",
    "            \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "3339it [00:00, 7624.17it/s]\n",
      "835it [00:00, 8386.48it/s]\n",
      "1040it [00:00, 9270.56it/s]\n",
      "260it [00:00, 6896.82it/s]\n",
      "688it [00:00, 5736.10it/s]\n",
      "173it [00:00, 6658.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# physics\n",
    "train_vecs_physics = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_train_physics))])\n",
    "test_vecs_physics = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_test_physics))])\n",
    "\n",
    "# race\n",
    "train_vecs_race = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_train_race))])\n",
    "test_vecs_race = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_test_race))])\n",
    "\n",
    "# religion\n",
    "train_vecs_religion = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_train_religion))])\n",
    "test_vecs_religion = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_test_religion))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM Aspect Detection Model for Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3339 samples, validate on 835 samples\n",
      "Epoch 1/100\n",
      "3339/3339 [==============================] - ETA: 4:20 - loss: 0.6931 - acc: 0.593 - ETA: 30s - loss: 0.6928 - acc: 0.535 - ETA: 13s - loss: 0.6928 - acc: 0.52 - ETA: 8s - loss: 0.6926 - acc: 0.5240 - ETA: 4s - loss: 0.6924 - acc: 0.524 - ETA: 3s - loss: 0.6922 - acc: 0.525 - ETA: 2s - loss: 0.6923 - acc: 0.518 - ETA: 1s - loss: 0.6923 - acc: 0.515 - ETA: 1s - loss: 0.6921 - acc: 0.516 - ETA: 0s - loss: 0.6920 - acc: 0.526 - ETA: 0s - loss: 0.6918 - acc: 0.538 - ETA: 0s - loss: 0.6917 - acc: 0.537 - 4s 1ms/step - loss: 0.6916 - acc: 0.5370 - val_loss: 0.6895 - val_acc: 0.6539\n",
      "Epoch 2/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.6875 - acc: 0.750 - ETA: 0s - loss: 0.6885 - acc: 0.664 - ETA: 0s - loss: 0.6889 - acc: 0.639 - ETA: 0s - loss: 0.6891 - acc: 0.613 - ETA: 0s - loss: 0.6890 - acc: 0.608 - ETA: 0s - loss: 0.6888 - acc: 0.615 - ETA: 0s - loss: 0.6884 - acc: 0.631 - ETA: 0s - loss: 0.6883 - acc: 0.641 - ETA: 0s - loss: 0.6883 - acc: 0.640 - ETA: 0s - loss: 0.6880 - acc: 0.649 - ETA: 0s - loss: 0.6877 - acc: 0.653 - ETA: 0s - loss: 0.6874 - acc: 0.662 - ETA: 0s - loss: 0.6871 - acc: 0.668 - 1s 212us/step - loss: 0.6870 - acc: 0.6682 - val_loss: 0.6823 - val_acc: 0.7485\n",
      "Epoch 3/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.6867 - acc: 0.625 - ETA: 0s - loss: 0.6833 - acc: 0.715 - ETA: 0s - loss: 0.6812 - acc: 0.716 - ETA: 0s - loss: 0.6813 - acc: 0.691 - ETA: 0s - loss: 0.6808 - acc: 0.686 - ETA: 0s - loss: 0.6808 - acc: 0.679 - ETA: 0s - loss: 0.6802 - acc: 0.677 - ETA: 0s - loss: 0.6796 - acc: 0.678 - ETA: 0s - loss: 0.6794 - acc: 0.678 - ETA: 0s - loss: 0.6794 - acc: 0.674 - ETA: 0s - loss: 0.6794 - acc: 0.673 - ETA: 0s - loss: 0.6789 - acc: 0.675 - ETA: 0s - loss: 0.6787 - acc: 0.676 - ETA: 0s - loss: 0.6784 - acc: 0.677 - ETA: 0s - loss: 0.6782 - acc: 0.678 - ETA: 0s - loss: 0.6777 - acc: 0.682 - ETA: 0s - loss: 0.6770 - acc: 0.689 - 1s 281us/step - loss: 0.6770 - acc: 0.6897 - val_loss: 0.6685 - val_acc: 0.7545\n",
      "Epoch 4/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.6713 - acc: 0.656 - ETA: 0s - loss: 0.6693 - acc: 0.734 - ETA: 0s - loss: 0.6703 - acc: 0.721 - ETA: 0s - loss: 0.6689 - acc: 0.725 - ETA: 0s - loss: 0.6670 - acc: 0.735 - ETA: 0s - loss: 0.6681 - acc: 0.725 - ETA: 0s - loss: 0.6667 - acc: 0.735 - ETA: 0s - loss: 0.6663 - acc: 0.742 - ETA: 0s - loss: 0.6651 - acc: 0.750 - ETA: 0s - loss: 0.6648 - acc: 0.746 - ETA: 0s - loss: 0.6641 - acc: 0.748 - ETA: 0s - loss: 0.6638 - acc: 0.748 - ETA: 0s - loss: 0.6637 - acc: 0.743 - ETA: 0s - loss: 0.6631 - acc: 0.743 - ETA: 0s - loss: 0.6622 - acc: 0.748 - ETA: 0s - loss: 0.6614 - acc: 0.749 - ETA: 0s - loss: 0.6606 - acc: 0.746 - ETA: 0s - loss: 0.6598 - acc: 0.747 - 1s 297us/step - loss: 0.6597 - acc: 0.7478 - val_loss: 0.6464 - val_acc: 0.7341\n",
      "Epoch 5/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.6439 - acc: 0.718 - ETA: 0s - loss: 0.6455 - acc: 0.767 - ETA: 0s - loss: 0.6453 - acc: 0.755 - ETA: 0s - loss: 0.6434 - acc: 0.754 - ETA: 0s - loss: 0.6434 - acc: 0.746 - ETA: 0s - loss: 0.6434 - acc: 0.747 - ETA: 0s - loss: 0.6448 - acc: 0.736 - ETA: 0s - loss: 0.6432 - acc: 0.742 - ETA: 0s - loss: 0.6418 - acc: 0.736 - ETA: 0s - loss: 0.6407 - acc: 0.735 - ETA: 0s - loss: 0.6408 - acc: 0.730 - ETA: 0s - loss: 0.6407 - acc: 0.725 - ETA: 0s - loss: 0.6393 - acc: 0.727 - ETA: 0s - loss: 0.6397 - acc: 0.725 - ETA: 0s - loss: 0.6389 - acc: 0.726 - ETA: 0s - loss: 0.6378 - acc: 0.728 - ETA: 0s - loss: 0.6369 - acc: 0.728 - ETA: 0s - loss: 0.6355 - acc: 0.730 - 1s 296us/step - loss: 0.6353 - acc: 0.7302 - val_loss: 0.6171 - val_acc: 0.7629\n",
      "Epoch 6/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.6602 - acc: 0.625 - ETA: 0s - loss: 0.6219 - acc: 0.738 - ETA: 0s - loss: 0.6230 - acc: 0.718 - ETA: 0s - loss: 0.6225 - acc: 0.724 - ETA: 0s - loss: 0.6200 - acc: 0.736 - ETA: 0s - loss: 0.6161 - acc: 0.754 - ETA: 0s - loss: 0.6144 - acc: 0.757 - ETA: 0s - loss: 0.6132 - acc: 0.761 - ETA: 0s - loss: 0.6100 - acc: 0.770 - ETA: 0s - loss: 0.6102 - acc: 0.768 - ETA: 0s - loss: 0.6106 - acc: 0.764 - ETA: 0s - loss: 0.6091 - acc: 0.765 - ETA: 0s - loss: 0.6077 - acc: 0.766 - ETA: 0s - loss: 0.6065 - acc: 0.767 - ETA: 0s - loss: 0.6061 - acc: 0.765 - ETA: 0s - loss: 0.6069 - acc: 0.760 - ETA: 0s - loss: 0.6063 - acc: 0.759 - 1s 299us/step - loss: 0.6066 - acc: 0.7586 - val_loss: 0.5851 - val_acc: 0.7533\n",
      "Epoch 7/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.6250 - acc: 0.656 - ETA: 0s - loss: 0.5898 - acc: 0.722 - ETA: 0s - loss: 0.5938 - acc: 0.722 - ETA: 0s - loss: 0.5960 - acc: 0.731 - ETA: 0s - loss: 0.5926 - acc: 0.740 - ETA: 0s - loss: 0.5885 - acc: 0.753 - ETA: 0s - loss: 0.5889 - acc: 0.751 - ETA: 0s - loss: 0.5902 - acc: 0.739 - ETA: 0s - loss: 0.5899 - acc: 0.740 - ETA: 0s - loss: 0.5912 - acc: 0.740 - ETA: 0s - loss: 0.5887 - acc: 0.742 - ETA: 0s - loss: 0.5860 - acc: 0.747 - ETA: 0s - loss: 0.5835 - acc: 0.753 - ETA: 0s - loss: 0.5815 - acc: 0.757 - ETA: 0s - loss: 0.5810 - acc: 0.760 - ETA: 0s - loss: 0.5802 - acc: 0.760 - ETA: 0s - loss: 0.5785 - acc: 0.764 - ETA: 0s - loss: 0.5767 - acc: 0.767 - 1s 300us/step - loss: 0.5763 - acc: 0.7679 - val_loss: 0.5531 - val_acc: 0.7725\n",
      "Epoch 8/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.5684 - acc: 0.781 - ETA: 0s - loss: 0.5489 - acc: 0.817 - ETA: 0s - loss: 0.5555 - acc: 0.794 - ETA: 0s - loss: 0.5487 - acc: 0.809 - ETA: 0s - loss: 0.5526 - acc: 0.795 - ETA: 0s - loss: 0.5496 - acc: 0.798 - ETA: 0s - loss: 0.5503 - acc: 0.792 - ETA: 0s - loss: 0.5455 - acc: 0.792 - ETA: 0s - loss: 0.5446 - acc: 0.792 - ETA: 0s - loss: 0.5482 - acc: 0.785 - ETA: 0s - loss: 0.5488 - acc: 0.784 - ETA: 0s - loss: 0.5513 - acc: 0.778 - ETA: 0s - loss: 0.5514 - acc: 0.778 - ETA: 0s - loss: 0.5510 - acc: 0.779 - ETA: 0s - loss: 0.5517 - acc: 0.776 - ETA: 0s - loss: 0.5508 - acc: 0.775 - 1s 280us/step - loss: 0.5489 - acc: 0.7757 - val_loss: 0.5257 - val_acc: 0.7772\n",
      "Epoch 9/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.5797 - acc: 0.812 - ETA: 0s - loss: 0.5310 - acc: 0.793 - ETA: 0s - loss: 0.5282 - acc: 0.789 - ETA: 0s - loss: 0.5307 - acc: 0.779 - ETA: 0s - loss: 0.5347 - acc: 0.777 - ETA: 0s - loss: 0.5374 - acc: 0.773 - ETA: 0s - loss: 0.5350 - acc: 0.780 - ETA: 0s - loss: 0.5322 - acc: 0.783 - ETA: 0s - loss: 0.5293 - acc: 0.785 - ETA: 0s - loss: 0.5282 - acc: 0.784 - ETA: 0s - loss: 0.5300 - acc: 0.781 - ETA: 0s - loss: 0.5302 - acc: 0.781 - ETA: 0s - loss: 0.5307 - acc: 0.781 - ETA: 0s - loss: 0.5280 - acc: 0.786 - ETA: 0s - loss: 0.5296 - acc: 0.785 - ETA: 0s - loss: 0.5279 - acc: 0.787 - 1s 271us/step - loss: 0.5272 - acc: 0.7877 - val_loss: 0.5028 - val_acc: 0.7976\n",
      "Epoch 10/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.5661 - acc: 0.750 - ETA: 0s - loss: 0.5289 - acc: 0.781 - ETA: 0s - loss: 0.5196 - acc: 0.783 - ETA: 0s - loss: 0.5183 - acc: 0.793 - ETA: 0s - loss: 0.5189 - acc: 0.796 - ETA: 0s - loss: 0.5239 - acc: 0.791 - ETA: 0s - loss: 0.5139 - acc: 0.798 - ETA: 0s - loss: 0.5098 - acc: 0.801 - ETA: 0s - loss: 0.5116 - acc: 0.796 - ETA: 0s - loss: 0.5114 - acc: 0.798 - ETA: 0s - loss: 0.5151 - acc: 0.795 - ETA: 0s - loss: 0.5160 - acc: 0.794 - ETA: 0s - loss: 0.5123 - acc: 0.798 - ETA: 0s - loss: 0.5113 - acc: 0.797 - 1s 238us/step - loss: 0.5089 - acc: 0.7969 - val_loss: 0.4838 - val_acc: 0.7988\n",
      "Epoch 11/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4574 - acc: 0.812 - ETA: 0s - loss: 0.5123 - acc: 0.812 - ETA: 0s - loss: 0.4998 - acc: 0.824 - ETA: 0s - loss: 0.4856 - acc: 0.832 - ETA: 0s - loss: 0.5012 - acc: 0.816 - ETA: 0s - loss: 0.5019 - acc: 0.817 - ETA: 0s - loss: 0.5018 - acc: 0.814 - ETA: 0s - loss: 0.5037 - acc: 0.809 - ETA: 0s - loss: 0.5028 - acc: 0.811 - ETA: 0s - loss: 0.5003 - acc: 0.813 - ETA: 0s - loss: 0.4988 - acc: 0.811 - ETA: 0s - loss: 0.4981 - acc: 0.810 - ETA: 0s - loss: 0.4971 - acc: 0.808 - ETA: 0s - loss: 0.4915 - acc: 0.810 - ETA: 0s - loss: 0.4927 - acc: 0.805 - ETA: 0s - loss: 0.4922 - acc: 0.806 - 1s 270us/step - loss: 0.4917 - acc: 0.8074 - val_loss: 0.4678 - val_acc: 0.8024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4663 - acc: 0.812 - ETA: 0s - loss: 0.4892 - acc: 0.784 - ETA: 0s - loss: 0.4924 - acc: 0.791 - ETA: 0s - loss: 0.4742 - acc: 0.807 - ETA: 0s - loss: 0.4798 - acc: 0.808 - ETA: 0s - loss: 0.4756 - acc: 0.812 - ETA: 0s - loss: 0.4743 - acc: 0.816 - ETA: 0s - loss: 0.4731 - acc: 0.817 - ETA: 0s - loss: 0.4791 - acc: 0.813 - ETA: 0s - loss: 0.4804 - acc: 0.814 - ETA: 0s - loss: 0.4791 - acc: 0.815 - ETA: 0s - loss: 0.4804 - acc: 0.812 - ETA: 0s - loss: 0.4788 - acc: 0.811 - ETA: 0s - loss: 0.4779 - acc: 0.810 - ETA: 0s - loss: 0.4806 - acc: 0.808 - ETA: 0s - loss: 0.4807 - acc: 0.805 - ETA: 0s - loss: 0.4820 - acc: 0.804 - 1s 287us/step - loss: 0.4809 - acc: 0.8056 - val_loss: 0.4552 - val_acc: 0.8048\n",
      "Epoch 13/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.6229 - acc: 0.781 - ETA: 0s - loss: 0.4770 - acc: 0.808 - ETA: 0s - loss: 0.4806 - acc: 0.808 - ETA: 0s - loss: 0.4749 - acc: 0.815 - ETA: 0s - loss: 0.4718 - acc: 0.818 - ETA: 0s - loss: 0.4699 - acc: 0.823 - ETA: 0s - loss: 0.4689 - acc: 0.823 - ETA: 0s - loss: 0.4695 - acc: 0.822 - ETA: 0s - loss: 0.4654 - acc: 0.821 - ETA: 0s - loss: 0.4691 - acc: 0.819 - ETA: 0s - loss: 0.4711 - acc: 0.818 - ETA: 0s - loss: 0.4672 - acc: 0.818 - ETA: 0s - loss: 0.4652 - acc: 0.819 - ETA: 0s - loss: 0.4657 - acc: 0.817 - ETA: 0s - loss: 0.4683 - acc: 0.817 - ETA: 0s - loss: 0.4696 - acc: 0.817 - ETA: 0s - loss: 0.4712 - acc: 0.815 - 1s 292us/step - loss: 0.4705 - acc: 0.8158 - val_loss: 0.4440 - val_acc: 0.8096\n",
      "Epoch 14/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.5297 - acc: 0.718 - ETA: 0s - loss: 0.4608 - acc: 0.800 - ETA: 0s - loss: 0.4822 - acc: 0.801 - ETA: 0s - loss: 0.4932 - acc: 0.791 - ETA: 0s - loss: 0.4784 - acc: 0.805 - ETA: 0s - loss: 0.4665 - acc: 0.817 - ETA: 0s - loss: 0.4685 - acc: 0.819 - ETA: 0s - loss: 0.4645 - acc: 0.823 - ETA: 0s - loss: 0.4630 - acc: 0.821 - ETA: 0s - loss: 0.4612 - acc: 0.821 - ETA: 0s - loss: 0.4592 - acc: 0.822 - ETA: 0s - loss: 0.4589 - acc: 0.821 - ETA: 0s - loss: 0.4570 - acc: 0.822 - ETA: 0s - loss: 0.4555 - acc: 0.820 - ETA: 0s - loss: 0.4580 - acc: 0.818 - ETA: 0s - loss: 0.4560 - acc: 0.820 - ETA: 0s - loss: 0.4587 - acc: 0.817 - 1s 284us/step - loss: 0.4591 - acc: 0.8176 - val_loss: 0.4349 - val_acc: 0.8156\n",
      "Epoch 15/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4925 - acc: 0.875 - ETA: 0s - loss: 0.4623 - acc: 0.808 - ETA: 0s - loss: 0.4351 - acc: 0.824 - ETA: 0s - loss: 0.4569 - acc: 0.811 - ETA: 0s - loss: 0.4446 - acc: 0.820 - ETA: 0s - loss: 0.4385 - acc: 0.821 - ETA: 0s - loss: 0.4459 - acc: 0.817 - ETA: 0s - loss: 0.4499 - acc: 0.814 - ETA: 0s - loss: 0.4523 - acc: 0.817 - ETA: 0s - loss: 0.4501 - acc: 0.818 - ETA: 0s - loss: 0.4508 - acc: 0.820 - ETA: 0s - loss: 0.4521 - acc: 0.818 - ETA: 0s - loss: 0.4523 - acc: 0.818 - ETA: 0s - loss: 0.4507 - acc: 0.819 - ETA: 0s - loss: 0.4533 - acc: 0.818 - ETA: 0s - loss: 0.4567 - acc: 0.815 - ETA: 0s - loss: 0.4535 - acc: 0.818 - ETA: 0s - loss: 0.4526 - acc: 0.819 - 1s 301us/step - loss: 0.4524 - acc: 0.8191 - val_loss: 0.4270 - val_acc: 0.8132\n",
      "Epoch 16/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2852 - acc: 1.000 - ETA: 0s - loss: 0.4463 - acc: 0.835 - ETA: 0s - loss: 0.4417 - acc: 0.831 - ETA: 0s - loss: 0.4402 - acc: 0.834 - ETA: 0s - loss: 0.4335 - acc: 0.841 - ETA: 0s - loss: 0.4334 - acc: 0.838 - ETA: 0s - loss: 0.4287 - acc: 0.835 - ETA: 0s - loss: 0.4469 - acc: 0.824 - ETA: 0s - loss: 0.4556 - acc: 0.820 - ETA: 0s - loss: 0.4524 - acc: 0.819 - ETA: 0s - loss: 0.4542 - acc: 0.816 - ETA: 0s - loss: 0.4540 - acc: 0.817 - ETA: 0s - loss: 0.4570 - acc: 0.817 - ETA: 0s - loss: 0.4558 - acc: 0.817 - ETA: 0s - loss: 0.4501 - acc: 0.820 - ETA: 0s - loss: 0.4497 - acc: 0.820 - ETA: 0s - loss: 0.4474 - acc: 0.822 - 1s 281us/step - loss: 0.4471 - acc: 0.8224 - val_loss: 0.4208 - val_acc: 0.8180\n",
      "Epoch 17/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4244 - acc: 0.875 - ETA: 0s - loss: 0.4694 - acc: 0.829 - ETA: 0s - loss: 0.4863 - acc: 0.805 - ETA: 0s - loss: 0.4767 - acc: 0.797 - ETA: 0s - loss: 0.4665 - acc: 0.806 - ETA: 0s - loss: 0.4572 - acc: 0.816 - ETA: 0s - loss: 0.4480 - acc: 0.823 - ETA: 0s - loss: 0.4476 - acc: 0.822 - ETA: 0s - loss: 0.4475 - acc: 0.820 - ETA: 0s - loss: 0.4481 - acc: 0.817 - ETA: 0s - loss: 0.4458 - acc: 0.821 - ETA: 0s - loss: 0.4463 - acc: 0.819 - ETA: 0s - loss: 0.4436 - acc: 0.821 - ETA: 0s - loss: 0.4433 - acc: 0.821 - ETA: 0s - loss: 0.4422 - acc: 0.821 - ETA: 0s - loss: 0.4429 - acc: 0.821 - ETA: 0s - loss: 0.4413 - acc: 0.822 - ETA: 0s - loss: 0.4387 - acc: 0.824 - 1s 302us/step - loss: 0.4380 - acc: 0.8248 - val_loss: 0.4152 - val_acc: 0.8204\n",
      "Epoch 18/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4471 - acc: 0.843 - ETA: 0s - loss: 0.4560 - acc: 0.824 - ETA: 0s - loss: 0.4625 - acc: 0.817 - ETA: 0s - loss: 0.4523 - acc: 0.812 - ETA: 0s - loss: 0.4491 - acc: 0.812 - ETA: 0s - loss: 0.4413 - acc: 0.809 - ETA: 0s - loss: 0.4318 - acc: 0.818 - ETA: 0s - loss: 0.4282 - acc: 0.822 - ETA: 0s - loss: 0.4354 - acc: 0.821 - ETA: 0s - loss: 0.4298 - acc: 0.825 - ETA: 0s - loss: 0.4271 - acc: 0.824 - ETA: 0s - loss: 0.4245 - acc: 0.827 - ETA: 0s - loss: 0.4241 - acc: 0.827 - ETA: 0s - loss: 0.4224 - acc: 0.830 - ETA: 0s - loss: 0.4300 - acc: 0.827 - ETA: 0s - loss: 0.4325 - acc: 0.827 - ETA: 0s - loss: 0.4369 - acc: 0.824 - 1s 289us/step - loss: 0.4349 - acc: 0.8257 - val_loss: 0.4092 - val_acc: 0.8204\n",
      "Epoch 19/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4564 - acc: 0.875 - ETA: 0s - loss: 0.4109 - acc: 0.839 - ETA: 0s - loss: 0.3995 - acc: 0.846 - ETA: 0s - loss: 0.4176 - acc: 0.828 - ETA: 0s - loss: 0.4161 - acc: 0.832 - ETA: 0s - loss: 0.4200 - acc: 0.827 - ETA: 0s - loss: 0.4228 - acc: 0.830 - ETA: 0s - loss: 0.4329 - acc: 0.823 - ETA: 0s - loss: 0.4358 - acc: 0.820 - ETA: 0s - loss: 0.4393 - acc: 0.820 - ETA: 0s - loss: 0.4422 - acc: 0.817 - ETA: 0s - loss: 0.4370 - acc: 0.821 - ETA: 0s - loss: 0.4335 - acc: 0.822 - ETA: 0s - loss: 0.4343 - acc: 0.821 - ETA: 0s - loss: 0.4330 - acc: 0.823 - ETA: 0s - loss: 0.4295 - acc: 0.825 - ETA: 0s - loss: 0.4318 - acc: 0.826 - 1s 297us/step - loss: 0.4303 - acc: 0.8272 - val_loss: 0.4053 - val_acc: 0.8228\n",
      "Epoch 20/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4716 - acc: 0.781 - ETA: 0s - loss: 0.4770 - acc: 0.781 - ETA: 0s - loss: 0.4691 - acc: 0.793 - ETA: 0s - loss: 0.4470 - acc: 0.807 - ETA: 0s - loss: 0.4375 - acc: 0.819 - ETA: 0s - loss: 0.4320 - acc: 0.816 - ETA: 0s - loss: 0.4242 - acc: 0.822 - ETA: 0s - loss: 0.4338 - acc: 0.816 - ETA: 0s - loss: 0.4299 - acc: 0.818 - ETA: 0s - loss: 0.4307 - acc: 0.823 - ETA: 0s - loss: 0.4286 - acc: 0.822 - ETA: 0s - loss: 0.4264 - acc: 0.824 - ETA: 0s - loss: 0.4284 - acc: 0.822 - ETA: 0s - loss: 0.4277 - acc: 0.824 - ETA: 0s - loss: 0.4262 - acc: 0.826 - 1s 261us/step - loss: 0.4256 - acc: 0.8263 - val_loss: 0.4024 - val_acc: 0.8240\n",
      "Epoch 21/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3902 - acc: 0.906 - ETA: 0s - loss: 0.4662 - acc: 0.812 - ETA: 0s - loss: 0.4371 - acc: 0.832 - ETA: 0s - loss: 0.4417 - acc: 0.823 - ETA: 0s - loss: 0.4381 - acc: 0.819 - ETA: 0s - loss: 0.4427 - acc: 0.819 - ETA: 0s - loss: 0.4367 - acc: 0.821 - ETA: 0s - loss: 0.4295 - acc: 0.826 - ETA: 0s - loss: 0.4261 - acc: 0.829 - ETA: 0s - loss: 0.4251 - acc: 0.827 - ETA: 0s - loss: 0.4249 - acc: 0.827 - ETA: 0s - loss: 0.4204 - acc: 0.831 - ETA: 0s - loss: 0.4205 - acc: 0.831 - ETA: 0s - loss: 0.4205 - acc: 0.831 - ETA: 0s - loss: 0.4186 - acc: 0.833 - ETA: 0s - loss: 0.4223 - acc: 0.831 - ETA: 0s - loss: 0.4236 - acc: 0.831 - 1s 277us/step - loss: 0.4227 - acc: 0.8317 - val_loss: 0.3981 - val_acc: 0.8335\n",
      "Epoch 22/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3815 - acc: 0.906 - ETA: 0s - loss: 0.4050 - acc: 0.843 - ETA: 0s - loss: 0.4234 - acc: 0.845 - ETA: 0s - loss: 0.4057 - acc: 0.849 - ETA: 0s - loss: 0.4043 - acc: 0.849 - ETA: 0s - loss: 0.3949 - acc: 0.848 - ETA: 0s - loss: 0.4014 - acc: 0.840 - ETA: 0s - loss: 0.4086 - acc: 0.838 - ETA: 0s - loss: 0.4124 - acc: 0.834 - ETA: 0s - loss: 0.4098 - acc: 0.837 - ETA: 0s - loss: 0.4135 - acc: 0.834 - ETA: 0s - loss: 0.4124 - acc: 0.834 - ETA: 0s - loss: 0.4187 - acc: 0.832 - ETA: 0s - loss: 0.4187 - acc: 0.832 - ETA: 0s - loss: 0.4185 - acc: 0.833 - ETA: 0s - loss: 0.4185 - acc: 0.833 - ETA: 0s - loss: 0.4161 - acc: 0.835 - 1s 288us/step - loss: 0.4166 - acc: 0.8350 - val_loss: 0.3961 - val_acc: 0.8240\n",
      "Epoch 23/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3675 - acc: 0.843 - ETA: 0s - loss: 0.4093 - acc: 0.824 - ETA: 0s - loss: 0.3758 - acc: 0.847 - ETA: 0s - loss: 0.3887 - acc: 0.843 - ETA: 0s - loss: 0.3944 - acc: 0.842 - ETA: 0s - loss: 0.3894 - acc: 0.848 - ETA: 0s - loss: 0.3953 - acc: 0.841 - ETA: 0s - loss: 0.3967 - acc: 0.839 - ETA: 0s - loss: 0.4073 - acc: 0.836 - ETA: 0s - loss: 0.4138 - acc: 0.830 - ETA: 0s - loss: 0.4115 - acc: 0.833 - ETA: 0s - loss: 0.4095 - acc: 0.834 - ETA: 0s - loss: 0.4119 - acc: 0.832 - ETA: 0s - loss: 0.4127 - acc: 0.832 - ETA: 0s - loss: 0.4138 - acc: 0.833 - ETA: 0s - loss: 0.4105 - acc: 0.835 - ETA: 0s - loss: 0.4131 - acc: 0.834 - 1s 286us/step - loss: 0.4123 - acc: 0.8347 - val_loss: 0.3936 - val_acc: 0.8335\n",
      "Epoch 24/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4705 - acc: 0.812 - ETA: 0s - loss: 0.4307 - acc: 0.843 - ETA: 0s - loss: 0.4234 - acc: 0.839 - ETA: 0s - loss: 0.4127 - acc: 0.843 - ETA: 0s - loss: 0.4249 - acc: 0.837 - ETA: 0s - loss: 0.4274 - acc: 0.837 - ETA: 0s - loss: 0.4281 - acc: 0.836 - ETA: 0s - loss: 0.4247 - acc: 0.838 - ETA: 0s - loss: 0.4274 - acc: 0.833 - ETA: 0s - loss: 0.4236 - acc: 0.835 - ETA: 0s - loss: 0.4219 - acc: 0.838 - ETA: 0s - loss: 0.4152 - acc: 0.839 - ETA: 0s - loss: 0.4137 - acc: 0.838 - ETA: 0s - loss: 0.4132 - acc: 0.839 - ETA: 0s - loss: 0.4112 - acc: 0.838 - ETA: 0s - loss: 0.4121 - acc: 0.838 - 1s 270us/step - loss: 0.4119 - acc: 0.8389 - val_loss: 0.3963 - val_acc: 0.8347\n",
      "Epoch 25/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3102 - acc: 0.906 - ETA: 0s - loss: 0.4221 - acc: 0.843 - ETA: 0s - loss: 0.3954 - acc: 0.862 - ETA: 0s - loss: 0.3887 - acc: 0.862 - ETA: 0s - loss: 0.3926 - acc: 0.857 - ETA: 0s - loss: 0.3944 - acc: 0.857 - ETA: 0s - loss: 0.4000 - acc: 0.854 - ETA: 0s - loss: 0.4025 - acc: 0.850 - ETA: 0s - loss: 0.4011 - acc: 0.849 - ETA: 0s - loss: 0.4039 - acc: 0.846 - ETA: 0s - loss: 0.4017 - acc: 0.847 - ETA: 0s - loss: 0.3999 - acc: 0.848 - ETA: 0s - loss: 0.3983 - acc: 0.847 - ETA: 0s - loss: 0.4033 - acc: 0.844 - ETA: 0s - loss: 0.4035 - acc: 0.843 - ETA: 0s - loss: 0.4074 - acc: 0.840 - 1s 271us/step - loss: 0.4117 - acc: 0.8392 - val_loss: 0.3901 - val_acc: 0.8419\n",
      "Epoch 26/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2806 - acc: 0.906 - ETA: 0s - loss: 0.3759 - acc: 0.855 - ETA: 0s - loss: 0.3914 - acc: 0.841 - ETA: 0s - loss: 0.3985 - acc: 0.835 - ETA: 0s - loss: 0.3943 - acc: 0.834 - ETA: 0s - loss: 0.3857 - acc: 0.842 - ETA: 0s - loss: 0.3856 - acc: 0.850 - ETA: 0s - loss: 0.3859 - acc: 0.849 - ETA: 0s - loss: 0.3877 - acc: 0.849 - ETA: 0s - loss: 0.3994 - acc: 0.843 - ETA: 0s - loss: 0.4009 - acc: 0.841 - ETA: 0s - loss: 0.4109 - acc: 0.837 - ETA: 0s - loss: 0.4052 - acc: 0.841 - ETA: 0s - loss: 0.4040 - acc: 0.842 - ETA: 0s - loss: 0.4059 - acc: 0.840 - 1s 250us/step - loss: 0.4057 - acc: 0.8404 - val_loss: 0.3883 - val_acc: 0.8395\n",
      "Epoch 27/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.8156 - acc: 0.718 - ETA: 0s - loss: 0.5098 - acc: 0.794 - ETA: 0s - loss: 0.4699 - acc: 0.814 - ETA: 0s - loss: 0.4632 - acc: 0.814 - ETA: 0s - loss: 0.4537 - acc: 0.818 - ETA: 0s - loss: 0.4462 - acc: 0.819 - ETA: 0s - loss: 0.4399 - acc: 0.817 - ETA: 0s - loss: 0.4297 - acc: 0.822 - ETA: 0s - loss: 0.4328 - acc: 0.819 - ETA: 0s - loss: 0.4291 - acc: 0.822 - ETA: 0s - loss: 0.4264 - acc: 0.826 - ETA: 0s - loss: 0.4218 - acc: 0.831 - ETA: 0s - loss: 0.4231 - acc: 0.829 - ETA: 0s - loss: 0.4178 - acc: 0.830 - ETA: 0s - loss: 0.4193 - acc: 0.828 - ETA: 0s - loss: 0.4140 - acc: 0.832 - ETA: 0s - loss: 0.4086 - acc: 0.838 - ETA: 0s - loss: 0.4064 - acc: 0.838 - 1s 293us/step - loss: 0.4079 - acc: 0.8383 - val_loss: 0.3871 - val_acc: 0.8359\n",
      "Epoch 28/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4198 - acc: 0.812 - ETA: 0s - loss: 0.4278 - acc: 0.828 - ETA: 0s - loss: 0.4340 - acc: 0.831 - ETA: 0s - loss: 0.4175 - acc: 0.839 - ETA: 0s - loss: 0.4131 - acc: 0.840 - ETA: 0s - loss: 0.4042 - acc: 0.847 - ETA: 0s - loss: 0.4029 - acc: 0.848 - ETA: 0s - loss: 0.4054 - acc: 0.847 - ETA: 0s - loss: 0.4051 - acc: 0.845 - ETA: 0s - loss: 0.4057 - acc: 0.844 - ETA: 0s - loss: 0.4018 - acc: 0.846 - ETA: 0s - loss: 0.4000 - acc: 0.842 - ETA: 0s - loss: 0.3992 - acc: 0.844 - ETA: 0s - loss: 0.3981 - acc: 0.846 - ETA: 0s - loss: 0.4025 - acc: 0.844 - ETA: 0s - loss: 0.4052 - acc: 0.843 - 1s 274us/step - loss: 0.4080 - acc: 0.8413 - val_loss: 0.3896 - val_acc: 0.8455\n",
      "Epoch 29/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.5313 - acc: 0.812 - ETA: 0s - loss: 0.4049 - acc: 0.836 - ETA: 0s - loss: 0.4086 - acc: 0.831 - ETA: 0s - loss: 0.3976 - acc: 0.836 - ETA: 0s - loss: 0.3987 - acc: 0.831 - ETA: 0s - loss: 0.3941 - acc: 0.836 - ETA: 0s - loss: 0.3950 - acc: 0.836 - ETA: 0s - loss: 0.3951 - acc: 0.835 - ETA: 0s - loss: 0.4050 - acc: 0.834 - ETA: 0s - loss: 0.4013 - acc: 0.836 - ETA: 0s - loss: 0.4022 - acc: 0.835 - ETA: 0s - loss: 0.4012 - acc: 0.836 - ETA: 0s - loss: 0.3998 - acc: 0.836 - ETA: 0s - loss: 0.3976 - acc: 0.836 - ETA: 0s - loss: 0.3999 - acc: 0.836 - ETA: 0s - loss: 0.3973 - acc: 0.838 - 1s 264us/step - loss: 0.3968 - acc: 0.8392 - val_loss: 0.3843 - val_acc: 0.8419\n",
      "Epoch 30/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4968 - acc: 0.750 - ETA: 0s - loss: 0.4168 - acc: 0.817 - ETA: 0s - loss: 0.4044 - acc: 0.830 - ETA: 0s - loss: 0.3982 - acc: 0.836 - ETA: 0s - loss: 0.4105 - acc: 0.831 - ETA: 0s - loss: 0.4108 - acc: 0.828 - ETA: 0s - loss: 0.4076 - acc: 0.832 - ETA: 0s - loss: 0.4017 - acc: 0.834 - ETA: 0s - loss: 0.3997 - acc: 0.835 - ETA: 0s - loss: 0.4037 - acc: 0.834 - ETA: 0s - loss: 0.4029 - acc: 0.831 - ETA: 0s - loss: 0.4057 - acc: 0.832 - ETA: 0s - loss: 0.4040 - acc: 0.834 - ETA: 0s - loss: 0.4049 - acc: 0.834 - ETA: 0s - loss: 0.4034 - acc: 0.836 - ETA: 0s - loss: 0.4024 - acc: 0.836 - 1s 274us/step - loss: 0.4007 - acc: 0.8380 - val_loss: 0.3827 - val_acc: 0.8491\n",
      "Epoch 31/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3023 - acc: 0.875 - ETA: 0s - loss: 0.3572 - acc: 0.843 - ETA: 0s - loss: 0.3752 - acc: 0.846 - ETA: 0s - loss: 0.3859 - acc: 0.853 - ETA: 0s - loss: 0.3795 - acc: 0.851 - ETA: 0s - loss: 0.3854 - acc: 0.850 - ETA: 0s - loss: 0.3887 - acc: 0.849 - ETA: 0s - loss: 0.3887 - acc: 0.849 - ETA: 0s - loss: 0.3925 - acc: 0.846 - ETA: 0s - loss: 0.3989 - acc: 0.844 - ETA: 0s - loss: 0.4004 - acc: 0.841 - ETA: 0s - loss: 0.4015 - acc: 0.839 - ETA: 0s - loss: 0.4048 - acc: 0.838 - ETA: 0s - loss: 0.4058 - acc: 0.839 - ETA: 0s - loss: 0.4018 - acc: 0.841 - ETA: 0s - loss: 0.4021 - acc: 0.841 - 1s 274us/step - loss: 0.3997 - acc: 0.8431 - val_loss: 0.3817 - val_acc: 0.8479\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3016 - acc: 0.906 - ETA: 0s - loss: 0.3465 - acc: 0.888 - ETA: 0s - loss: 0.3860 - acc: 0.868 - ETA: 0s - loss: 0.3908 - acc: 0.865 - ETA: 0s - loss: 0.3631 - acc: 0.869 - ETA: 0s - loss: 0.3723 - acc: 0.852 - ETA: 0s - loss: 0.3727 - acc: 0.857 - ETA: 0s - loss: 0.3787 - acc: 0.850 - ETA: 0s - loss: 0.3928 - acc: 0.843 - ETA: 0s - loss: 0.3896 - acc: 0.844 - ETA: 0s - loss: 0.3878 - acc: 0.845 - ETA: 0s - loss: 0.3925 - acc: 0.841 - ETA: 0s - loss: 0.3970 - acc: 0.839 - ETA: 0s - loss: 0.3991 - acc: 0.840 - ETA: 0s - loss: 0.3999 - acc: 0.841 - ETA: 0s - loss: 0.3991 - acc: 0.841 - 1s 272us/step - loss: 0.3985 - acc: 0.8410 - val_loss: 0.3875 - val_acc: 0.8455\n",
      "Epoch 33/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4814 - acc: 0.718 - ETA: 0s - loss: 0.3618 - acc: 0.843 - ETA: 0s - loss: 0.3661 - acc: 0.834 - ETA: 0s - loss: 0.3749 - acc: 0.830 - ETA: 0s - loss: 0.3798 - acc: 0.832 - ETA: 0s - loss: 0.3786 - acc: 0.833 - ETA: 0s - loss: 0.3827 - acc: 0.832 - ETA: 0s - loss: 0.3870 - acc: 0.833 - ETA: 0s - loss: 0.3876 - acc: 0.831 - ETA: 0s - loss: 0.3896 - acc: 0.832 - ETA: 0s - loss: 0.3918 - acc: 0.834 - ETA: 0s - loss: 0.3901 - acc: 0.836 - ETA: 0s - loss: 0.3878 - acc: 0.838 - ETA: 0s - loss: 0.3920 - acc: 0.838 - ETA: 0s - loss: 0.3911 - acc: 0.839 - ETA: 0s - loss: 0.3960 - acc: 0.838 - ETA: 0s - loss: 0.3965 - acc: 0.838 - 1s 278us/step - loss: 0.3963 - acc: 0.8389 - val_loss: 0.3796 - val_acc: 0.8431\n",
      "Epoch 34/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4259 - acc: 0.843 - ETA: 0s - loss: 0.4072 - acc: 0.826 - ETA: 0s - loss: 0.4106 - acc: 0.814 - ETA: 0s - loss: 0.4117 - acc: 0.830 - ETA: 0s - loss: 0.4083 - acc: 0.836 - ETA: 0s - loss: 0.4139 - acc: 0.831 - ETA: 0s - loss: 0.4106 - acc: 0.838 - ETA: 0s - loss: 0.4042 - acc: 0.841 - ETA: 0s - loss: 0.3989 - acc: 0.841 - ETA: 0s - loss: 0.3991 - acc: 0.842 - ETA: 0s - loss: 0.4024 - acc: 0.840 - ETA: 0s - loss: 0.4045 - acc: 0.840 - ETA: 0s - loss: 0.4021 - acc: 0.842 - ETA: 0s - loss: 0.4014 - acc: 0.843 - ETA: 0s - loss: 0.3985 - acc: 0.846 - ETA: 0s - loss: 0.4004 - acc: 0.842 - 1s 263us/step - loss: 0.3992 - acc: 0.8434 - val_loss: 0.3789 - val_acc: 0.8479\n",
      "Epoch 35/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.5192 - acc: 0.875 - ETA: 0s - loss: 0.4040 - acc: 0.866 - ETA: 0s - loss: 0.4003 - acc: 0.861 - ETA: 0s - loss: 0.3901 - acc: 0.855 - ETA: 0s - loss: 0.3928 - acc: 0.844 - ETA: 0s - loss: 0.3979 - acc: 0.842 - ETA: 0s - loss: 0.3880 - acc: 0.849 - ETA: 0s - loss: 0.3830 - acc: 0.852 - ETA: 0s - loss: 0.3795 - acc: 0.854 - ETA: 0s - loss: 0.3807 - acc: 0.852 - ETA: 0s - loss: 0.3798 - acc: 0.853 - ETA: 0s - loss: 0.3760 - acc: 0.853 - ETA: 0s - loss: 0.3774 - acc: 0.852 - ETA: 0s - loss: 0.3849 - acc: 0.848 - ETA: 0s - loss: 0.3953 - acc: 0.843 - ETA: 0s - loss: 0.3970 - acc: 0.844 - 1s 268us/step - loss: 0.3969 - acc: 0.8428 - val_loss: 0.3784 - val_acc: 0.8467\n",
      "Epoch 36/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3322 - acc: 0.843 - ETA: 0s - loss: 0.3509 - acc: 0.863 - ETA: 0s - loss: 0.3564 - acc: 0.870 - ETA: 0s - loss: 0.3565 - acc: 0.865 - ETA: 0s - loss: 0.3723 - acc: 0.847 - ETA: 0s - loss: 0.3772 - acc: 0.842 - ETA: 0s - loss: 0.3873 - acc: 0.839 - ETA: 0s - loss: 0.3885 - acc: 0.837 - ETA: 0s - loss: 0.3925 - acc: 0.838 - ETA: 0s - loss: 0.3917 - acc: 0.839 - ETA: 0s - loss: 0.3890 - acc: 0.841 - ETA: 0s - loss: 0.3899 - acc: 0.840 - ETA: 0s - loss: 0.3856 - acc: 0.844 - ETA: 0s - loss: 0.3856 - acc: 0.845 - ETA: 0s - loss: 0.3904 - acc: 0.843 - 1s 262us/step - loss: 0.3934 - acc: 0.8422 - val_loss: 0.3775 - val_acc: 0.8395\n",
      "Epoch 37/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2551 - acc: 0.906 - ETA: 0s - loss: 0.3908 - acc: 0.840 - ETA: 0s - loss: 0.3789 - acc: 0.845 - ETA: 0s - loss: 0.3891 - acc: 0.845 - ETA: 0s - loss: 0.4030 - acc: 0.837 - ETA: 0s - loss: 0.3978 - acc: 0.838 - ETA: 0s - loss: 0.3974 - acc: 0.838 - ETA: 0s - loss: 0.3879 - acc: 0.847 - ETA: 0s - loss: 0.3859 - acc: 0.849 - ETA: 0s - loss: 0.3817 - acc: 0.852 - ETA: 0s - loss: 0.3809 - acc: 0.852 - ETA: 0s - loss: 0.3801 - acc: 0.851 - ETA: 0s - loss: 0.3799 - acc: 0.852 - ETA: 0s - loss: 0.3857 - acc: 0.849 - ETA: 0s - loss: 0.3898 - acc: 0.847 - ETA: 0s - loss: 0.3881 - acc: 0.850 - 1s 278us/step - loss: 0.3926 - acc: 0.8467 - val_loss: 0.3773 - val_acc: 0.8479\n",
      "Epoch 38/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4355 - acc: 0.843 - ETA: 0s - loss: 0.3703 - acc: 0.855 - ETA: 0s - loss: 0.4153 - acc: 0.847 - ETA: 0s - loss: 0.3987 - acc: 0.850 - ETA: 0s - loss: 0.4130 - acc: 0.838 - ETA: 0s - loss: 0.4184 - acc: 0.835 - ETA: 0s - loss: 0.4141 - acc: 0.841 - ETA: 0s - loss: 0.4058 - acc: 0.845 - ETA: 0s - loss: 0.4021 - acc: 0.848 - ETA: 0s - loss: 0.3981 - acc: 0.849 - ETA: 0s - loss: 0.3966 - acc: 0.850 - ETA: 0s - loss: 0.3962 - acc: 0.850 - ETA: 0s - loss: 0.3930 - acc: 0.850 - ETA: 0s - loss: 0.3943 - acc: 0.849 - ETA: 0s - loss: 0.3949 - acc: 0.848 - 1s 256us/step - loss: 0.3938 - acc: 0.8485 - val_loss: 0.3800 - val_acc: 0.8527\n",
      "Epoch 39/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3870 - acc: 0.812 - ETA: 0s - loss: 0.3909 - acc: 0.850 - ETA: 0s - loss: 0.4066 - acc: 0.840 - ETA: 0s - loss: 0.4134 - acc: 0.837 - ETA: 0s - loss: 0.4132 - acc: 0.835 - ETA: 0s - loss: 0.3978 - acc: 0.840 - ETA: 0s - loss: 0.3922 - acc: 0.840 - ETA: 0s - loss: 0.4024 - acc: 0.837 - ETA: 0s - loss: 0.4002 - acc: 0.838 - ETA: 0s - loss: 0.3943 - acc: 0.839 - ETA: 0s - loss: 0.3932 - acc: 0.842 - ETA: 0s - loss: 0.3921 - acc: 0.845 - ETA: 0s - loss: 0.3915 - acc: 0.846 - ETA: 0s - loss: 0.3945 - acc: 0.844 - ETA: 0s - loss: 0.3911 - acc: 0.847 - ETA: 0s - loss: 0.3900 - acc: 0.846 - 1s 272us/step - loss: 0.3895 - acc: 0.8470 - val_loss: 0.3753 - val_acc: 0.8491\n",
      "Epoch 40/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3373 - acc: 0.812 - ETA: 0s - loss: 0.4137 - acc: 0.821 - ETA: 0s - loss: 0.4412 - acc: 0.817 - ETA: 0s - loss: 0.4341 - acc: 0.825 - ETA: 0s - loss: 0.4135 - acc: 0.835 - ETA: 0s - loss: 0.4159 - acc: 0.830 - ETA: 0s - loss: 0.4198 - acc: 0.830 - ETA: 0s - loss: 0.4241 - acc: 0.830 - ETA: 0s - loss: 0.4155 - acc: 0.835 - ETA: 0s - loss: 0.4057 - acc: 0.841 - ETA: 0s - loss: 0.4041 - acc: 0.842 - ETA: 0s - loss: 0.4038 - acc: 0.843 - ETA: 0s - loss: 0.4010 - acc: 0.844 - ETA: 0s - loss: 0.3990 - acc: 0.844 - ETA: 0s - loss: 0.3951 - acc: 0.846 - ETA: 0s - loss: 0.3958 - acc: 0.845 - ETA: 0s - loss: 0.3927 - acc: 0.846 - ETA: 0s - loss: 0.3931 - acc: 0.847 - 1s 307us/step - loss: 0.3923 - acc: 0.8488 - val_loss: 0.3752 - val_acc: 0.8479\n",
      "Epoch 41/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4808 - acc: 0.812 - ETA: 0s - loss: 0.3887 - acc: 0.849 - ETA: 0s - loss: 0.3909 - acc: 0.852 - ETA: 0s - loss: 0.3936 - acc: 0.857 - ETA: 0s - loss: 0.3921 - acc: 0.858 - ETA: 0s - loss: 0.3881 - acc: 0.861 - ETA: 0s - loss: 0.3931 - acc: 0.854 - ETA: 0s - loss: 0.3990 - acc: 0.848 - ETA: 0s - loss: 0.4013 - acc: 0.848 - ETA: 0s - loss: 0.4128 - acc: 0.843 - ETA: 0s - loss: 0.4058 - acc: 0.846 - ETA: 0s - loss: 0.4052 - acc: 0.845 - ETA: 0s - loss: 0.4006 - acc: 0.847 - ETA: 0s - loss: 0.4002 - acc: 0.848 - ETA: 0s - loss: 0.3947 - acc: 0.849 - ETA: 0s - loss: 0.3945 - acc: 0.848 - 1s 272us/step - loss: 0.3944 - acc: 0.8485 - val_loss: 0.3744 - val_acc: 0.8515\n",
      "Epoch 42/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2187 - acc: 0.937 - ETA: 0s - loss: 0.3630 - acc: 0.861 - ETA: 0s - loss: 0.3579 - acc: 0.870 - ETA: 0s - loss: 0.3788 - acc: 0.849 - ETA: 0s - loss: 0.3817 - acc: 0.851 - ETA: 0s - loss: 0.3854 - acc: 0.850 - ETA: 0s - loss: 0.3929 - acc: 0.848 - ETA: 0s - loss: 0.3875 - acc: 0.849 - ETA: 0s - loss: 0.3917 - acc: 0.844 - ETA: 0s - loss: 0.3925 - acc: 0.840 - ETA: 0s - loss: 0.3909 - acc: 0.841 - ETA: 0s - loss: 0.3943 - acc: 0.840 - ETA: 0s - loss: 0.3922 - acc: 0.840 - ETA: 0s - loss: 0.3928 - acc: 0.840 - ETA: 0s - loss: 0.3871 - acc: 0.843 - 1s 256us/step - loss: 0.3871 - acc: 0.8452 - val_loss: 0.3749 - val_acc: 0.8455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.5672 - acc: 0.750 - ETA: 0s - loss: 0.4499 - acc: 0.816 - ETA: 0s - loss: 0.4265 - acc: 0.822 - ETA: 0s - loss: 0.3944 - acc: 0.839 - ETA: 0s - loss: 0.3861 - acc: 0.844 - ETA: 0s - loss: 0.3746 - acc: 0.847 - ETA: 0s - loss: 0.3843 - acc: 0.846 - ETA: 0s - loss: 0.3852 - acc: 0.845 - ETA: 0s - loss: 0.3897 - acc: 0.841 - ETA: 0s - loss: 0.3930 - acc: 0.840 - ETA: 0s - loss: 0.3997 - acc: 0.836 - ETA: 0s - loss: 0.3935 - acc: 0.837 - ETA: 0s - loss: 0.3942 - acc: 0.840 - ETA: 0s - loss: 0.3892 - acc: 0.841 - ETA: 0s - loss: 0.3939 - acc: 0.843 - ETA: 0s - loss: 0.3906 - acc: 0.845 - ETA: 0s - loss: 0.3899 - acc: 0.845 - 1s 280us/step - loss: 0.3894 - acc: 0.8464 - val_loss: 0.3742 - val_acc: 0.8467\n",
      "Epoch 44/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3687 - acc: 0.843 - ETA: 0s - loss: 0.3906 - acc: 0.878 - ETA: 0s - loss: 0.3840 - acc: 0.872 - ETA: 0s - loss: 0.3997 - acc: 0.851 - ETA: 0s - loss: 0.3915 - acc: 0.847 - ETA: 0s - loss: 0.3850 - acc: 0.852 - ETA: 0s - loss: 0.3817 - acc: 0.847 - ETA: 0s - loss: 0.3810 - acc: 0.849 - ETA: 0s - loss: 0.3835 - acc: 0.849 - ETA: 0s - loss: 0.3896 - acc: 0.844 - ETA: 0s - loss: 0.3839 - acc: 0.847 - ETA: 0s - loss: 0.3836 - acc: 0.850 - ETA: 0s - loss: 0.3843 - acc: 0.848 - ETA: 0s - loss: 0.3817 - acc: 0.849 - ETA: 0s - loss: 0.3829 - acc: 0.847 - ETA: 0s - loss: 0.3843 - acc: 0.845 - 1s 271us/step - loss: 0.3844 - acc: 0.8443 - val_loss: 0.3747 - val_acc: 0.8467\n",
      "Epoch 45/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2793 - acc: 0.968 - ETA: 0s - loss: 0.3771 - acc: 0.847 - ETA: 0s - loss: 0.3982 - acc: 0.810 - ETA: 0s - loss: 0.3926 - acc: 0.832 - ETA: 0s - loss: 0.3940 - acc: 0.831 - ETA: 0s - loss: 0.3840 - acc: 0.841 - ETA: 0s - loss: 0.3878 - acc: 0.842 - ETA: 0s - loss: 0.3898 - acc: 0.843 - ETA: 0s - loss: 0.3898 - acc: 0.844 - ETA: 0s - loss: 0.3892 - acc: 0.845 - ETA: 0s - loss: 0.3897 - acc: 0.844 - ETA: 0s - loss: 0.3908 - acc: 0.841 - ETA: 0s - loss: 0.3931 - acc: 0.842 - ETA: 0s - loss: 0.3878 - acc: 0.846 - ETA: 0s - loss: 0.3863 - acc: 0.846 - ETA: 0s - loss: 0.3888 - acc: 0.845 - ETA: 0s - loss: 0.3844 - acc: 0.847 - 1s 278us/step - loss: 0.3850 - acc: 0.8473 - val_loss: 0.3719 - val_acc: 0.8527\n",
      "Epoch 46/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3593 - acc: 0.843 - ETA: 0s - loss: 0.4383 - acc: 0.852 - ETA: 0s - loss: 0.4212 - acc: 0.848 - ETA: 0s - loss: 0.4051 - acc: 0.843 - ETA: 0s - loss: 0.3971 - acc: 0.849 - ETA: 0s - loss: 0.3952 - acc: 0.846 - ETA: 0s - loss: 0.3977 - acc: 0.849 - ETA: 0s - loss: 0.3913 - acc: 0.850 - ETA: 0s - loss: 0.3881 - acc: 0.852 - ETA: 0s - loss: 0.3887 - acc: 0.852 - ETA: 0s - loss: 0.3867 - acc: 0.853 - ETA: 0s - loss: 0.3831 - acc: 0.853 - ETA: 0s - loss: 0.3815 - acc: 0.854 - ETA: 0s - loss: 0.3788 - acc: 0.854 - ETA: 0s - loss: 0.3807 - acc: 0.853 - ETA: 0s - loss: 0.3781 - acc: 0.856 - ETA: 0s - loss: 0.3827 - acc: 0.852 - 1s 288us/step - loss: 0.3850 - acc: 0.8506 - val_loss: 0.3709 - val_acc: 0.8539\n",
      "Epoch 47/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3439 - acc: 0.875 - ETA: 0s - loss: 0.3848 - acc: 0.848 - ETA: 0s - loss: 0.3551 - acc: 0.847 - ETA: 0s - loss: 0.3777 - acc: 0.843 - ETA: 0s - loss: 0.3798 - acc: 0.839 - ETA: 0s - loss: 0.3742 - acc: 0.843 - ETA: 0s - loss: 0.3817 - acc: 0.843 - ETA: 0s - loss: 0.3797 - acc: 0.848 - ETA: 0s - loss: 0.3776 - acc: 0.849 - ETA: 0s - loss: 0.3776 - acc: 0.851 - ETA: 0s - loss: 0.3787 - acc: 0.850 - ETA: 0s - loss: 0.3802 - acc: 0.853 - ETA: 0s - loss: 0.3838 - acc: 0.851 - ETA: 0s - loss: 0.3804 - acc: 0.852 - ETA: 0s - loss: 0.3762 - acc: 0.853 - ETA: 0s - loss: 0.3799 - acc: 0.852 - ETA: 0s - loss: 0.3808 - acc: 0.851 - 1s 288us/step - loss: 0.3819 - acc: 0.8503 - val_loss: 0.3700 - val_acc: 0.8491\n",
      "Epoch 48/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.7249 - acc: 0.812 - ETA: 0s - loss: 0.4489 - acc: 0.828 - ETA: 0s - loss: 0.4220 - acc: 0.843 - ETA: 0s - loss: 0.3964 - acc: 0.857 - ETA: 0s - loss: 0.3774 - acc: 0.863 - ETA: 0s - loss: 0.3795 - acc: 0.859 - ETA: 0s - loss: 0.3733 - acc: 0.861 - ETA: 0s - loss: 0.3815 - acc: 0.853 - ETA: 0s - loss: 0.3915 - acc: 0.848 - ETA: 0s - loss: 0.3903 - acc: 0.848 - ETA: 0s - loss: 0.3882 - acc: 0.850 - ETA: 0s - loss: 0.3831 - acc: 0.854 - ETA: 0s - loss: 0.3862 - acc: 0.852 - ETA: 0s - loss: 0.3839 - acc: 0.853 - ETA: 0s - loss: 0.3879 - acc: 0.850 - 1s 253us/step - loss: 0.3845 - acc: 0.8521 - val_loss: 0.3701 - val_acc: 0.8527\n",
      "Epoch 49/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4406 - acc: 0.781 - ETA: 0s - loss: 0.4072 - acc: 0.812 - ETA: 0s - loss: 0.3680 - acc: 0.841 - ETA: 0s - loss: 0.3812 - acc: 0.832 - ETA: 0s - loss: 0.3696 - acc: 0.847 - ETA: 0s - loss: 0.3752 - acc: 0.845 - ETA: 0s - loss: 0.3775 - acc: 0.839 - ETA: 0s - loss: 0.3867 - acc: 0.837 - ETA: 0s - loss: 0.3853 - acc: 0.838 - ETA: 0s - loss: 0.3820 - acc: 0.841 - ETA: 0s - loss: 0.3847 - acc: 0.840 - ETA: 0s - loss: 0.3847 - acc: 0.842 - ETA: 0s - loss: 0.3802 - acc: 0.844 - ETA: 0s - loss: 0.3784 - acc: 0.846 - ETA: 0s - loss: 0.3782 - acc: 0.848 - ETA: 0s - loss: 0.3752 - acc: 0.850 - ETA: 0s - loss: 0.3800 - acc: 0.849 - 1s 286us/step - loss: 0.3809 - acc: 0.8497 - val_loss: 0.3688 - val_acc: 0.8479\n",
      "Epoch 50/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3098 - acc: 0.843 - ETA: 0s - loss: 0.3361 - acc: 0.857 - ETA: 0s - loss: 0.3555 - acc: 0.860 - ETA: 0s - loss: 0.3831 - acc: 0.851 - ETA: 0s - loss: 0.3940 - acc: 0.845 - ETA: 0s - loss: 0.3878 - acc: 0.848 - ETA: 0s - loss: 0.3755 - acc: 0.854 - ETA: 0s - loss: 0.3731 - acc: 0.857 - ETA: 0s - loss: 0.3731 - acc: 0.856 - ETA: 0s - loss: 0.3748 - acc: 0.856 - ETA: 0s - loss: 0.3764 - acc: 0.854 - ETA: 0s - loss: 0.3784 - acc: 0.851 - ETA: 0s - loss: 0.3768 - acc: 0.850 - ETA: 0s - loss: 0.3753 - acc: 0.850 - ETA: 0s - loss: 0.3807 - acc: 0.848 - ETA: 0s - loss: 0.3791 - acc: 0.848 - ETA: 0s - loss: 0.3804 - acc: 0.848 - ETA: 0s - loss: 0.3820 - acc: 0.846 - 1s 301us/step - loss: 0.3793 - acc: 0.8476 - val_loss: 0.3704 - val_acc: 0.8491\n",
      "Epoch 51/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3982 - acc: 0.875 - ETA: 0s - loss: 0.3850 - acc: 0.863 - ETA: 0s - loss: 0.3941 - acc: 0.841 - ETA: 0s - loss: 0.3906 - acc: 0.842 - ETA: 0s - loss: 0.3753 - acc: 0.848 - ETA: 0s - loss: 0.3705 - acc: 0.849 - ETA: 0s - loss: 0.3720 - acc: 0.848 - ETA: 0s - loss: 0.3867 - acc: 0.844 - ETA: 0s - loss: 0.3821 - acc: 0.844 - ETA: 0s - loss: 0.3790 - acc: 0.846 - ETA: 0s - loss: 0.3785 - acc: 0.844 - ETA: 0s - loss: 0.3815 - acc: 0.843 - ETA: 0s - loss: 0.3823 - acc: 0.842 - ETA: 0s - loss: 0.3893 - acc: 0.841 - ETA: 0s - loss: 0.3890 - acc: 0.842 - ETA: 0s - loss: 0.3853 - acc: 0.847 - ETA: 0s - loss: 0.3829 - acc: 0.848 - 1s 287us/step - loss: 0.3799 - acc: 0.8491 - val_loss: 0.3687 - val_acc: 0.8539\n",
      "Epoch 52/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4008 - acc: 0.875 - ETA: 0s - loss: 0.3418 - acc: 0.875 - ETA: 0s - loss: 0.3204 - acc: 0.881 - ETA: 0s - loss: 0.3387 - acc: 0.864 - ETA: 0s - loss: 0.3684 - acc: 0.851 - ETA: 0s - loss: 0.3581 - acc: 0.856 - ETA: 0s - loss: 0.3619 - acc: 0.853 - ETA: 0s - loss: 0.3782 - acc: 0.849 - ETA: 0s - loss: 0.3750 - acc: 0.852 - ETA: 0s - loss: 0.3799 - acc: 0.852 - ETA: 0s - loss: 0.3739 - acc: 0.856 - ETA: 0s - loss: 0.3748 - acc: 0.855 - ETA: 0s - loss: 0.3723 - acc: 0.854 - ETA: 0s - loss: 0.3755 - acc: 0.852 - ETA: 0s - loss: 0.3770 - acc: 0.852 - ETA: 0s - loss: 0.3788 - acc: 0.851 - 1s 272us/step - loss: 0.3785 - acc: 0.8509 - val_loss: 0.3679 - val_acc: 0.8503\n",
      "Epoch 53/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4666 - acc: 0.750 - ETA: 0s - loss: 0.4012 - acc: 0.835 - ETA: 0s - loss: 0.4135 - acc: 0.847 - ETA: 0s - loss: 0.3996 - acc: 0.855 - ETA: 0s - loss: 0.3909 - acc: 0.860 - ETA: 0s - loss: 0.3898 - acc: 0.855 - ETA: 0s - loss: 0.3855 - acc: 0.853 - ETA: 0s - loss: 0.3853 - acc: 0.849 - ETA: 0s - loss: 0.3826 - acc: 0.853 - ETA: 0s - loss: 0.3813 - acc: 0.853 - ETA: 0s - loss: 0.3831 - acc: 0.851 - ETA: 0s - loss: 0.3809 - acc: 0.851 - ETA: 0s - loss: 0.3789 - acc: 0.851 - ETA: 0s - loss: 0.3771 - acc: 0.851 - ETA: 0s - loss: 0.3802 - acc: 0.850 - ETA: 0s - loss: 0.3800 - acc: 0.850 - ETA: 0s - loss: 0.3818 - acc: 0.848 - 1s 286us/step - loss: 0.3792 - acc: 0.8515 - val_loss: 0.3669 - val_acc: 0.8503\n",
      "Epoch 54/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3765 - acc: 0.812 - ETA: 0s - loss: 0.3598 - acc: 0.847 - ETA: 0s - loss: 0.3973 - acc: 0.841 - ETA: 0s - loss: 0.4156 - acc: 0.842 - ETA: 0s - loss: 0.4136 - acc: 0.840 - ETA: 0s - loss: 0.4291 - acc: 0.831 - ETA: 0s - loss: 0.4108 - acc: 0.838 - ETA: 0s - loss: 0.4027 - acc: 0.839 - ETA: 0s - loss: 0.3972 - acc: 0.843 - ETA: 0s - loss: 0.3905 - acc: 0.846 - ETA: 0s - loss: 0.3846 - acc: 0.849 - ETA: 0s - loss: 0.3815 - acc: 0.853 - ETA: 0s - loss: 0.3823 - acc: 0.853 - ETA: 0s - loss: 0.3792 - acc: 0.854 - 1s 235us/step - loss: 0.3799 - acc: 0.8512 - val_loss: 0.3680 - val_acc: 0.8551\n",
      "Epoch 55/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3320 - acc: 0.937 - ETA: 0s - loss: 0.4344 - acc: 0.820 - ETA: 0s - loss: 0.4050 - acc: 0.832 - ETA: 0s - loss: 0.3881 - acc: 0.845 - ETA: 0s - loss: 0.3998 - acc: 0.838 - ETA: 0s - loss: 0.3926 - acc: 0.845 - ETA: 0s - loss: 0.3845 - acc: 0.847 - ETA: 0s - loss: 0.3770 - acc: 0.851 - ETA: 0s - loss: 0.3719 - acc: 0.854 - ETA: 0s - loss: 0.3707 - acc: 0.854 - ETA: 0s - loss: 0.3677 - acc: 0.856 - ETA: 0s - loss: 0.3706 - acc: 0.850 - ETA: 0s - loss: 0.3715 - acc: 0.850 - 1s 226us/step - loss: 0.3724 - acc: 0.8506 - val_loss: 0.3667 - val_acc: 0.8539\n",
      "Epoch 56/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4084 - acc: 0.875 - ETA: 0s - loss: 0.3759 - acc: 0.855 - ETA: 0s - loss: 0.3854 - acc: 0.846 - ETA: 0s - loss: 0.3768 - acc: 0.856 - ETA: 0s - loss: 0.3749 - acc: 0.863 - ETA: 0s - loss: 0.3651 - acc: 0.864 - ETA: 0s - loss: 0.3682 - acc: 0.862 - ETA: 0s - loss: 0.3708 - acc: 0.861 - ETA: 0s - loss: 0.3718 - acc: 0.859 - ETA: 0s - loss: 0.3727 - acc: 0.854 - ETA: 0s - loss: 0.3677 - acc: 0.856 - ETA: 0s - loss: 0.3702 - acc: 0.855 - ETA: 0s - loss: 0.3711 - acc: 0.855 - ETA: 0s - loss: 0.3694 - acc: 0.855 - ETA: 0s - loss: 0.3731 - acc: 0.854 - ETA: 0s - loss: 0.3771 - acc: 0.851 - ETA: 0s - loss: 0.3774 - acc: 0.852 - ETA: 0s - loss: 0.3761 - acc: 0.853 - 1s 293us/step - loss: 0.3761 - acc: 0.8529 - val_loss: 0.3678 - val_acc: 0.8539\n",
      "Epoch 57/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2109 - acc: 0.906 - ETA: 0s - loss: 0.3585 - acc: 0.866 - ETA: 0s - loss: 0.3616 - acc: 0.856 - ETA: 0s - loss: 0.3429 - acc: 0.870 - ETA: 0s - loss: 0.3656 - acc: 0.861 - ETA: 0s - loss: 0.3634 - acc: 0.859 - ETA: 0s - loss: 0.3674 - acc: 0.856 - ETA: 0s - loss: 0.3735 - acc: 0.853 - ETA: 0s - loss: 0.3723 - acc: 0.854 - ETA: 0s - loss: 0.3692 - acc: 0.856 - ETA: 0s - loss: 0.3720 - acc: 0.856 - ETA: 0s - loss: 0.3784 - acc: 0.855 - ETA: 0s - loss: 0.3784 - acc: 0.852 - ETA: 0s - loss: 0.3750 - acc: 0.855 - ETA: 0s - loss: 0.3739 - acc: 0.853 - ETA: 0s - loss: 0.3732 - acc: 0.854 - ETA: 0s - loss: 0.3758 - acc: 0.852 - 1s 281us/step - loss: 0.3752 - acc: 0.8532 - val_loss: 0.3670 - val_acc: 0.8599\n",
      "Epoch 58/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2447 - acc: 0.906 - ETA: 0s - loss: 0.4030 - acc: 0.825 - ETA: 0s - loss: 0.3835 - acc: 0.825 - ETA: 0s - loss: 0.3895 - acc: 0.826 - ETA: 0s - loss: 0.3858 - acc: 0.830 - ETA: 0s - loss: 0.3637 - acc: 0.848 - ETA: 0s - loss: 0.3594 - acc: 0.851 - ETA: 0s - loss: 0.3656 - acc: 0.849 - ETA: 0s - loss: 0.3704 - acc: 0.848 - ETA: 0s - loss: 0.3648 - acc: 0.853 - ETA: 0s - loss: 0.3688 - acc: 0.853 - ETA: 0s - loss: 0.3671 - acc: 0.855 - ETA: 0s - loss: 0.3676 - acc: 0.855 - ETA: 0s - loss: 0.3700 - acc: 0.853 - ETA: 0s - loss: 0.3695 - acc: 0.854 - ETA: 0s - loss: 0.3734 - acc: 0.853 - 1s 270us/step - loss: 0.3736 - acc: 0.8529 - val_loss: 0.3657 - val_acc: 0.8551\n",
      "Epoch 59/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2807 - acc: 0.937 - ETA: 0s - loss: 0.3644 - acc: 0.843 - ETA: 0s - loss: 0.3551 - acc: 0.851 - ETA: 0s - loss: 0.3868 - acc: 0.847 - ETA: 0s - loss: 0.3844 - acc: 0.847 - ETA: 0s - loss: 0.3852 - acc: 0.844 - ETA: 0s - loss: 0.3865 - acc: 0.847 - ETA: 0s - loss: 0.3823 - acc: 0.848 - ETA: 0s - loss: 0.3822 - acc: 0.847 - ETA: 0s - loss: 0.3818 - acc: 0.850 - ETA: 0s - loss: 0.3789 - acc: 0.852 - ETA: 0s - loss: 0.3815 - acc: 0.852 - ETA: 0s - loss: 0.3878 - acc: 0.849 - ETA: 0s - loss: 0.3831 - acc: 0.852 - ETA: 0s - loss: 0.3815 - acc: 0.852 - ETA: 0s - loss: 0.3779 - acc: 0.852 - ETA: 0s - loss: 0.3812 - acc: 0.851 - 1s 278us/step - loss: 0.3765 - acc: 0.8524 - val_loss: 0.3692 - val_acc: 0.8515\n",
      "Epoch 60/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4657 - acc: 0.812 - ETA: 0s - loss: 0.3761 - acc: 0.859 - ETA: 0s - loss: 0.3892 - acc: 0.860 - ETA: 0s - loss: 0.3797 - acc: 0.863 - ETA: 0s - loss: 0.3820 - acc: 0.857 - ETA: 0s - loss: 0.3801 - acc: 0.851 - ETA: 0s - loss: 0.3701 - acc: 0.855 - ETA: 0s - loss: 0.3720 - acc: 0.855 - ETA: 0s - loss: 0.3728 - acc: 0.854 - ETA: 0s - loss: 0.3723 - acc: 0.853 - ETA: 0s - loss: 0.3724 - acc: 0.854 - ETA: 0s - loss: 0.3693 - acc: 0.858 - ETA: 0s - loss: 0.3696 - acc: 0.859 - ETA: 0s - loss: 0.3703 - acc: 0.858 - ETA: 0s - loss: 0.3724 - acc: 0.857 - ETA: 0s - loss: 0.3730 - acc: 0.857 - 1s 269us/step - loss: 0.3743 - acc: 0.8580 - val_loss: 0.3680 - val_acc: 0.8563\n",
      "Epoch 61/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2598 - acc: 0.906 - ETA: 0s - loss: 0.4142 - acc: 0.824 - ETA: 0s - loss: 0.4047 - acc: 0.831 - ETA: 0s - loss: 0.3892 - acc: 0.839 - ETA: 0s - loss: 0.3999 - acc: 0.834 - ETA: 0s - loss: 0.3873 - acc: 0.844 - ETA: 0s - loss: 0.3877 - acc: 0.843 - ETA: 0s - loss: 0.3848 - acc: 0.846 - ETA: 0s - loss: 0.3789 - acc: 0.849 - ETA: 0s - loss: 0.3731 - acc: 0.854 - ETA: 0s - loss: 0.3769 - acc: 0.852 - ETA: 0s - loss: 0.3811 - acc: 0.851 - ETA: 0s - loss: 0.3800 - acc: 0.850 - ETA: 0s - loss: 0.3736 - acc: 0.854 - ETA: 0s - loss: 0.3682 - acc: 0.857 - ETA: 0s - loss: 0.3684 - acc: 0.856 - 1s 276us/step - loss: 0.3727 - acc: 0.8535 - val_loss: 0.3646 - val_acc: 0.8551\n",
      "Epoch 62/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2582 - acc: 0.906 - ETA: 0s - loss: 0.3581 - acc: 0.854 - ETA: 0s - loss: 0.3826 - acc: 0.853 - ETA: 0s - loss: 0.3765 - acc: 0.850 - ETA: 0s - loss: 0.3843 - acc: 0.851 - ETA: 0s - loss: 0.3846 - acc: 0.847 - ETA: 0s - loss: 0.3862 - acc: 0.844 - ETA: 0s - loss: 0.3805 - acc: 0.845 - ETA: 0s - loss: 0.3727 - acc: 0.849 - ETA: 0s - loss: 0.3759 - acc: 0.850 - ETA: 0s - loss: 0.3681 - acc: 0.854 - ETA: 0s - loss: 0.3704 - acc: 0.851 - ETA: 0s - loss: 0.3703 - acc: 0.853 - ETA: 0s - loss: 0.3749 - acc: 0.851 - ETA: 0s - loss: 0.3768 - acc: 0.849 - ETA: 0s - loss: 0.3753 - acc: 0.853 - ETA: 0s - loss: 0.3730 - acc: 0.855 - 1s 282us/step - loss: 0.3735 - acc: 0.8547 - val_loss: 0.3638 - val_acc: 0.8491\n",
      "Epoch 63/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3606 - acc: 0.750 - ETA: 0s - loss: 0.3079 - acc: 0.871 - ETA: 0s - loss: 0.3426 - acc: 0.859 - ETA: 0s - loss: 0.3316 - acc: 0.862 - ETA: 0s - loss: 0.3464 - acc: 0.857 - ETA: 0s - loss: 0.3417 - acc: 0.863 - ETA: 0s - loss: 0.3492 - acc: 0.863 - ETA: 0s - loss: 0.3555 - acc: 0.860 - ETA: 0s - loss: 0.3548 - acc: 0.861 - ETA: 0s - loss: 0.3578 - acc: 0.864 - ETA: 0s - loss: 0.3571 - acc: 0.866 - ETA: 0s - loss: 0.3620 - acc: 0.862 - ETA: 0s - loss: 0.3630 - acc: 0.862 - ETA: 0s - loss: 0.3639 - acc: 0.861 - ETA: 0s - loss: 0.3705 - acc: 0.859 - ETA: 0s - loss: 0.3698 - acc: 0.858 - 1s 272us/step - loss: 0.3707 - acc: 0.8583 - val_loss: 0.3634 - val_acc: 0.8467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2714 - acc: 0.906 - ETA: 0s - loss: 0.4671 - acc: 0.852 - ETA: 0s - loss: 0.3989 - acc: 0.872 - ETA: 0s - loss: 0.3852 - acc: 0.870 - ETA: 0s - loss: 0.3972 - acc: 0.858 - ETA: 0s - loss: 0.4072 - acc: 0.852 - ETA: 0s - loss: 0.3983 - acc: 0.850 - ETA: 0s - loss: 0.3941 - acc: 0.849 - ETA: 0s - loss: 0.3875 - acc: 0.850 - ETA: 0s - loss: 0.3861 - acc: 0.851 - ETA: 0s - loss: 0.3879 - acc: 0.846 - ETA: 0s - loss: 0.3815 - acc: 0.848 - ETA: 0s - loss: 0.3854 - acc: 0.847 - ETA: 0s - loss: 0.3846 - acc: 0.848 - ETA: 0s - loss: 0.3836 - acc: 0.848 - ETA: 0s - loss: 0.3806 - acc: 0.850 - ETA: 0s - loss: 0.3742 - acc: 0.852 - 1s 278us/step - loss: 0.3747 - acc: 0.8524 - val_loss: 0.3633 - val_acc: 0.8455\n",
      "Epoch 65/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2379 - acc: 0.968 - ETA: 0s - loss: 0.3519 - acc: 0.875 - ETA: 0s - loss: 0.3567 - acc: 0.862 - ETA: 0s - loss: 0.3690 - acc: 0.853 - ETA: 0s - loss: 0.3821 - acc: 0.847 - ETA: 0s - loss: 0.3797 - acc: 0.840 - ETA: 0s - loss: 0.3866 - acc: 0.839 - ETA: 0s - loss: 0.3814 - acc: 0.843 - ETA: 0s - loss: 0.3767 - acc: 0.846 - ETA: 0s - loss: 0.3750 - acc: 0.846 - ETA: 0s - loss: 0.3790 - acc: 0.844 - ETA: 0s - loss: 0.3784 - acc: 0.847 - ETA: 0s - loss: 0.3816 - acc: 0.846 - ETA: 0s - loss: 0.3822 - acc: 0.846 - ETA: 0s - loss: 0.3728 - acc: 0.851 - ETA: 0s - loss: 0.3707 - acc: 0.854 - ETA: 0s - loss: 0.3691 - acc: 0.854 - ETA: 0s - loss: 0.3720 - acc: 0.852 - 1s 293us/step - loss: 0.3685 - acc: 0.8553 - val_loss: 0.3627 - val_acc: 0.8479\n",
      "Epoch 66/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3808 - acc: 0.875 - ETA: 0s - loss: 0.3354 - acc: 0.866 - ETA: 0s - loss: 0.3730 - acc: 0.868 - ETA: 0s - loss: 0.3602 - acc: 0.870 - ETA: 0s - loss: 0.3742 - acc: 0.859 - ETA: 0s - loss: 0.3769 - acc: 0.855 - ETA: 0s - loss: 0.3630 - acc: 0.860 - ETA: 0s - loss: 0.3672 - acc: 0.857 - ETA: 0s - loss: 0.3721 - acc: 0.856 - ETA: 0s - loss: 0.3709 - acc: 0.858 - ETA: 0s - loss: 0.3702 - acc: 0.857 - ETA: 0s - loss: 0.3721 - acc: 0.858 - ETA: 0s - loss: 0.3722 - acc: 0.856 - ETA: 0s - loss: 0.3737 - acc: 0.856 - ETA: 0s - loss: 0.3731 - acc: 0.854 - ETA: 0s - loss: 0.3797 - acc: 0.854 - ETA: 0s - loss: 0.3776 - acc: 0.855 - ETA: 0s - loss: 0.3748 - acc: 0.855 - 1s 292us/step - loss: 0.3743 - acc: 0.8559 - val_loss: 0.3638 - val_acc: 0.8611\n",
      "Epoch 67/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2999 - acc: 0.875 - ETA: 0s - loss: 0.3716 - acc: 0.855 - ETA: 0s - loss: 0.3873 - acc: 0.843 - ETA: 0s - loss: 0.3683 - acc: 0.842 - ETA: 0s - loss: 0.3999 - acc: 0.831 - ETA: 0s - loss: 0.3956 - acc: 0.839 - ETA: 0s - loss: 0.3991 - acc: 0.840 - ETA: 0s - loss: 0.3895 - acc: 0.846 - ETA: 0s - loss: 0.3858 - acc: 0.846 - ETA: 0s - loss: 0.3872 - acc: 0.845 - ETA: 0s - loss: 0.3864 - acc: 0.846 - ETA: 0s - loss: 0.3844 - acc: 0.847 - ETA: 0s - loss: 0.3805 - acc: 0.850 - ETA: 0s - loss: 0.3765 - acc: 0.852 - ETA: 0s - loss: 0.3765 - acc: 0.854 - ETA: 0s - loss: 0.3750 - acc: 0.854 - ETA: 0s - loss: 0.3724 - acc: 0.856 - 1s 287us/step - loss: 0.3724 - acc: 0.8568 - val_loss: 0.3623 - val_acc: 0.8539\n",
      "Epoch 68/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4284 - acc: 0.843 - ETA: 0s - loss: 0.3401 - acc: 0.870 - ETA: 0s - loss: 0.3631 - acc: 0.859 - ETA: 0s - loss: 0.3575 - acc: 0.865 - ETA: 0s - loss: 0.3547 - acc: 0.859 - ETA: 0s - loss: 0.3572 - acc: 0.860 - ETA: 0s - loss: 0.3655 - acc: 0.858 - ETA: 0s - loss: 0.3612 - acc: 0.860 - ETA: 0s - loss: 0.3644 - acc: 0.858 - ETA: 0s - loss: 0.3635 - acc: 0.859 - ETA: 0s - loss: 0.3620 - acc: 0.857 - ETA: 0s - loss: 0.3656 - acc: 0.858 - ETA: 0s - loss: 0.3702 - acc: 0.856 - ETA: 0s - loss: 0.3704 - acc: 0.855 - ETA: 0s - loss: 0.3671 - acc: 0.857 - ETA: 0s - loss: 0.3724 - acc: 0.854 - 1s 270us/step - loss: 0.3729 - acc: 0.8544 - val_loss: 0.3654 - val_acc: 0.8599\n",
      "Epoch 69/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2953 - acc: 0.875 - ETA: 0s - loss: 0.4055 - acc: 0.847 - ETA: 0s - loss: 0.3709 - acc: 0.854 - ETA: 0s - loss: 0.3633 - acc: 0.855 - ETA: 0s - loss: 0.3515 - acc: 0.860 - ETA: 0s - loss: 0.3638 - acc: 0.854 - ETA: 0s - loss: 0.3630 - acc: 0.857 - ETA: 0s - loss: 0.3631 - acc: 0.859 - ETA: 0s - loss: 0.3566 - acc: 0.863 - ETA: 0s - loss: 0.3606 - acc: 0.858 - ETA: 0s - loss: 0.3619 - acc: 0.857 - ETA: 0s - loss: 0.3620 - acc: 0.857 - ETA: 0s - loss: 0.3625 - acc: 0.856 - ETA: 0s - loss: 0.3698 - acc: 0.854 - ETA: 0s - loss: 0.3702 - acc: 0.855 - ETA: 0s - loss: 0.3670 - acc: 0.856 - ETA: 0s - loss: 0.3672 - acc: 0.856 - 1s 280us/step - loss: 0.3704 - acc: 0.8559 - val_loss: 0.3632 - val_acc: 0.8623\n",
      "Epoch 70/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3425 - acc: 0.875 - ETA: 0s - loss: 0.3637 - acc: 0.867 - ETA: 0s - loss: 0.3588 - acc: 0.850 - ETA: 0s - loss: 0.3652 - acc: 0.848 - ETA: 0s - loss: 0.3744 - acc: 0.844 - ETA: 0s - loss: 0.3614 - acc: 0.849 - ETA: 0s - loss: 0.3724 - acc: 0.846 - ETA: 0s - loss: 0.3651 - acc: 0.851 - ETA: 0s - loss: 0.3603 - acc: 0.857 - ETA: 0s - loss: 0.3634 - acc: 0.859 - ETA: 0s - loss: 0.3635 - acc: 0.858 - ETA: 0s - loss: 0.3659 - acc: 0.858 - ETA: 0s - loss: 0.3671 - acc: 0.857 - ETA: 0s - loss: 0.3678 - acc: 0.855 - ETA: 0s - loss: 0.3702 - acc: 0.855 - ETA: 0s - loss: 0.3710 - acc: 0.853 - ETA: 0s - loss: 0.3679 - acc: 0.854 - 1s 284us/step - loss: 0.3684 - acc: 0.8538 - val_loss: 0.3631 - val_acc: 0.8647\n",
      "Epoch 71/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.5307 - acc: 0.843 - ETA: 0s - loss: 0.3843 - acc: 0.863 - ETA: 0s - loss: 0.3731 - acc: 0.868 - ETA: 0s - loss: 0.3709 - acc: 0.863 - ETA: 0s - loss: 0.3809 - acc: 0.857 - ETA: 0s - loss: 0.3652 - acc: 0.863 - ETA: 0s - loss: 0.3611 - acc: 0.867 - ETA: 0s - loss: 0.3652 - acc: 0.863 - ETA: 0s - loss: 0.3659 - acc: 0.861 - ETA: 0s - loss: 0.3686 - acc: 0.860 - ETA: 0s - loss: 0.3628 - acc: 0.861 - ETA: 0s - loss: 0.3694 - acc: 0.859 - ETA: 0s - loss: 0.3697 - acc: 0.858 - ETA: 0s - loss: 0.3722 - acc: 0.854 - ETA: 0s - loss: 0.3723 - acc: 0.852 - ETA: 0s - loss: 0.3716 - acc: 0.853 - 1s 277us/step - loss: 0.3701 - acc: 0.8544 - val_loss: 0.3614 - val_acc: 0.8587\n",
      "Epoch 72/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2962 - acc: 0.875 - ETA: 0s - loss: 0.3632 - acc: 0.863 - ETA: 0s - loss: 0.3529 - acc: 0.860 - ETA: 0s - loss: 0.3712 - acc: 0.855 - ETA: 0s - loss: 0.3722 - acc: 0.853 - ETA: 0s - loss: 0.3708 - acc: 0.858 - ETA: 0s - loss: 0.3741 - acc: 0.852 - ETA: 0s - loss: 0.3695 - acc: 0.857 - ETA: 0s - loss: 0.3686 - acc: 0.856 - ETA: 0s - loss: 0.3683 - acc: 0.859 - ETA: 0s - loss: 0.3737 - acc: 0.858 - ETA: 0s - loss: 0.3746 - acc: 0.856 - ETA: 0s - loss: 0.3735 - acc: 0.856 - ETA: 0s - loss: 0.3718 - acc: 0.856 - ETA: 0s - loss: 0.3696 - acc: 0.857 - ETA: 0s - loss: 0.3666 - acc: 0.859 - ETA: 0s - loss: 0.3652 - acc: 0.860 - 1s 287us/step - loss: 0.3693 - acc: 0.8583 - val_loss: 0.3624 - val_acc: 0.8623\n",
      "Epoch 73/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.5550 - acc: 0.718 - ETA: 0s - loss: 0.4454 - acc: 0.832 - ETA: 0s - loss: 0.4124 - acc: 0.850 - ETA: 0s - loss: 0.4057 - acc: 0.848 - ETA: 0s - loss: 0.3993 - acc: 0.853 - ETA: 0s - loss: 0.3829 - acc: 0.858 - ETA: 0s - loss: 0.3903 - acc: 0.855 - ETA: 0s - loss: 0.3761 - acc: 0.856 - ETA: 0s - loss: 0.3759 - acc: 0.857 - ETA: 0s - loss: 0.3776 - acc: 0.858 - ETA: 0s - loss: 0.3856 - acc: 0.856 - ETA: 0s - loss: 0.3870 - acc: 0.853 - ETA: 0s - loss: 0.3781 - acc: 0.856 - ETA: 0s - loss: 0.3729 - acc: 0.858 - ETA: 0s - loss: 0.3740 - acc: 0.856 - ETA: 0s - loss: 0.3722 - acc: 0.856 - ETA: 0s - loss: 0.3734 - acc: 0.856 - 1s 287us/step - loss: 0.3731 - acc: 0.8562 - val_loss: 0.3706 - val_acc: 0.8611\n",
      "Epoch 74/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3221 - acc: 0.875 - ETA: 0s - loss: 0.3655 - acc: 0.852 - ETA: 0s - loss: 0.3705 - acc: 0.853 - ETA: 0s - loss: 0.3502 - acc: 0.856 - ETA: 0s - loss: 0.3550 - acc: 0.850 - ETA: 0s - loss: 0.3473 - acc: 0.856 - ETA: 0s - loss: 0.3523 - acc: 0.852 - ETA: 0s - loss: 0.3489 - acc: 0.854 - ETA: 0s - loss: 0.3578 - acc: 0.853 - ETA: 0s - loss: 0.3620 - acc: 0.853 - ETA: 0s - loss: 0.3607 - acc: 0.855 - ETA: 0s - loss: 0.3631 - acc: 0.856 - ETA: 0s - loss: 0.3637 - acc: 0.856 - ETA: 0s - loss: 0.3637 - acc: 0.859 - ETA: 0s - loss: 0.3636 - acc: 0.859 - ETA: 0s - loss: 0.3677 - acc: 0.858 - 1s 269us/step - loss: 0.3672 - acc: 0.8574 - val_loss: 0.3603 - val_acc: 0.8503\n",
      "Epoch 75/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3353 - acc: 0.906 - ETA: 0s - loss: 0.3351 - acc: 0.863 - ETA: 0s - loss: 0.3127 - acc: 0.872 - ETA: 0s - loss: 0.3485 - acc: 0.858 - ETA: 0s - loss: 0.3549 - acc: 0.867 - ETA: 0s - loss: 0.3580 - acc: 0.864 - ETA: 0s - loss: 0.3550 - acc: 0.860 - ETA: 0s - loss: 0.3531 - acc: 0.862 - ETA: 0s - loss: 0.3513 - acc: 0.861 - ETA: 0s - loss: 0.3564 - acc: 0.862 - ETA: 0s - loss: 0.3563 - acc: 0.863 - ETA: 0s - loss: 0.3640 - acc: 0.864 - ETA: 0s - loss: 0.3736 - acc: 0.857 - ETA: 0s - loss: 0.3717 - acc: 0.857 - ETA: 0s - loss: 0.3709 - acc: 0.857 - ETA: 0s - loss: 0.3735 - acc: 0.854 - ETA: 0s - loss: 0.3738 - acc: 0.853 - 1s 285us/step - loss: 0.3751 - acc: 0.8527 - val_loss: 0.3610 - val_acc: 0.8635\n",
      "Epoch 76/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3792 - acc: 0.875 - ETA: 0s - loss: 0.3738 - acc: 0.834 - ETA: 0s - loss: 0.3489 - acc: 0.854 - ETA: 0s - loss: 0.3320 - acc: 0.865 - ETA: 0s - loss: 0.3459 - acc: 0.861 - ETA: 0s - loss: 0.3613 - acc: 0.854 - ETA: 0s - loss: 0.3594 - acc: 0.859 - ETA: 0s - loss: 0.3604 - acc: 0.857 - ETA: 0s - loss: 0.3687 - acc: 0.858 - ETA: 0s - loss: 0.3661 - acc: 0.859 - ETA: 0s - loss: 0.3716 - acc: 0.857 - ETA: 0s - loss: 0.3741 - acc: 0.853 - ETA: 0s - loss: 0.3744 - acc: 0.855 - ETA: 0s - loss: 0.3707 - acc: 0.859 - ETA: 0s - loss: 0.3737 - acc: 0.856 - ETA: 0s - loss: 0.3717 - acc: 0.857 - ETA: 0s - loss: 0.3699 - acc: 0.857 - 1s 286us/step - loss: 0.3679 - acc: 0.8583 - val_loss: 0.3603 - val_acc: 0.8575\n",
      "Epoch 77/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3871 - acc: 0.843 - ETA: 0s - loss: 0.3255 - acc: 0.878 - ETA: 0s - loss: 0.3648 - acc: 0.854 - ETA: 0s - loss: 0.3774 - acc: 0.848 - ETA: 0s - loss: 0.3656 - acc: 0.855 - ETA: 0s - loss: 0.3622 - acc: 0.863 - ETA: 0s - loss: 0.3708 - acc: 0.860 - ETA: 0s - loss: 0.3670 - acc: 0.863 - ETA: 0s - loss: 0.3710 - acc: 0.862 - ETA: 0s - loss: 0.3724 - acc: 0.860 - ETA: 0s - loss: 0.3669 - acc: 0.863 - ETA: 0s - loss: 0.3631 - acc: 0.864 - ETA: 0s - loss: 0.3570 - acc: 0.867 - ETA: 0s - loss: 0.3638 - acc: 0.862 - ETA: 0s - loss: 0.3674 - acc: 0.860 - ETA: 0s - loss: 0.3687 - acc: 0.861 - ETA: 0s - loss: 0.3662 - acc: 0.863 - 1s 284us/step - loss: 0.3659 - acc: 0.8634 - val_loss: 0.3629 - val_acc: 0.8671\n",
      "Epoch 78/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3249 - acc: 0.781 - ETA: 0s - loss: 0.3662 - acc: 0.840 - ETA: 0s - loss: 0.3576 - acc: 0.864 - ETA: 0s - loss: 0.3680 - acc: 0.855 - ETA: 0s - loss: 0.3719 - acc: 0.847 - ETA: 0s - loss: 0.3778 - acc: 0.849 - ETA: 0s - loss: 0.3781 - acc: 0.850 - ETA: 0s - loss: 0.3719 - acc: 0.854 - ETA: 0s - loss: 0.3693 - acc: 0.853 - ETA: 0s - loss: 0.3595 - acc: 0.858 - ETA: 0s - loss: 0.3635 - acc: 0.857 - ETA: 0s - loss: 0.3670 - acc: 0.853 - ETA: 0s - loss: 0.3655 - acc: 0.856 - 1s 221us/step - loss: 0.3638 - acc: 0.8568 - val_loss: 0.3591 - val_acc: 0.8563\n",
      "Epoch 79/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3081 - acc: 0.906 - ETA: 0s - loss: 0.3526 - acc: 0.875 - ETA: 0s - loss: 0.3697 - acc: 0.862 - ETA: 0s - loss: 0.3721 - acc: 0.866 - ETA: 0s - loss: 0.3653 - acc: 0.869 - ETA: 0s - loss: 0.3617 - acc: 0.866 - ETA: 0s - loss: 0.3563 - acc: 0.866 - ETA: 0s - loss: 0.3565 - acc: 0.868 - ETA: 0s - loss: 0.3625 - acc: 0.864 - ETA: 0s - loss: 0.3617 - acc: 0.864 - ETA: 0s - loss: 0.3646 - acc: 0.864 - ETA: 0s - loss: 0.3600 - acc: 0.863 - ETA: 0s - loss: 0.3623 - acc: 0.862 - ETA: 0s - loss: 0.3642 - acc: 0.860 - 1s 238us/step - loss: 0.3661 - acc: 0.8598 - val_loss: 0.3590 - val_acc: 0.8563\n",
      "Epoch 80/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4782 - acc: 0.843 - ETA: 0s - loss: 0.3954 - acc: 0.871 - ETA: 0s - loss: 0.3972 - acc: 0.859 - ETA: 0s - loss: 0.3944 - acc: 0.851 - ETA: 0s - loss: 0.3766 - acc: 0.858 - ETA: 0s - loss: 0.3765 - acc: 0.862 - ETA: 0s - loss: 0.3736 - acc: 0.862 - ETA: 0s - loss: 0.3752 - acc: 0.863 - ETA: 0s - loss: 0.3745 - acc: 0.860 - ETA: 0s - loss: 0.3661 - acc: 0.865 - ETA: 0s - loss: 0.3687 - acc: 0.863 - ETA: 0s - loss: 0.3722 - acc: 0.859 - ETA: 0s - loss: 0.3737 - acc: 0.857 - ETA: 0s - loss: 0.3697 - acc: 0.857 - ETA: 0s - loss: 0.3672 - acc: 0.858 - 1s 257us/step - loss: 0.3670 - acc: 0.8586 - val_loss: 0.3601 - val_acc: 0.8647\n",
      "Epoch 81/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3856 - acc: 0.875 - ETA: 0s - loss: 0.3452 - acc: 0.875 - ETA: 0s - loss: 0.3639 - acc: 0.868 - ETA: 0s - loss: 0.3639 - acc: 0.866 - ETA: 0s - loss: 0.3528 - acc: 0.869 - ETA: 0s - loss: 0.3474 - acc: 0.873 - ETA: 0s - loss: 0.3434 - acc: 0.874 - ETA: 0s - loss: 0.3444 - acc: 0.873 - ETA: 0s - loss: 0.3485 - acc: 0.870 - ETA: 0s - loss: 0.3513 - acc: 0.868 - ETA: 0s - loss: 0.3466 - acc: 0.870 - ETA: 0s - loss: 0.3468 - acc: 0.867 - ETA: 0s - loss: 0.3463 - acc: 0.867 - ETA: 0s - loss: 0.3550 - acc: 0.863 - ETA: 0s - loss: 0.3605 - acc: 0.859 - ETA: 0s - loss: 0.3610 - acc: 0.859 - ETA: 0s - loss: 0.3627 - acc: 0.857 - 1s 281us/step - loss: 0.3632 - acc: 0.8559 - val_loss: 0.3636 - val_acc: 0.8731\n",
      "Epoch 82/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.4865 - acc: 0.812 - ETA: 0s - loss: 0.3491 - acc: 0.871 - ETA: 0s - loss: 0.3835 - acc: 0.851 - ETA: 0s - loss: 0.3670 - acc: 0.857 - ETA: 0s - loss: 0.3600 - acc: 0.857 - ETA: 0s - loss: 0.3513 - acc: 0.861 - ETA: 0s - loss: 0.3545 - acc: 0.858 - ETA: 0s - loss: 0.3590 - acc: 0.859 - ETA: 0s - loss: 0.3545 - acc: 0.860 - ETA: 0s - loss: 0.3590 - acc: 0.858 - ETA: 0s - loss: 0.3630 - acc: 0.856 - ETA: 0s - loss: 0.3609 - acc: 0.857 - ETA: 0s - loss: 0.3604 - acc: 0.857 - ETA: 0s - loss: 0.3602 - acc: 0.857 - ETA: 0s - loss: 0.3673 - acc: 0.855 - 1s 260us/step - loss: 0.3648 - acc: 0.8562 - val_loss: 0.3584 - val_acc: 0.8563\n",
      "Epoch 83/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.1657 - acc: 1.000 - ETA: 0s - loss: 0.3565 - acc: 0.861 - ETA: 0s - loss: 0.3526 - acc: 0.865 - ETA: 0s - loss: 0.3404 - acc: 0.869 - ETA: 0s - loss: 0.3578 - acc: 0.862 - ETA: 0s - loss: 0.3539 - acc: 0.861 - ETA: 0s - loss: 0.3549 - acc: 0.861 - ETA: 0s - loss: 0.3625 - acc: 0.862 - ETA: 0s - loss: 0.3654 - acc: 0.862 - ETA: 0s - loss: 0.3696 - acc: 0.863 - ETA: 0s - loss: 0.3676 - acc: 0.862 - ETA: 0s - loss: 0.3696 - acc: 0.858 - ETA: 0s - loss: 0.3684 - acc: 0.860 - ETA: 0s - loss: 0.3682 - acc: 0.861 - ETA: 0s - loss: 0.3646 - acc: 0.861 - ETA: 0s - loss: 0.3647 - acc: 0.861 - 1s 275us/step - loss: 0.3668 - acc: 0.8607 - val_loss: 0.3577 - val_acc: 0.8539\n",
      "Epoch 84/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2996 - acc: 0.906 - ETA: 0s - loss: 0.3657 - acc: 0.851 - ETA: 0s - loss: 0.3820 - acc: 0.860 - ETA: 0s - loss: 0.3512 - acc: 0.868 - ETA: 0s - loss: 0.3470 - acc: 0.867 - ETA: 0s - loss: 0.3473 - acc: 0.864 - ETA: 0s - loss: 0.3485 - acc: 0.866 - ETA: 0s - loss: 0.3644 - acc: 0.860 - ETA: 0s - loss: 0.3654 - acc: 0.861 - ETA: 0s - loss: 0.3625 - acc: 0.860 - ETA: 0s - loss: 0.3676 - acc: 0.857 - ETA: 0s - loss: 0.3679 - acc: 0.853 - ETA: 0s - loss: 0.3616 - acc: 0.856 - ETA: 0s - loss: 0.3653 - acc: 0.854 - ETA: 0s - loss: 0.3645 - acc: 0.855 - ETA: 0s - loss: 0.3627 - acc: 0.856 - 1s 275us/step - loss: 0.3615 - acc: 0.8580 - val_loss: 0.3614 - val_acc: 0.8683\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2769 - acc: 0.875 - ETA: 0s - loss: 0.3969 - acc: 0.832 - ETA: 0s - loss: 0.3570 - acc: 0.856 - ETA: 0s - loss: 0.3548 - acc: 0.858 - ETA: 0s - loss: 0.3627 - acc: 0.854 - ETA: 0s - loss: 0.3654 - acc: 0.855 - ETA: 0s - loss: 0.3697 - acc: 0.852 - ETA: 0s - loss: 0.3659 - acc: 0.855 - ETA: 0s - loss: 0.3661 - acc: 0.856 - ETA: 0s - loss: 0.3717 - acc: 0.855 - ETA: 0s - loss: 0.3785 - acc: 0.850 - ETA: 0s - loss: 0.3766 - acc: 0.852 - ETA: 0s - loss: 0.3711 - acc: 0.855 - ETA: 0s - loss: 0.3679 - acc: 0.856 - ETA: 0s - loss: 0.3637 - acc: 0.858 - ETA: 0s - loss: 0.3627 - acc: 0.859 - ETA: 0s - loss: 0.3618 - acc: 0.859 - 1s 288us/step - loss: 0.3623 - acc: 0.8589 - val_loss: 0.3572 - val_acc: 0.8575\n",
      "Epoch 86/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.3496 - acc: 0.875 - ETA: 0s - loss: 0.3893 - acc: 0.835 - ETA: 0s - loss: 0.3749 - acc: 0.845 - ETA: 0s - loss: 0.3926 - acc: 0.840 - ETA: 0s - loss: 0.3991 - acc: 0.833 - ETA: 0s - loss: 0.3791 - acc: 0.847 - ETA: 0s - loss: 0.3786 - acc: 0.850 - ETA: 0s - loss: 0.3818 - acc: 0.849 - ETA: 0s - loss: 0.3704 - acc: 0.852 - ETA: 0s - loss: 0.3718 - acc: 0.853 - ETA: 0s - loss: 0.3699 - acc: 0.855 - ETA: 0s - loss: 0.3674 - acc: 0.858 - ETA: 0s - loss: 0.3660 - acc: 0.858 - ETA: 0s - loss: 0.3653 - acc: 0.858 - ETA: 0s - loss: 0.3633 - acc: 0.857 - ETA: 0s - loss: 0.3636 - acc: 0.855 - ETA: 0s - loss: 0.3612 - acc: 0.856 - 1s 288us/step - loss: 0.3631 - acc: 0.8583 - val_loss: 0.3593 - val_acc: 0.8671\n",
      "Epoch 87/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3968 - acc: 0.906 - ETA: 0s - loss: 0.3527 - acc: 0.878 - ETA: 0s - loss: 0.3605 - acc: 0.859 - ETA: 0s - loss: 0.3859 - acc: 0.845 - ETA: 0s - loss: 0.3811 - acc: 0.850 - ETA: 0s - loss: 0.3688 - acc: 0.855 - ETA: 0s - loss: 0.3848 - acc: 0.853 - ETA: 0s - loss: 0.3754 - acc: 0.855 - ETA: 0s - loss: 0.3734 - acc: 0.855 - ETA: 0s - loss: 0.3744 - acc: 0.855 - ETA: 0s - loss: 0.3699 - acc: 0.857 - ETA: 0s - loss: 0.3709 - acc: 0.858 - ETA: 0s - loss: 0.3676 - acc: 0.859 - ETA: 0s - loss: 0.3638 - acc: 0.861 - ETA: 0s - loss: 0.3644 - acc: 0.860 - ETA: 0s - loss: 0.3625 - acc: 0.861 - ETA: 0s - loss: 0.3638 - acc: 0.860 - 1s 284us/step - loss: 0.3648 - acc: 0.8607 - val_loss: 0.3580 - val_acc: 0.8647\n",
      "Epoch 88/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.1686 - acc: 0.968 - ETA: 0s - loss: 0.3867 - acc: 0.871 - ETA: 0s - loss: 0.3967 - acc: 0.868 - ETA: 0s - loss: 0.3795 - acc: 0.871 - ETA: 0s - loss: 0.3746 - acc: 0.870 - ETA: 0s - loss: 0.3645 - acc: 0.871 - ETA: 0s - loss: 0.3547 - acc: 0.871 - ETA: 0s - loss: 0.3537 - acc: 0.872 - ETA: 0s - loss: 0.3677 - acc: 0.867 - ETA: 0s - loss: 0.3712 - acc: 0.860 - ETA: 0s - loss: 0.3738 - acc: 0.861 - ETA: 0s - loss: 0.3717 - acc: 0.861 - ETA: 0s - loss: 0.3688 - acc: 0.863 - ETA: 0s - loss: 0.3676 - acc: 0.865 - ETA: 0s - loss: 0.3645 - acc: 0.866 - ETA: 0s - loss: 0.3643 - acc: 0.865 - ETA: 0s - loss: 0.3624 - acc: 0.866 - 1s 289us/step - loss: 0.3678 - acc: 0.8634 - val_loss: 0.3616 - val_acc: 0.8683\n",
      "Epoch 89/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2325 - acc: 0.937 - ETA: 0s - loss: 0.3531 - acc: 0.839 - ETA: 0s - loss: 0.3429 - acc: 0.854 - ETA: 0s - loss: 0.3516 - acc: 0.850 - ETA: 0s - loss: 0.3374 - acc: 0.862 - ETA: 0s - loss: 0.3508 - acc: 0.855 - ETA: 0s - loss: 0.3468 - acc: 0.855 - ETA: 0s - loss: 0.3479 - acc: 0.854 - ETA: 0s - loss: 0.3596 - acc: 0.850 - ETA: 0s - loss: 0.3594 - acc: 0.850 - ETA: 0s - loss: 0.3633 - acc: 0.847 - ETA: 0s - loss: 0.3734 - acc: 0.845 - ETA: 0s - loss: 0.3646 - acc: 0.849 - ETA: 0s - loss: 0.3683 - acc: 0.849 - ETA: 0s - loss: 0.3683 - acc: 0.851 - ETA: 0s - loss: 0.3640 - acc: 0.854 - ETA: 0s - loss: 0.3648 - acc: 0.856 - 1s 287us/step - loss: 0.3604 - acc: 0.8586 - val_loss: 0.3556 - val_acc: 0.8563\n",
      "Epoch 90/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2247 - acc: 0.937 - ETA: 0s - loss: 0.3837 - acc: 0.843 - ETA: 0s - loss: 0.3859 - acc: 0.839 - ETA: 0s - loss: 0.3663 - acc: 0.853 - ETA: 0s - loss: 0.3699 - acc: 0.857 - ETA: 0s - loss: 0.3640 - acc: 0.859 - ETA: 0s - loss: 0.3591 - acc: 0.855 - ETA: 0s - loss: 0.3599 - acc: 0.857 - ETA: 0s - loss: 0.3596 - acc: 0.859 - ETA: 0s - loss: 0.3605 - acc: 0.857 - ETA: 0s - loss: 0.3619 - acc: 0.858 - ETA: 0s - loss: 0.3642 - acc: 0.858 - ETA: 0s - loss: 0.3670 - acc: 0.855 - ETA: 0s - loss: 0.3643 - acc: 0.856 - ETA: 0s - loss: 0.3632 - acc: 0.856 - ETA: 0s - loss: 0.3591 - acc: 0.857 - ETA: 0s - loss: 0.3619 - acc: 0.856 - 1s 285us/step - loss: 0.3620 - acc: 0.8562 - val_loss: 0.3568 - val_acc: 0.8659\n",
      "Epoch 91/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2923 - acc: 0.906 - ETA: 0s - loss: 0.3492 - acc: 0.870 - ETA: 0s - loss: 0.3508 - acc: 0.865 - ETA: 0s - loss: 0.3635 - acc: 0.866 - ETA: 0s - loss: 0.3676 - acc: 0.862 - ETA: 0s - loss: 0.3690 - acc: 0.859 - ETA: 0s - loss: 0.3640 - acc: 0.862 - ETA: 0s - loss: 0.3649 - acc: 0.859 - ETA: 0s - loss: 0.3628 - acc: 0.859 - ETA: 0s - loss: 0.3629 - acc: 0.859 - ETA: 0s - loss: 0.3655 - acc: 0.858 - ETA: 0s - loss: 0.3676 - acc: 0.858 - ETA: 0s - loss: 0.3647 - acc: 0.858 - ETA: 0s - loss: 0.3661 - acc: 0.858 - ETA: 0s - loss: 0.3661 - acc: 0.857 - ETA: 0s - loss: 0.3667 - acc: 0.857 - ETA: 0s - loss: 0.3655 - acc: 0.858 - 1s 292us/step - loss: 0.3647 - acc: 0.8592 - val_loss: 0.3577 - val_acc: 0.8659\n",
      "Epoch 92/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3280 - acc: 0.843 - ETA: 0s - loss: 0.3276 - acc: 0.882 - ETA: 0s - loss: 0.3156 - acc: 0.888 - ETA: 0s - loss: 0.3436 - acc: 0.875 - ETA: 0s - loss: 0.3420 - acc: 0.872 - ETA: 0s - loss: 0.3463 - acc: 0.869 - ETA: 0s - loss: 0.3460 - acc: 0.868 - ETA: 0s - loss: 0.3484 - acc: 0.866 - ETA: 0s - loss: 0.3557 - acc: 0.865 - ETA: 0s - loss: 0.3561 - acc: 0.866 - ETA: 0s - loss: 0.3569 - acc: 0.863 - ETA: 0s - loss: 0.3544 - acc: 0.864 - ETA: 0s - loss: 0.3582 - acc: 0.860 - ETA: 0s - loss: 0.3568 - acc: 0.862 - ETA: 0s - loss: 0.3600 - acc: 0.860 - ETA: 0s - loss: 0.3596 - acc: 0.861 - ETA: 0s - loss: 0.3584 - acc: 0.861 - 1s 290us/step - loss: 0.3606 - acc: 0.8613 - val_loss: 0.3573 - val_acc: 0.8671\n",
      "Epoch 93/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.4574 - acc: 0.750 - ETA: 0s - loss: 0.3472 - acc: 0.855 - ETA: 0s - loss: 0.3706 - acc: 0.856 - ETA: 0s - loss: 0.3543 - acc: 0.866 - ETA: 0s - loss: 0.3593 - acc: 0.865 - ETA: 0s - loss: 0.3616 - acc: 0.863 - ETA: 0s - loss: 0.3530 - acc: 0.867 - ETA: 0s - loss: 0.3537 - acc: 0.868 - ETA: 0s - loss: 0.3478 - acc: 0.868 - ETA: 0s - loss: 0.3525 - acc: 0.864 - ETA: 0s - loss: 0.3562 - acc: 0.863 - ETA: 0s - loss: 0.3563 - acc: 0.863 - ETA: 0s - loss: 0.3542 - acc: 0.864 - ETA: 0s - loss: 0.3592 - acc: 0.862 - ETA: 0s - loss: 0.3639 - acc: 0.861 - ETA: 0s - loss: 0.3601 - acc: 0.862 - ETA: 0s - loss: 0.3584 - acc: 0.862 - 1s 284us/step - loss: 0.3587 - acc: 0.8625 - val_loss: 0.3556 - val_acc: 0.8623\n",
      "Epoch 94/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2891 - acc: 0.906 - ETA: 1s - loss: 0.3056 - acc: 0.880 - ETA: 0s - loss: 0.3251 - acc: 0.859 - ETA: 0s - loss: 0.3479 - acc: 0.862 - ETA: 0s - loss: 0.3516 - acc: 0.863 - ETA: 0s - loss: 0.3528 - acc: 0.865 - ETA: 0s - loss: 0.3589 - acc: 0.870 - ETA: 0s - loss: 0.3549 - acc: 0.870 - ETA: 0s - loss: 0.3515 - acc: 0.871 - ETA: 0s - loss: 0.3557 - acc: 0.871 - ETA: 0s - loss: 0.3599 - acc: 0.865 - ETA: 0s - loss: 0.3602 - acc: 0.866 - ETA: 0s - loss: 0.3551 - acc: 0.868 - ETA: 0s - loss: 0.3570 - acc: 0.866 - ETA: 0s - loss: 0.3531 - acc: 0.866 - ETA: 0s - loss: 0.3541 - acc: 0.864 - ETA: 0s - loss: 0.3585 - acc: 0.862 - ETA: 0s - loss: 0.3616 - acc: 0.859 - 1s 293us/step - loss: 0.3612 - acc: 0.8598 - val_loss: 0.3546 - val_acc: 0.8563\n",
      "Epoch 95/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.1808 - acc: 1.000 - ETA: 0s - loss: 0.3329 - acc: 0.863 - ETA: 0s - loss: 0.3404 - acc: 0.860 - ETA: 0s - loss: 0.3433 - acc: 0.867 - ETA: 0s - loss: 0.3691 - acc: 0.863 - ETA: 0s - loss: 0.3670 - acc: 0.859 - ETA: 0s - loss: 0.3675 - acc: 0.860 - ETA: 0s - loss: 0.3635 - acc: 0.859 - ETA: 0s - loss: 0.3615 - acc: 0.861 - ETA: 0s - loss: 0.3644 - acc: 0.860 - ETA: 0s - loss: 0.3644 - acc: 0.860 - ETA: 0s - loss: 0.3614 - acc: 0.860 - ETA: 0s - loss: 0.3586 - acc: 0.861 - ETA: 0s - loss: 0.3603 - acc: 0.860 - ETA: 0s - loss: 0.3591 - acc: 0.862 - ETA: 0s - loss: 0.3634 - acc: 0.859 - 1s 271us/step - loss: 0.3599 - acc: 0.8604 - val_loss: 0.3543 - val_acc: 0.8575\n",
      "Epoch 96/100\n",
      "3339/3339 [==============================] - ETA: 1s - loss: 0.2452 - acc: 0.937 - ETA: 0s - loss: 0.3220 - acc: 0.882 - ETA: 0s - loss: 0.3499 - acc: 0.866 - ETA: 0s - loss: 0.3466 - acc: 0.860 - ETA: 0s - loss: 0.3613 - acc: 0.853 - ETA: 0s - loss: 0.3632 - acc: 0.854 - ETA: 0s - loss: 0.3535 - acc: 0.859 - ETA: 0s - loss: 0.3518 - acc: 0.860 - ETA: 0s - loss: 0.3481 - acc: 0.864 - ETA: 0s - loss: 0.3497 - acc: 0.865 - ETA: 0s - loss: 0.3504 - acc: 0.869 - ETA: 0s - loss: 0.3508 - acc: 0.869 - ETA: 0s - loss: 0.3499 - acc: 0.868 - ETA: 0s - loss: 0.3462 - acc: 0.868 - ETA: 0s - loss: 0.3529 - acc: 0.866 - ETA: 0s - loss: 0.3514 - acc: 0.867 - ETA: 0s - loss: 0.3547 - acc: 0.863 - 1s 282us/step - loss: 0.3572 - acc: 0.8622 - val_loss: 0.3572 - val_acc: 0.8695\n",
      "Epoch 97/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.3747 - acc: 0.843 - ETA: 0s - loss: 0.3538 - acc: 0.894 - ETA: 0s - loss: 0.3704 - acc: 0.870 - ETA: 0s - loss: 0.3650 - acc: 0.872 - ETA: 0s - loss: 0.3659 - acc: 0.867 - ETA: 0s - loss: 0.3620 - acc: 0.866 - ETA: 0s - loss: 0.3655 - acc: 0.865 - ETA: 0s - loss: 0.3662 - acc: 0.863 - ETA: 0s - loss: 0.3702 - acc: 0.858 - ETA: 0s - loss: 0.3690 - acc: 0.858 - ETA: 0s - loss: 0.3765 - acc: 0.858 - ETA: 0s - loss: 0.3716 - acc: 0.859 - ETA: 0s - loss: 0.3715 - acc: 0.859 - ETA: 0s - loss: 0.3695 - acc: 0.860 - ETA: 0s - loss: 0.3646 - acc: 0.862 - ETA: 0s - loss: 0.3630 - acc: 0.862 - ETA: 0s - loss: 0.3602 - acc: 0.863 - 1s 288us/step - loss: 0.3597 - acc: 0.8646 - val_loss: 0.3574 - val_acc: 0.8707\n",
      "Epoch 98/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2850 - acc: 0.843 - ETA: 0s - loss: 0.4097 - acc: 0.839 - ETA: 0s - loss: 0.3846 - acc: 0.848 - ETA: 0s - loss: 0.3666 - acc: 0.861 - ETA: 0s - loss: 0.3460 - acc: 0.868 - ETA: 0s - loss: 0.3376 - acc: 0.874 - ETA: 0s - loss: 0.3475 - acc: 0.867 - ETA: 0s - loss: 0.3576 - acc: 0.862 - ETA: 0s - loss: 0.3591 - acc: 0.863 - ETA: 0s - loss: 0.3632 - acc: 0.862 - ETA: 0s - loss: 0.3619 - acc: 0.862 - ETA: 0s - loss: 0.3608 - acc: 0.861 - ETA: 0s - loss: 0.3619 - acc: 0.860 - ETA: 0s - loss: 0.3637 - acc: 0.860 - ETA: 0s - loss: 0.3573 - acc: 0.861 - ETA: 0s - loss: 0.3643 - acc: 0.857 - ETA: 0s - loss: 0.3608 - acc: 0.860 - ETA: 0s - loss: 0.3590 - acc: 0.861 - 1s 297us/step - loss: 0.3587 - acc: 0.8616 - val_loss: 0.3642 - val_acc: 0.8659\n",
      "Epoch 99/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2377 - acc: 0.937 - ETA: 0s - loss: 0.4039 - acc: 0.870 - ETA: 0s - loss: 0.4279 - acc: 0.855 - ETA: 0s - loss: 0.3939 - acc: 0.865 - ETA: 0s - loss: 0.3958 - acc: 0.858 - ETA: 0s - loss: 0.4001 - acc: 0.855 - ETA: 0s - loss: 0.3853 - acc: 0.864 - ETA: 0s - loss: 0.3847 - acc: 0.859 - ETA: 0s - loss: 0.3775 - acc: 0.858 - ETA: 0s - loss: 0.3778 - acc: 0.860 - ETA: 0s - loss: 0.3761 - acc: 0.861 - ETA: 0s - loss: 0.3734 - acc: 0.861 - ETA: 0s - loss: 0.3706 - acc: 0.863 - ETA: 0s - loss: 0.3698 - acc: 0.859 - ETA: 0s - loss: 0.3669 - acc: 0.860 - ETA: 0s - loss: 0.3633 - acc: 0.861 - ETA: 0s - loss: 0.3595 - acc: 0.863 - ETA: 0s - loss: 0.3590 - acc: 0.864 - 1s 299us/step - loss: 0.3594 - acc: 0.8643 - val_loss: 0.3536 - val_acc: 0.8599\n",
      "Epoch 100/100\n",
      "3339/3339 [==============================] - ETA: 0s - loss: 0.2669 - acc: 0.875 - ETA: 0s - loss: 0.3565 - acc: 0.852 - ETA: 0s - loss: 0.3583 - acc: 0.870 - ETA: 0s - loss: 0.3772 - acc: 0.860 - ETA: 0s - loss: 0.3712 - acc: 0.861 - ETA: 0s - loss: 0.3707 - acc: 0.864 - ETA: 0s - loss: 0.3831 - acc: 0.860 - ETA: 0s - loss: 0.3810 - acc: 0.859 - ETA: 0s - loss: 0.3833 - acc: 0.856 - ETA: 0s - loss: 0.3735 - acc: 0.856 - ETA: 0s - loss: 0.3646 - acc: 0.860 - ETA: 0s - loss: 0.3688 - acc: 0.856 - ETA: 0s - loss: 0.3687 - acc: 0.855 - ETA: 0s - loss: 0.3657 - acc: 0.856 - ETA: 0s - loss: 0.3643 - acc: 0.856 - ETA: 0s - loss: 0.3621 - acc: 0.858 - ETA: 0s - loss: 0.3596 - acc: 0.859 - 1s 287us/step - loss: 0.3584 - acc: 0.8601 - val_loss: 0.3528 - val_acc: 0.8563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de74ec19e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_epochs = 100\n",
    "hidden_size = 10\n",
    "timesteps = 1\n",
    "num_class = 1\n",
    "data_dim = len(train_vecs_physics[0])\n",
    "num_data = len(train_vecs_physics)\n",
    "num_data_test = len(test_vecs_physics)\n",
    "\n",
    "train_vecs_physics = train_vecs_physics.reshape((num_data, timesteps, data_dim))\n",
    "y_train_physics = y_train_physics.reshape((num_data, num_class))\n",
    "test_vecs_physics = test_vecs_physics.reshape((num_data_test, timesteps, data_dim))\n",
    "y_test_physics = y_test_physics.reshape((num_data_test, num_class))\n",
    "\n",
    "model_ad_physics = Sequential()\n",
    "model_ad_physics.add(Bidirectional(LSTM(hidden_size, input_shape=(timesteps, data_dim)), merge_mode='concat'))\n",
    "model_ad_physics.add(Dropout(0.5))\n",
    "model_ad_physics.add(Dense(1, activation='sigmoid'))\n",
    "model_ad_physics.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_ad_physics.fit(train_vecs_physics, y_train_physics, epochs=num_epochs, validation_data=[test_vecs_physics, y_test_physics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8562874251497006 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.89      0.87       431\n",
      "          1       0.88      0.82      0.85       404\n",
      "\n",
      "avg / total       0.86      0.86      0.86       835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "prediction = {}\n",
    "prediction['ad_physics'] = model_ad_physics.predict(test_vecs_physics)\n",
    "\n",
    "for i in range(len(prediction['ad_physics'])):\n",
    "    prediction['ad_physics'][i][0] = round(prediction['ad_physics'][i][0])\n",
    "\n",
    "accuracy = {}\n",
    "accuracy['ad_physics'] = accuracy_score(y_test_physics, prediction['ad_physics'])\n",
    "print(\"Accuracy: \", accuracy['ad_physics'], \"\\n\")\n",
    "print(classification_report(y_test_physics, prediction['ad_physics'], labels = [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM Aspect Detection Model for Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1040 samples, validate on 260 samples\n",
      "Epoch 1/100\n",
      "1040/1040 [==============================] - ETA: 1:29 - loss: 0.6931 - acc: 0.593 - ETA: 10s - loss: 0.6924 - acc: 0.611 - ETA: 5s - loss: 0.6921 - acc: 0.6042 - ETA: 2s - loss: 0.6909 - acc: 0.618 - ETA: 0s - loss: 0.6901 - acc: 0.623 - ETA: 0s - loss: 0.6889 - acc: 0.630 - 3s 3ms/step - loss: 0.6886 - acc: 0.6327 - val_loss: 0.6846 - val_acc: 0.6346\n",
      "Epoch 2/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6847 - acc: 0.625 - ETA: 0s - loss: 0.6816 - acc: 0.656 - ETA: 0s - loss: 0.6824 - acc: 0.640 - ETA: 0s - loss: 0.6805 - acc: 0.651 - ETA: 0s - loss: 0.6819 - acc: 0.633 - ETA: 0s - loss: 0.6808 - acc: 0.634 - 0s 307us/step - loss: 0.6803 - acc: 0.6385 - val_loss: 0.6765 - val_acc: 0.6346\n",
      "Epoch 3/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6856 - acc: 0.562 - ETA: 0s - loss: 0.6759 - acc: 0.632 - ETA: 0s - loss: 0.6722 - acc: 0.645 - ETA: 0s - loss: 0.6706 - acc: 0.651 - ETA: 0s - loss: 0.6730 - acc: 0.635 - ETA: 0s - loss: 0.6720 - acc: 0.635 - 0s 301us/step - loss: 0.6714 - acc: 0.6385 - val_loss: 0.6688 - val_acc: 0.6346\n",
      "Epoch 4/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6780 - acc: 0.593 - ETA: 0s - loss: 0.6647 - acc: 0.644 - ETA: 0s - loss: 0.6640 - acc: 0.643 - ETA: 0s - loss: 0.6676 - acc: 0.628 - ETA: 0s - loss: 0.6651 - acc: 0.636 - 0s 297us/step - loss: 0.6634 - acc: 0.6385 - val_loss: 0.6626 - val_acc: 0.6346\n",
      "Epoch 5/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6504 - acc: 0.656 - ETA: 0s - loss: 0.6545 - acc: 0.656 - ETA: 0s - loss: 0.6649 - acc: 0.631 - ETA: 0s - loss: 0.6600 - acc: 0.641 - ETA: 0s - loss: 0.6578 - acc: 0.645 - 0s 281us/step - loss: 0.6595 - acc: 0.6385 - val_loss: 0.6575 - val_acc: 0.6346\n",
      "Epoch 6/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6516 - acc: 0.656 - ETA: 0s - loss: 0.6563 - acc: 0.642 - ETA: 0s - loss: 0.6468 - acc: 0.663 - ETA: 0s - loss: 0.6535 - acc: 0.640 - ETA: 0s - loss: 0.6579 - acc: 0.630 - ETA: 0s - loss: 0.6551 - acc: 0.637 - 0s 312us/step - loss: 0.6546 - acc: 0.6385 - val_loss: 0.6539 - val_acc: 0.6346\n",
      "Epoch 7/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5836 - acc: 0.812 - ETA: 0s - loss: 0.6622 - acc: 0.604 - ETA: 0s - loss: 0.6630 - acc: 0.602 - ETA: 0s - loss: 0.6514 - acc: 0.631 - ETA: 0s - loss: 0.6537 - acc: 0.626 - ETA: 0s - loss: 0.6490 - acc: 0.638 - 0s 321us/step - loss: 0.6487 - acc: 0.6385 - val_loss: 0.6508 - val_acc: 0.6346\n",
      "Epoch 8/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6001 - acc: 0.750 - ETA: 0s - loss: 0.6260 - acc: 0.684 - ETA: 0s - loss: 0.6306 - acc: 0.676 - ETA: 0s - loss: 0.6376 - acc: 0.660 - ETA: 0s - loss: 0.6459 - acc: 0.643 - ETA: 0s - loss: 0.6468 - acc: 0.641 - 0s 309us/step - loss: 0.6483 - acc: 0.6385 - val_loss: 0.6479 - val_acc: 0.6346\n",
      "Epoch 9/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6078 - acc: 0.718 - ETA: 0s - loss: 0.6206 - acc: 0.699 - ETA: 0s - loss: 0.6396 - acc: 0.651 - ETA: 0s - loss: 0.6392 - acc: 0.650 - ETA: 0s - loss: 0.6483 - acc: 0.629 - 0s 280us/step - loss: 0.6432 - acc: 0.6385 - val_loss: 0.6453 - val_acc: 0.6346\n",
      "Epoch 10/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6068 - acc: 0.718 - ETA: 0s - loss: 0.6291 - acc: 0.671 - ETA: 0s - loss: 0.6325 - acc: 0.661 - ETA: 0s - loss: 0.6405 - acc: 0.639 - ETA: 0s - loss: 0.6403 - acc: 0.641 - 0s 243us/step - loss: 0.6417 - acc: 0.6385 - val_loss: 0.6424 - val_acc: 0.6346\n",
      "Epoch 11/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6596 - acc: 0.593 - ETA: 0s - loss: 0.6357 - acc: 0.642 - ETA: 0s - loss: 0.6281 - acc: 0.654 - ETA: 0s - loss: 0.6297 - acc: 0.652 - ETA: 0s - loss: 0.6374 - acc: 0.639 - 0s 230us/step - loss: 0.6382 - acc: 0.6385 - val_loss: 0.6395 - val_acc: 0.6346\n",
      "Epoch 12/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5950 - acc: 0.718 - ETA: 0s - loss: 0.6462 - acc: 0.618 - ETA: 0s - loss: 0.6316 - acc: 0.646 - ETA: 0s - loss: 0.6325 - acc: 0.644 - 0s 212us/step - loss: 0.6361 - acc: 0.6385 - val_loss: 0.6364 - val_acc: 0.6346\n",
      "Epoch 13/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6841 - acc: 0.531 - ETA: 0s - loss: 0.6352 - acc: 0.628 - ETA: 0s - loss: 0.6429 - acc: 0.617 - ETA: 0s - loss: 0.6379 - acc: 0.627 - ETA: 0s - loss: 0.6337 - acc: 0.635 - 0s 245us/step - loss: 0.6318 - acc: 0.6385 - val_loss: 0.6331 - val_acc: 0.6346\n",
      "Epoch 14/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6012 - acc: 0.687 - ETA: 0s - loss: 0.6183 - acc: 0.653 - ETA: 0s - loss: 0.6249 - acc: 0.642 - ETA: 0s - loss: 0.6271 - acc: 0.636 - 0s 193us/step - loss: 0.6264 - acc: 0.6385 - val_loss: 0.6294 - val_acc: 0.6346\n",
      "Epoch 15/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6819 - acc: 0.531 - ETA: 0s - loss: 0.6470 - acc: 0.596 - ETA: 0s - loss: 0.6297 - acc: 0.621 - ETA: 0s - loss: 0.6242 - acc: 0.633 - 0s 193us/step - loss: 0.6217 - acc: 0.6385 - val_loss: 0.6254 - val_acc: 0.6346\n",
      "Epoch 16/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4991 - acc: 0.875 - ETA: 0s - loss: 0.6053 - acc: 0.664 - ETA: 0s - loss: 0.6102 - acc: 0.653 - ETA: 0s - loss: 0.6200 - acc: 0.634 - 0s 215us/step - loss: 0.6174 - acc: 0.6385 - val_loss: 0.6210 - val_acc: 0.6346\n",
      "Epoch 17/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6294 - acc: 0.625 - ETA: 0s - loss: 0.6106 - acc: 0.640 - ETA: 0s - loss: 0.6121 - acc: 0.638 - ETA: 0s - loss: 0.6121 - acc: 0.640 - 0s 218us/step - loss: 0.6122 - acc: 0.6385 - val_loss: 0.6162 - val_acc: 0.6346\n",
      "Epoch 18/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5869 - acc: 0.687 - ETA: 0s - loss: 0.6012 - acc: 0.664 - ETA: 0s - loss: 0.6063 - acc: 0.645 - ETA: 0s - loss: 0.6077 - acc: 0.640 - 0s 199us/step - loss: 0.6084 - acc: 0.6385 - val_loss: 0.6109 - val_acc: 0.6385\n",
      "Epoch 19/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5779 - acc: 0.656 - ETA: 0s - loss: 0.6093 - acc: 0.648 - ETA: 0s - loss: 0.6080 - acc: 0.643 - ETA: 0s - loss: 0.6034 - acc: 0.643 - 0s 206us/step - loss: 0.6053 - acc: 0.6394 - val_loss: 0.6052 - val_acc: 0.6385\n",
      "Epoch 20/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6128 - acc: 0.625 - ETA: 0s - loss: 0.5996 - acc: 0.646 - ETA: 0s - loss: 0.5981 - acc: 0.644 - ETA: 0s - loss: 0.5965 - acc: 0.643 - 0s 221us/step - loss: 0.5964 - acc: 0.6394 - val_loss: 0.5989 - val_acc: 0.6385\n",
      "Epoch 21/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6060 - acc: 0.593 - ETA: 0s - loss: 0.5809 - acc: 0.646 - ETA: 0s - loss: 0.5873 - acc: 0.638 - ETA: 0s - loss: 0.5840 - acc: 0.644 - 0s 207us/step - loss: 0.5847 - acc: 0.6423 - val_loss: 0.5918 - val_acc: 0.6385\n",
      "Epoch 22/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4964 - acc: 0.781 - ETA: 0s - loss: 0.5736 - acc: 0.673 - ETA: 0s - loss: 0.5757 - acc: 0.666 - ETA: 0s - loss: 0.5811 - acc: 0.649 - 0s 202us/step - loss: 0.5797 - acc: 0.6500 - val_loss: 0.5840 - val_acc: 0.6538\n",
      "Epoch 23/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5813 - acc: 0.656 - ETA: 0s - loss: 0.5721 - acc: 0.667 - ETA: 0s - loss: 0.5727 - acc: 0.664 - ETA: 0s - loss: 0.5701 - acc: 0.660 - 0s 212us/step - loss: 0.5738 - acc: 0.6567 - val_loss: 0.5761 - val_acc: 0.6577\n",
      "Epoch 24/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5770 - acc: 0.625 - ETA: 0s - loss: 0.5818 - acc: 0.628 - ETA: 0s - loss: 0.5650 - acc: 0.665 - ETA: 0s - loss: 0.5627 - acc: 0.677 - 0s 215us/step - loss: 0.5611 - acc: 0.6769 - val_loss: 0.5675 - val_acc: 0.6923\n",
      "Epoch 25/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5788 - acc: 0.750 - ETA: 0s - loss: 0.5682 - acc: 0.673 - ETA: 0s - loss: 0.5470 - acc: 0.704 - ETA: 0s - loss: 0.5533 - acc: 0.687 - 0s 218us/step - loss: 0.5523 - acc: 0.6913 - val_loss: 0.5584 - val_acc: 0.7000\n",
      "Epoch 26/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5318 - acc: 0.718 - ETA: 0s - loss: 0.5351 - acc: 0.728 - ETA: 0s - loss: 0.5434 - acc: 0.708 - ETA: 0s - loss: 0.5437 - acc: 0.710 - ETA: 0s - loss: 0.5425 - acc: 0.708 - 0s 234us/step - loss: 0.5437 - acc: 0.7067 - val_loss: 0.5496 - val_acc: 0.7077\n",
      "Epoch 27/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4819 - acc: 0.781 - ETA: 0s - loss: 0.5209 - acc: 0.764 - ETA: 0s - loss: 0.5212 - acc: 0.739 - ETA: 0s - loss: 0.5354 - acc: 0.723 - 0s 187us/step - loss: 0.5319 - acc: 0.7337 - val_loss: 0.5400 - val_acc: 0.7500\n",
      "Epoch 28/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4866 - acc: 0.843 - ETA: 0s - loss: 0.5111 - acc: 0.795 - ETA: 0s - loss: 0.5225 - acc: 0.762 - ETA: 0s - loss: 0.5189 - acc: 0.761 - 0s 221us/step - loss: 0.5223 - acc: 0.7606 - val_loss: 0.5308 - val_acc: 0.7615\n",
      "Epoch 29/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5022 - acc: 0.750 - ETA: 0s - loss: 0.5197 - acc: 0.762 - ETA: 0s - loss: 0.5179 - acc: 0.763 - ETA: 0s - loss: 0.5146 - acc: 0.771 - 0s 200us/step - loss: 0.5121 - acc: 0.7760 - val_loss: 0.5217 - val_acc: 0.7692\n",
      "Epoch 30/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5304 - acc: 0.781 - ETA: 0s - loss: 0.5133 - acc: 0.757 - ETA: 0s - loss: 0.5059 - acc: 0.775 - ETA: 0s - loss: 0.5024 - acc: 0.783 - 0s 204us/step - loss: 0.5039 - acc: 0.7827 - val_loss: 0.5127 - val_acc: 0.7808\n",
      "Epoch 31/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5273 - acc: 0.812 - ETA: 0s - loss: 0.5039 - acc: 0.787 - ETA: 0s - loss: 0.4948 - acc: 0.792 - ETA: 0s - loss: 0.4959 - acc: 0.784 - 0s 214us/step - loss: 0.4916 - acc: 0.7865 - val_loss: 0.5046 - val_acc: 0.7846\n",
      "Epoch 32/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5340 - acc: 0.812 - ETA: 0s - loss: 0.4823 - acc: 0.798 - ETA: 0s - loss: 0.4831 - acc: 0.794 - ETA: 0s - loss: 0.4827 - acc: 0.796 - 0s 185us/step - loss: 0.4835 - acc: 0.7952 - val_loss: 0.4967 - val_acc: 0.7962\n",
      "Epoch 33/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5039 - acc: 0.781 - ETA: 0s - loss: 0.4706 - acc: 0.815 - ETA: 0s - loss: 0.4762 - acc: 0.804 - ETA: 0s - loss: 0.4754 - acc: 0.810 - 0s 219us/step - loss: 0.4747 - acc: 0.8106 - val_loss: 0.4891 - val_acc: 0.7962\n",
      "Epoch 34/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4899 - acc: 0.843 - ETA: 0s - loss: 0.4487 - acc: 0.847 - ETA: 0s - loss: 0.4720 - acc: 0.822 - ETA: 0s - loss: 0.4655 - acc: 0.819 - 0s 203us/step - loss: 0.4682 - acc: 0.8183 - val_loss: 0.4820 - val_acc: 0.7962\n",
      "Epoch 35/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4715 - acc: 0.843 - ETA: 0s - loss: 0.4636 - acc: 0.802 - ETA: 0s - loss: 0.4699 - acc: 0.807 - ETA: 0s - loss: 0.4640 - acc: 0.812 - 0s 192us/step - loss: 0.4573 - acc: 0.8173 - val_loss: 0.4760 - val_acc: 0.8000\n",
      "Epoch 36/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.5576 - acc: 0.718 - ETA: 0s - loss: 0.4604 - acc: 0.821 - ETA: 0s - loss: 0.4448 - acc: 0.830 - ETA: 0s - loss: 0.4487 - acc: 0.825 - 0s 209us/step - loss: 0.4544 - acc: 0.8173 - val_loss: 0.4700 - val_acc: 0.8038\n",
      "Epoch 37/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3742 - acc: 0.906 - ETA: 0s - loss: 0.4353 - acc: 0.850 - ETA: 0s - loss: 0.4448 - acc: 0.821 - ETA: 0s - loss: 0.4455 - acc: 0.820 - 0s 214us/step - loss: 0.4454 - acc: 0.8269 - val_loss: 0.4648 - val_acc: 0.8077\n",
      "Epoch 38/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4078 - acc: 0.812 - ETA: 0s - loss: 0.4385 - acc: 0.804 - ETA: 0s - loss: 0.4410 - acc: 0.822 - ETA: 0s - loss: 0.4398 - acc: 0.824 - 0s 204us/step - loss: 0.4363 - acc: 0.8288 - val_loss: 0.4603 - val_acc: 0.8077\n",
      "Epoch 39/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4534 - acc: 0.812 - ETA: 0s - loss: 0.4472 - acc: 0.823 - ETA: 0s - loss: 0.4468 - acc: 0.815 - ETA: 0s - loss: 0.4285 - acc: 0.830 - 0s 212us/step - loss: 0.4286 - acc: 0.8269 - val_loss: 0.4555 - val_acc: 0.8115\n",
      "Epoch 40/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4220 - acc: 0.906 - ETA: 0s - loss: 0.3954 - acc: 0.871 - ETA: 0s - loss: 0.4158 - acc: 0.837 - ETA: 0s - loss: 0.4241 - acc: 0.831 - 0s 199us/step - loss: 0.4246 - acc: 0.8327 - val_loss: 0.4516 - val_acc: 0.8077\n",
      "Epoch 41/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3547 - acc: 0.968 - ETA: 0s - loss: 0.4194 - acc: 0.862 - ETA: 0s - loss: 0.4342 - acc: 0.842 - ETA: 0s - loss: 0.4226 - acc: 0.833 - 0s 214us/step - loss: 0.4169 - acc: 0.8385 - val_loss: 0.4479 - val_acc: 0.8115\n",
      "Epoch 42/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6772 - acc: 0.687 - ETA: 0s - loss: 0.4549 - acc: 0.816 - ETA: 0s - loss: 0.4186 - acc: 0.836 - ETA: 0s - loss: 0.4152 - acc: 0.838 - 0s 207us/step - loss: 0.4169 - acc: 0.8375 - val_loss: 0.4443 - val_acc: 0.8154\n",
      "Epoch 43/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3412 - acc: 0.968 - ETA: 0s - loss: 0.4080 - acc: 0.838 - ETA: 0s - loss: 0.4049 - acc: 0.846 - ETA: 0s - loss: 0.4102 - acc: 0.840 - 0s 198us/step - loss: 0.4171 - acc: 0.8346 - val_loss: 0.4411 - val_acc: 0.8192\n",
      "Epoch 44/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4085 - acc: 0.750 - ETA: 0s - loss: 0.4314 - acc: 0.829 - ETA: 0s - loss: 0.3996 - acc: 0.856 - ETA: 0s - loss: 0.4043 - acc: 0.841 - 0s 207us/step - loss: 0.4091 - acc: 0.8385 - val_loss: 0.4388 - val_acc: 0.8154\n",
      "Epoch 45/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4208 - acc: 0.812 - ETA: 0s - loss: 0.3983 - acc: 0.851 - ETA: 0s - loss: 0.4008 - acc: 0.849 - ETA: 0s - loss: 0.3987 - acc: 0.845 - 0s 203us/step - loss: 0.4053 - acc: 0.8442 - val_loss: 0.4364 - val_acc: 0.8154\n",
      "Epoch 46/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4336 - acc: 0.812 - ETA: 0s - loss: 0.4241 - acc: 0.823 - ETA: 0s - loss: 0.4129 - acc: 0.827 - ETA: 0s - loss: 0.4037 - acc: 0.841 - 0s 195us/step - loss: 0.4005 - acc: 0.8413 - val_loss: 0.4346 - val_acc: 0.8154\n",
      "Epoch 47/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4937 - acc: 0.750 - ETA: 0s - loss: 0.4068 - acc: 0.840 - ETA: 0s - loss: 0.4073 - acc: 0.843 - ETA: 0s - loss: 0.4084 - acc: 0.838 - 0s 212us/step - loss: 0.3982 - acc: 0.8413 - val_loss: 0.4317 - val_acc: 0.8192\n",
      "Epoch 48/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4205 - acc: 0.843 - ETA: 0s - loss: 0.4038 - acc: 0.825 - ETA: 0s - loss: 0.4048 - acc: 0.837 - ETA: 0s - loss: 0.3990 - acc: 0.841 - 0s 216us/step - loss: 0.3944 - acc: 0.8394 - val_loss: 0.4306 - val_acc: 0.8192\n",
      "Epoch 49/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4455 - acc: 0.812 - ETA: 0s - loss: 0.4021 - acc: 0.822 - ETA: 0s - loss: 0.4118 - acc: 0.818 - ETA: 0s - loss: 0.3970 - acc: 0.836 - 0s 207us/step - loss: 0.3868 - acc: 0.8404 - val_loss: 0.4278 - val_acc: 0.8192\n",
      "Epoch 50/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4053 - acc: 0.937 - ETA: 0s - loss: 0.4261 - acc: 0.828 - ETA: 0s - loss: 0.4089 - acc: 0.835 - ETA: 0s - loss: 0.4065 - acc: 0.837 - 0s 217us/step - loss: 0.3986 - acc: 0.8413 - val_loss: 0.4264 - val_acc: 0.8192\n",
      "Epoch 51/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4825 - acc: 0.750 - ETA: 0s - loss: 0.4036 - acc: 0.837 - ETA: 0s - loss: 0.3831 - acc: 0.838 - ETA: 0s - loss: 0.3804 - acc: 0.846 - 0s 201us/step - loss: 0.3832 - acc: 0.8452 - val_loss: 0.4262 - val_acc: 0.8192\n",
      "Epoch 52/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4034 - acc: 0.812 - ETA: 0s - loss: 0.3783 - acc: 0.838 - ETA: 0s - loss: 0.3741 - acc: 0.838 - ETA: 0s - loss: 0.3779 - acc: 0.842 - 0s 211us/step - loss: 0.3804 - acc: 0.8423 - val_loss: 0.4238 - val_acc: 0.8192\n",
      "Epoch 53/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3463 - acc: 0.906 - ETA: 0s - loss: 0.3938 - acc: 0.829 - ETA: 0s - loss: 0.3953 - acc: 0.832 - ETA: 0s - loss: 0.3853 - acc: 0.835 - 0s 226us/step - loss: 0.3826 - acc: 0.8413 - val_loss: 0.4226 - val_acc: 0.8192\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3230 - acc: 0.812 - ETA: 0s - loss: 0.3709 - acc: 0.840 - ETA: 0s - loss: 0.3953 - acc: 0.840 - ETA: 0s - loss: 0.3895 - acc: 0.846 - 0s 194us/step - loss: 0.3795 - acc: 0.8500 - val_loss: 0.4215 - val_acc: 0.8192\n",
      "Epoch 55/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3485 - acc: 0.875 - ETA: 0s - loss: 0.4153 - acc: 0.828 - ETA: 0s - loss: 0.3727 - acc: 0.847 - ETA: 0s - loss: 0.3702 - acc: 0.849 - 0s 214us/step - loss: 0.3788 - acc: 0.8433 - val_loss: 0.4215 - val_acc: 0.8154\n",
      "Epoch 56/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3309 - acc: 0.875 - ETA: 0s - loss: 0.3532 - acc: 0.849 - ETA: 0s - loss: 0.3682 - acc: 0.856 - ETA: 0s - loss: 0.3700 - acc: 0.850 - 0s 205us/step - loss: 0.3704 - acc: 0.8442 - val_loss: 0.4205 - val_acc: 0.8192\n",
      "Epoch 57/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2507 - acc: 0.906 - ETA: 0s - loss: 0.4190 - acc: 0.858 - ETA: 0s - loss: 0.4100 - acc: 0.836 - ETA: 0s - loss: 0.3875 - acc: 0.856 - 0s 207us/step - loss: 0.3799 - acc: 0.8510 - val_loss: 0.4188 - val_acc: 0.8231\n",
      "Epoch 58/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2601 - acc: 0.937 - ETA: 0s - loss: 0.3667 - acc: 0.853 - ETA: 0s - loss: 0.3725 - acc: 0.857 - ETA: 0s - loss: 0.3734 - acc: 0.853 - 0s 199us/step - loss: 0.3704 - acc: 0.8529 - val_loss: 0.4185 - val_acc: 0.8192\n",
      "Epoch 59/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2652 - acc: 0.906 - ETA: 0s - loss: 0.3730 - acc: 0.840 - ETA: 0s - loss: 0.3702 - acc: 0.837 - ETA: 0s - loss: 0.3706 - acc: 0.849 - 0s 195us/step - loss: 0.3730 - acc: 0.8462 - val_loss: 0.4179 - val_acc: 0.8192\n",
      "Epoch 60/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3202 - acc: 0.875 - ETA: 0s - loss: 0.3982 - acc: 0.840 - ETA: 0s - loss: 0.3658 - acc: 0.857 - ETA: 0s - loss: 0.3734 - acc: 0.848 - 0s 209us/step - loss: 0.3643 - acc: 0.8490 - val_loss: 0.4168 - val_acc: 0.8192\n",
      "Epoch 61/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2967 - acc: 0.875 - ETA: 0s - loss: 0.3524 - acc: 0.864 - ETA: 0s - loss: 0.3646 - acc: 0.849 - ETA: 0s - loss: 0.3594 - acc: 0.853 - 0s 222us/step - loss: 0.3633 - acc: 0.8510 - val_loss: 0.4160 - val_acc: 0.8192\n",
      "Epoch 62/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4176 - acc: 0.875 - ETA: 0s - loss: 0.3696 - acc: 0.859 - ETA: 0s - loss: 0.3583 - acc: 0.852 - ETA: 0s - loss: 0.3663 - acc: 0.853 - 0s 194us/step - loss: 0.3691 - acc: 0.8519 - val_loss: 0.4157 - val_acc: 0.8231\n",
      "Epoch 63/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.1831 - acc: 0.968 - ETA: 0s - loss: 0.3008 - acc: 0.871 - ETA: 0s - loss: 0.3402 - acc: 0.852 - ETA: 0s - loss: 0.3552 - acc: 0.848 - 0s 212us/step - loss: 0.3590 - acc: 0.8490 - val_loss: 0.4148 - val_acc: 0.8231\n",
      "Epoch 64/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3629 - acc: 0.906 - ETA: 0s - loss: 0.3791 - acc: 0.868 - ETA: 0s - loss: 0.3780 - acc: 0.850 - ETA: 0s - loss: 0.3781 - acc: 0.848 - 0s 223us/step - loss: 0.3652 - acc: 0.8538 - val_loss: 0.4148 - val_acc: 0.8192\n",
      "Epoch 65/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3171 - acc: 0.906 - ETA: 0s - loss: 0.3500 - acc: 0.859 - ETA: 0s - loss: 0.3512 - acc: 0.861 - ETA: 0s - loss: 0.3712 - acc: 0.851 - 0s 207us/step - loss: 0.3583 - acc: 0.8548 - val_loss: 0.4142 - val_acc: 0.8231\n",
      "Epoch 66/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3693 - acc: 0.875 - ETA: 0s - loss: 0.3434 - acc: 0.875 - ETA: 0s - loss: 0.3409 - acc: 0.866 - ETA: 0s - loss: 0.3560 - acc: 0.860 - ETA: 0s - loss: 0.3587 - acc: 0.856 - 0s 241us/step - loss: 0.3592 - acc: 0.8548 - val_loss: 0.4133 - val_acc: 0.8231\n",
      "Epoch 67/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3310 - acc: 0.906 - ETA: 0s - loss: 0.3435 - acc: 0.850 - ETA: 0s - loss: 0.3712 - acc: 0.842 - ETA: 0s - loss: 0.3554 - acc: 0.856 - 0s 208us/step - loss: 0.3514 - acc: 0.8567 - val_loss: 0.4126 - val_acc: 0.8231\n",
      "Epoch 68/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3282 - acc: 0.906 - ETA: 0s - loss: 0.3519 - acc: 0.863 - ETA: 0s - loss: 0.3613 - acc: 0.853 - ETA: 0s - loss: 0.3677 - acc: 0.850 - 0s 195us/step - loss: 0.3617 - acc: 0.8538 - val_loss: 0.4120 - val_acc: 0.8231\n",
      "Epoch 69/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3224 - acc: 0.812 - ETA: 0s - loss: 0.3290 - acc: 0.856 - ETA: 0s - loss: 0.3783 - acc: 0.840 - ETA: 0s - loss: 0.3650 - acc: 0.849 - 0s 222us/step - loss: 0.3567 - acc: 0.8538 - val_loss: 0.4121 - val_acc: 0.8231\n",
      "Epoch 70/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4159 - acc: 0.781 - ETA: 0s - loss: 0.3655 - acc: 0.822 - ETA: 0s - loss: 0.3635 - acc: 0.842 - ETA: 0s - loss: 0.3618 - acc: 0.847 - 0s 214us/step - loss: 0.3504 - acc: 0.8538 - val_loss: 0.4113 - val_acc: 0.8231\n",
      "Epoch 71/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3250 - acc: 0.843 - ETA: 0s - loss: 0.3799 - acc: 0.826 - ETA: 0s - loss: 0.3608 - acc: 0.848 - ETA: 0s - loss: 0.3545 - acc: 0.854 - 0s 216us/step - loss: 0.3510 - acc: 0.8558 - val_loss: 0.4108 - val_acc: 0.8231\n",
      "Epoch 72/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4508 - acc: 0.750 - ETA: 0s - loss: 0.3266 - acc: 0.885 - ETA: 0s - loss: 0.3485 - acc: 0.859 - ETA: 0s - loss: 0.3437 - acc: 0.860 - ETA: 0s - loss: 0.3523 - acc: 0.856 - 0s 257us/step - loss: 0.3538 - acc: 0.8548 - val_loss: 0.4109 - val_acc: 0.8231\n",
      "Epoch 73/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4529 - acc: 0.843 - ETA: 0s - loss: 0.3644 - acc: 0.862 - ETA: 0s - loss: 0.3397 - acc: 0.871 - ETA: 0s - loss: 0.3571 - acc: 0.865 - 0s 232us/step - loss: 0.3541 - acc: 0.8635 - val_loss: 0.4106 - val_acc: 0.8231\n",
      "Epoch 74/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2847 - acc: 0.875 - ETA: 0s - loss: 0.3501 - acc: 0.868 - ETA: 0s - loss: 0.3489 - acc: 0.863 - ETA: 0s - loss: 0.3420 - acc: 0.865 - ETA: 0s - loss: 0.3538 - acc: 0.863 - 0s 252us/step - loss: 0.3540 - acc: 0.8644 - val_loss: 0.4109 - val_acc: 0.8192\n",
      "Epoch 75/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2676 - acc: 0.937 - ETA: 0s - loss: 0.3415 - acc: 0.885 - ETA: 0s - loss: 0.3375 - acc: 0.871 - ETA: 0s - loss: 0.3505 - acc: 0.860 - 0s 223us/step - loss: 0.3454 - acc: 0.8625 - val_loss: 0.4098 - val_acc: 0.8231\n",
      "Epoch 76/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3066 - acc: 0.875 - ETA: 0s - loss: 0.3625 - acc: 0.840 - ETA: 0s - loss: 0.3493 - acc: 0.849 - ETA: 0s - loss: 0.3305 - acc: 0.866 - ETA: 0s - loss: 0.3424 - acc: 0.860 - 0s 240us/step - loss: 0.3446 - acc: 0.8596 - val_loss: 0.4101 - val_acc: 0.8231\n",
      "Epoch 77/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2450 - acc: 0.937 - ETA: 0s - loss: 0.3662 - acc: 0.857 - ETA: 0s - loss: 0.3487 - acc: 0.867 - ETA: 0s - loss: 0.3430 - acc: 0.866 - 0s 204us/step - loss: 0.3468 - acc: 0.8625 - val_loss: 0.4097 - val_acc: 0.8231\n",
      "Epoch 78/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3871 - acc: 0.781 - ETA: 0s - loss: 0.3495 - acc: 0.863 - ETA: 0s - loss: 0.3437 - acc: 0.881 - ETA: 0s - loss: 0.3528 - acc: 0.871 - 0s 216us/step - loss: 0.3562 - acc: 0.8673 - val_loss: 0.4098 - val_acc: 0.8231\n",
      "Epoch 79/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4042 - acc: 0.812 - ETA: 0s - loss: 0.3626 - acc: 0.847 - ETA: 0s - loss: 0.3428 - acc: 0.862 - ETA: 0s - loss: 0.3402 - acc: 0.862 - 0s 224us/step - loss: 0.3427 - acc: 0.8587 - val_loss: 0.4094 - val_acc: 0.8192\n",
      "Epoch 80/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.6330 - acc: 0.781 - ETA: 0s - loss: 0.3915 - acc: 0.854 - ETA: 0s - loss: 0.3762 - acc: 0.854 - ETA: 0s - loss: 0.3513 - acc: 0.869 - 0s 214us/step - loss: 0.3481 - acc: 0.8654 - val_loss: 0.4086 - val_acc: 0.8192\n",
      "Epoch 81/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3513 - acc: 0.906 - ETA: 0s - loss: 0.3522 - acc: 0.880 - ETA: 0s - loss: 0.3487 - acc: 0.865 - ETA: 0s - loss: 0.3382 - acc: 0.869 - 0s 205us/step - loss: 0.3438 - acc: 0.8654 - val_loss: 0.4084 - val_acc: 0.8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3411 - acc: 0.812 - ETA: 0s - loss: 0.3296 - acc: 0.871 - ETA: 0s - loss: 0.3391 - acc: 0.864 - ETA: 0s - loss: 0.3362 - acc: 0.869 - 0s 225us/step - loss: 0.3415 - acc: 0.8692 - val_loss: 0.4079 - val_acc: 0.8231\n",
      "Epoch 83/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4004 - acc: 0.812 - ETA: 0s - loss: 0.3171 - acc: 0.863 - ETA: 0s - loss: 0.3291 - acc: 0.871 - ETA: 0s - loss: 0.3315 - acc: 0.869 - 0s 231us/step - loss: 0.3450 - acc: 0.8606 - val_loss: 0.4083 - val_acc: 0.8192\n",
      "Epoch 84/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2498 - acc: 0.906 - ETA: 0s - loss: 0.3089 - acc: 0.888 - ETA: 0s - loss: 0.3101 - acc: 0.882 - ETA: 0s - loss: 0.3142 - acc: 0.885 - ETA: 0s - loss: 0.3425 - acc: 0.871 - 0s 255us/step - loss: 0.3443 - acc: 0.8683 - val_loss: 0.4076 - val_acc: 0.8231\n",
      "Epoch 85/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3001 - acc: 0.843 - ETA: 0s - loss: 0.3271 - acc: 0.856 - ETA: 0s - loss: 0.3266 - acc: 0.875 - ETA: 0s - loss: 0.3335 - acc: 0.873 - 0s 214us/step - loss: 0.3415 - acc: 0.8721 - val_loss: 0.4081 - val_acc: 0.8192\n",
      "Epoch 86/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4456 - acc: 0.750 - ETA: 0s - loss: 0.3116 - acc: 0.878 - ETA: 0s - loss: 0.3344 - acc: 0.873 - ETA: 0s - loss: 0.3458 - acc: 0.868 - ETA: 0s - loss: 0.3431 - acc: 0.872 - 0s 250us/step - loss: 0.3424 - acc: 0.8721 - val_loss: 0.4075 - val_acc: 0.8192\n",
      "Epoch 87/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.1743 - acc: 0.968 - ETA: 0s - loss: 0.3061 - acc: 0.877 - ETA: 0s - loss: 0.3142 - acc: 0.871 - ETA: 0s - loss: 0.3248 - acc: 0.873 - 0s 217us/step - loss: 0.3341 - acc: 0.8692 - val_loss: 0.4072 - val_acc: 0.8231\n",
      "Epoch 88/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3427 - acc: 0.875 - ETA: 0s - loss: 0.3593 - acc: 0.872 - ETA: 0s - loss: 0.3402 - acc: 0.868 - ETA: 0s - loss: 0.3439 - acc: 0.869 - 0s 192us/step - loss: 0.3395 - acc: 0.8712 - val_loss: 0.4063 - val_acc: 0.8231\n",
      "Epoch 89/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3427 - acc: 0.843 - ETA: 0s - loss: 0.3806 - acc: 0.843 - ETA: 0s - loss: 0.3604 - acc: 0.854 - ETA: 0s - loss: 0.3486 - acc: 0.862 - 0s 202us/step - loss: 0.3397 - acc: 0.8673 - val_loss: 0.4074 - val_acc: 0.8192\n",
      "Epoch 90/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.1433 - acc: 1.000 - ETA: 0s - loss: 0.3169 - acc: 0.885 - ETA: 0s - loss: 0.3288 - acc: 0.867 - ETA: 0s - loss: 0.3395 - acc: 0.868 - 0s 216us/step - loss: 0.3371 - acc: 0.8712 - val_loss: 0.4072 - val_acc: 0.8231\n",
      "Epoch 91/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4054 - acc: 0.875 - ETA: 0s - loss: 0.3036 - acc: 0.887 - ETA: 0s - loss: 0.3352 - acc: 0.873 - ETA: 0s - loss: 0.3512 - acc: 0.866 - 0s 205us/step - loss: 0.3447 - acc: 0.8663 - val_loss: 0.4064 - val_acc: 0.8231\n",
      "Epoch 92/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3453 - acc: 0.906 - ETA: 0s - loss: 0.3761 - acc: 0.865 - ETA: 0s - loss: 0.3376 - acc: 0.876 - ETA: 0s - loss: 0.3438 - acc: 0.869 - 0s 224us/step - loss: 0.3404 - acc: 0.8712 - val_loss: 0.4061 - val_acc: 0.8231\n",
      "Epoch 93/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3532 - acc: 0.812 - ETA: 0s - loss: 0.3625 - acc: 0.869 - ETA: 0s - loss: 0.3382 - acc: 0.875 - ETA: 0s - loss: 0.3391 - acc: 0.870 - 0s 220us/step - loss: 0.3322 - acc: 0.8760 - val_loss: 0.4061 - val_acc: 0.8231\n",
      "Epoch 94/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2716 - acc: 0.875 - ETA: 0s - loss: 0.3142 - acc: 0.881 - ETA: 0s - loss: 0.3473 - acc: 0.865 - ETA: 0s - loss: 0.3299 - acc: 0.875 - 0s 237us/step - loss: 0.3411 - acc: 0.8740 - val_loss: 0.4060 - val_acc: 0.8231\n",
      "Epoch 95/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.4854 - acc: 0.812 - ETA: 0s - loss: 0.3071 - acc: 0.878 - ETA: 0s - loss: 0.3521 - acc: 0.864 - ETA: 0s - loss: 0.3465 - acc: 0.867 - ETA: 0s - loss: 0.3357 - acc: 0.875 - 0s 238us/step - loss: 0.3338 - acc: 0.8731 - val_loss: 0.4054 - val_acc: 0.8231\n",
      "Epoch 96/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3356 - acc: 0.906 - ETA: 0s - loss: 0.2845 - acc: 0.881 - ETA: 0s - loss: 0.3391 - acc: 0.857 - ETA: 0s - loss: 0.3326 - acc: 0.867 - ETA: 0s - loss: 0.3393 - acc: 0.871 - 0s 235us/step - loss: 0.3381 - acc: 0.8721 - val_loss: 0.4053 - val_acc: 0.8231\n",
      "Epoch 97/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2774 - acc: 0.875 - ETA: 0s - loss: 0.3389 - acc: 0.881 - ETA: 0s - loss: 0.3262 - acc: 0.875 - ETA: 0s - loss: 0.3204 - acc: 0.877 - 0s 216us/step - loss: 0.3328 - acc: 0.8750 - val_loss: 0.4059 - val_acc: 0.8269\n",
      "Epoch 98/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3221 - acc: 0.875 - ETA: 0s - loss: 0.3468 - acc: 0.861 - ETA: 0s - loss: 0.3219 - acc: 0.875 - ETA: 0s - loss: 0.3285 - acc: 0.870 - 0s 217us/step - loss: 0.3316 - acc: 0.8731 - val_loss: 0.4048 - val_acc: 0.8231\n",
      "Epoch 99/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.2171 - acc: 0.937 - ETA: 0s - loss: 0.3413 - acc: 0.875 - ETA: 0s - loss: 0.3427 - acc: 0.872 - ETA: 0s - loss: 0.3296 - acc: 0.876 - ETA: 0s - loss: 0.3385 - acc: 0.873 - 0s 234us/step - loss: 0.3370 - acc: 0.8740 - val_loss: 0.4050 - val_acc: 0.8269\n",
      "Epoch 100/100\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.3133 - acc: 0.843 - ETA: 0s - loss: 0.3332 - acc: 0.866 - ETA: 0s - loss: 0.3384 - acc: 0.875 - ETA: 0s - loss: 0.3294 - acc: 0.878 - 0s 200us/step - loss: 0.3353 - acc: 0.8740 - val_loss: 0.4050 - val_acc: 0.8269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de794f62e8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dim = len(train_vecs_race[0])\n",
    "num_data = len(train_vecs_race)\n",
    "num_data_test = len(test_vecs_race)\n",
    "\n",
    "train_vecs_race = train_vecs_race.reshape((num_data, timesteps, data_dim))\n",
    "y_train_race = y_train_race.reshape((num_data, num_class))\n",
    "test_vecs_race = test_vecs_race.reshape((num_data_test, timesteps, data_dim))\n",
    "y_test_race = y_test_race.reshape((num_data_test, num_class))\n",
    "\n",
    "model_ad_race = Sequential()\n",
    "model_ad_race.add(Bidirectional(LSTM(hidden_size, input_shape=(timesteps, data_dim)), merge_mode='concat'))\n",
    "model_ad_race.add(Dropout(0.5))\n",
    "model_ad_race.add(Dense(1, activation='sigmoid'))\n",
    "model_ad_race.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_ad_race.fit(train_vecs_race, y_train_race, epochs=num_epochs, validation_data=[test_vecs_race, y_test_race])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8269230769230769 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.93      0.87       165\n",
      "          1       0.84      0.65      0.73        95\n",
      "\n",
      "avg / total       0.83      0.83      0.82       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction['ad_race'] = model_ad_race.predict(test_vecs_race)\n",
    "\n",
    "for i in range(len(prediction['ad_race'])):\n",
    "    prediction['ad_race'][i][0] = round(prediction['ad_race'][i][0])\n",
    "\n",
    "accuracy['ad_race'] = accuracy_score(y_test_race, prediction['ad_race'])\n",
    "print(\"Accuracy: \", accuracy['ad_race'], \"\\n\")\n",
    "print(classification_report(y_test_race, prediction['ad_race'], labels = [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM Aspect Detection Model for Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 688 samples, validate on 173 samples\n",
      "Epoch 1/100\n",
      "688/688 [==============================] - ETA: 1:01 - loss: 0.6928 - acc: 0.593 - ETA: 4s - loss: 0.6923 - acc: 0.5903  - ETA: 0s - loss: 0.6912 - acc: 0.601 - 4s 5ms/step - loss: 0.6913 - acc: 0.5974 - val_loss: 0.6894 - val_acc: 0.5954\n",
      "Epoch 2/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6865 - acc: 0.656 - ETA: 0s - loss: 0.6888 - acc: 0.590 - ETA: 0s - loss: 0.6885 - acc: 0.586 - 0s 221us/step - loss: 0.6875 - acc: 0.6017 - val_loss: 0.6863 - val_acc: 0.5954\n",
      "Epoch 3/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6912 - acc: 0.500 - ETA: 0s - loss: 0.6840 - acc: 0.609 - ETA: 0s - loss: 0.6846 - acc: 0.598 - 0s 228us/step - loss: 0.6842 - acc: 0.6017 - val_loss: 0.6830 - val_acc: 0.5954\n",
      "Epoch 4/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6692 - acc: 0.718 - ETA: 0s - loss: 0.6771 - acc: 0.633 - ETA: 0s - loss: 0.6796 - acc: 0.609 - 0s 225us/step - loss: 0.6808 - acc: 0.6017 - val_loss: 0.6800 - val_acc: 0.5954\n",
      "Epoch 5/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6695 - acc: 0.656 - ETA: 0s - loss: 0.6683 - acc: 0.659 - ETA: 0s - loss: 0.6771 - acc: 0.606 - 0s 232us/step - loss: 0.6778 - acc: 0.6017 - val_loss: 0.6773 - val_acc: 0.5954\n",
      "Epoch 6/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6954 - acc: 0.531 - ETA: 0s - loss: 0.6736 - acc: 0.610 - ETA: 0s - loss: 0.6744 - acc: 0.602 - 0s 214us/step - loss: 0.6746 - acc: 0.6017 - val_loss: 0.6752 - val_acc: 0.5954\n",
      "Epoch 7/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6589 - acc: 0.656 - ETA: 0s - loss: 0.6743 - acc: 0.599 - 0s 204us/step - loss: 0.6722 - acc: 0.6017 - val_loss: 0.6733 - val_acc: 0.5954\n",
      "Epoch 8/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6708 - acc: 0.593 - ETA: 0s - loss: 0.6656 - acc: 0.618 - ETA: 0s - loss: 0.6710 - acc: 0.601 - 0s 234us/step - loss: 0.6710 - acc: 0.6017 - val_loss: 0.6713 - val_acc: 0.5954\n",
      "Epoch 9/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6729 - acc: 0.593 - ETA: 0s - loss: 0.6698 - acc: 0.596 - ETA: 0s - loss: 0.6665 - acc: 0.607 - 0s 219us/step - loss: 0.6687 - acc: 0.6017 - val_loss: 0.6698 - val_acc: 0.5954\n",
      "Epoch 10/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6454 - acc: 0.656 - ETA: 0s - loss: 0.6613 - acc: 0.622 - ETA: 0s - loss: 0.6650 - acc: 0.609 - 0s 198us/step - loss: 0.6675 - acc: 0.6017 - val_loss: 0.6681 - val_acc: 0.5954\n",
      "Epoch 11/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.7429 - acc: 0.375 - ETA: 0s - loss: 0.6667 - acc: 0.593 - ETA: 0s - loss: 0.6655 - acc: 0.600 - 0s 223us/step - loss: 0.6656 - acc: 0.6017 - val_loss: 0.6666 - val_acc: 0.5954\n",
      "Epoch 12/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.7110 - acc: 0.468 - ETA: 0s - loss: 0.6574 - acc: 0.625 - ETA: 0s - loss: 0.6637 - acc: 0.602 - 0s 227us/step - loss: 0.6637 - acc: 0.6017 - val_loss: 0.6650 - val_acc: 0.5954\n",
      "Epoch 13/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6713 - acc: 0.593 - ETA: 0s - loss: 0.6556 - acc: 0.622 - ETA: 0s - loss: 0.6695 - acc: 0.586 - 0s 222us/step - loss: 0.6642 - acc: 0.6017 - val_loss: 0.6634 - val_acc: 0.5954\n",
      "Epoch 14/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.7110 - acc: 0.468 - ETA: 0s - loss: 0.6660 - acc: 0.586 - ETA: 0s - loss: 0.6590 - acc: 0.605 - 0s 249us/step - loss: 0.6613 - acc: 0.6017 - val_loss: 0.6617 - val_acc: 0.5954\n",
      "Epoch 15/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6872 - acc: 0.531 - ETA: 0s - loss: 0.6654 - acc: 0.588 - 0s 185us/step - loss: 0.6597 - acc: 0.6017 - val_loss: 0.6599 - val_acc: 0.5954\n",
      "Epoch 16/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6617 - acc: 0.593 - ETA: 0s - loss: 0.6688 - acc: 0.576 - ETA: 0s - loss: 0.6662 - acc: 0.585 - 0s 209us/step - loss: 0.6591 - acc: 0.6017 - val_loss: 0.6580 - val_acc: 0.5954\n",
      "Epoch 17/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6741 - acc: 0.562 - ETA: 0s - loss: 0.6683 - acc: 0.572 - ETA: 0s - loss: 0.6594 - acc: 0.586 - 0s 228us/step - loss: 0.6550 - acc: 0.6017 - val_loss: 0.6558 - val_acc: 0.5954\n",
      "Epoch 18/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6525 - acc: 0.562 - ETA: 0s - loss: 0.6491 - acc: 0.609 - ETA: 0s - loss: 0.6522 - acc: 0.602 - 0s 223us/step - loss: 0.6533 - acc: 0.6017 - val_loss: 0.6536 - val_acc: 0.5954\n",
      "Epoch 19/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6457 - acc: 0.625 - ETA: 0s - loss: 0.6371 - acc: 0.640 - ETA: 0s - loss: 0.6525 - acc: 0.599 - 0s 230us/step - loss: 0.6515 - acc: 0.6017 - val_loss: 0.6513 - val_acc: 0.5954\n",
      "Epoch 20/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6506 - acc: 0.593 - ETA: 0s - loss: 0.6487 - acc: 0.602 - ETA: 0s - loss: 0.6488 - acc: 0.600 - 0s 229us/step - loss: 0.6482 - acc: 0.6017 - val_loss: 0.6488 - val_acc: 0.5954\n",
      "Epoch 21/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6307 - acc: 0.656 - ETA: 0s - loss: 0.6528 - acc: 0.587 - ETA: 0s - loss: 0.6482 - acc: 0.598 - 0s 213us/step - loss: 0.6466 - acc: 0.6017 - val_loss: 0.6460 - val_acc: 0.5954\n",
      "Epoch 22/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6385 - acc: 0.593 - ETA: 0s - loss: 0.6615 - acc: 0.576 - ETA: 0s - loss: 0.6528 - acc: 0.590 - 0s 213us/step - loss: 0.6467 - acc: 0.6017 - val_loss: 0.6432 - val_acc: 0.5954\n",
      "Epoch 23/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6458 - acc: 0.593 - ETA: 0s - loss: 0.6291 - acc: 0.640 - ETA: 0s - loss: 0.6436 - acc: 0.604 - 0s 222us/step - loss: 0.6434 - acc: 0.6017 - val_loss: 0.6398 - val_acc: 0.5954\n",
      "Epoch 24/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6109 - acc: 0.625 - ETA: 0s - loss: 0.6422 - acc: 0.596 - ETA: 0s - loss: 0.6395 - acc: 0.600 - 0s 226us/step - loss: 0.6385 - acc: 0.6032 - val_loss: 0.6363 - val_acc: 0.6069\n",
      "Epoch 25/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6565 - acc: 0.562 - ETA: 0s - loss: 0.6286 - acc: 0.618 - ETA: 0s - loss: 0.6365 - acc: 0.605 - 0s 236us/step - loss: 0.6357 - acc: 0.6061 - val_loss: 0.6326 - val_acc: 0.6069\n",
      "Epoch 26/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5735 - acc: 0.750 - ETA: 0s - loss: 0.6365 - acc: 0.593 - ETA: 0s - loss: 0.6321 - acc: 0.609 - 0s 211us/step - loss: 0.6313 - acc: 0.6119 - val_loss: 0.6284 - val_acc: 0.6301\n",
      "Epoch 27/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6274 - acc: 0.625 - ETA: 0s - loss: 0.6297 - acc: 0.622 - ETA: 0s - loss: 0.6319 - acc: 0.623 - 0s 216us/step - loss: 0.6309 - acc: 0.6265 - val_loss: 0.6238 - val_acc: 0.6358\n",
      "Epoch 28/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5738 - acc: 0.781 - ETA: 0s - loss: 0.6153 - acc: 0.656 - ETA: 0s - loss: 0.6225 - acc: 0.637 - 0s 232us/step - loss: 0.6260 - acc: 0.6308 - val_loss: 0.6194 - val_acc: 0.6416\n",
      "Epoch 29/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6804 - acc: 0.500 - ETA: 0s - loss: 0.6289 - acc: 0.625 - ETA: 0s - loss: 0.6286 - acc: 0.628 - 0s 224us/step - loss: 0.6240 - acc: 0.6381 - val_loss: 0.6144 - val_acc: 0.6647\n",
      "Epoch 30/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6143 - acc: 0.625 - ETA: 0s - loss: 0.6317 - acc: 0.628 - ETA: 0s - loss: 0.6204 - acc: 0.643 - 0s 231us/step - loss: 0.6190 - acc: 0.6483 - val_loss: 0.6091 - val_acc: 0.6647\n",
      "Epoch 31/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6661 - acc: 0.562 - ETA: 0s - loss: 0.6244 - acc: 0.634 - ETA: 0s - loss: 0.6143 - acc: 0.660 - 0s 212us/step - loss: 0.6127 - acc: 0.6628 - val_loss: 0.6033 - val_acc: 0.6705\n",
      "Epoch 32/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6487 - acc: 0.656 - ETA: 0s - loss: 0.6026 - acc: 0.667 - ETA: 0s - loss: 0.6072 - acc: 0.659 - 0s 219us/step - loss: 0.6091 - acc: 0.6570 - val_loss: 0.5976 - val_acc: 0.6705\n",
      "Epoch 33/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6307 - acc: 0.656 - ETA: 0s - loss: 0.6112 - acc: 0.652 - ETA: 0s - loss: 0.6012 - acc: 0.661 - 0s 226us/step - loss: 0.6008 - acc: 0.6657 - val_loss: 0.5912 - val_acc: 0.6705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5584 - acc: 0.750 - ETA: 0s - loss: 0.5869 - acc: 0.684 - 0s 192us/step - loss: 0.5963 - acc: 0.6875 - val_loss: 0.5841 - val_acc: 0.7052\n",
      "Epoch 35/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6079 - acc: 0.656 - ETA: 0s - loss: 0.5800 - acc: 0.735 - ETA: 0s - loss: 0.5901 - acc: 0.706 - 0s 229us/step - loss: 0.5901 - acc: 0.7049 - val_loss: 0.5772 - val_acc: 0.7399\n",
      "Epoch 36/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5398 - acc: 0.750 - ETA: 0s - loss: 0.5990 - acc: 0.696 - ETA: 0s - loss: 0.5885 - acc: 0.713 - 0s 213us/step - loss: 0.5838 - acc: 0.7209 - val_loss: 0.5700 - val_acc: 0.7861\n",
      "Epoch 37/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5591 - acc: 0.843 - ETA: 0s - loss: 0.5835 - acc: 0.730 - ETA: 0s - loss: 0.5861 - acc: 0.722 - 0s 221us/step - loss: 0.5823 - acc: 0.7267 - val_loss: 0.5621 - val_acc: 0.7861\n",
      "Epoch 38/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6532 - acc: 0.593 - ETA: 0s - loss: 0.5728 - acc: 0.731 - ETA: 0s - loss: 0.5765 - acc: 0.720 - 0s 222us/step - loss: 0.5728 - acc: 0.7253 - val_loss: 0.5543 - val_acc: 0.7919\n",
      "Epoch 39/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5891 - acc: 0.656 - ETA: 0s - loss: 0.5503 - acc: 0.761 - ETA: 0s - loss: 0.5611 - acc: 0.745 - 0s 212us/step - loss: 0.5619 - acc: 0.7442 - val_loss: 0.5461 - val_acc: 0.7919\n",
      "Epoch 40/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5810 - acc: 0.718 - ETA: 0s - loss: 0.5536 - acc: 0.762 - 0s 189us/step - loss: 0.5608 - acc: 0.7529 - val_loss: 0.5381 - val_acc: 0.7919\n",
      "Epoch 41/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5432 - acc: 0.781 - ETA: 0s - loss: 0.5624 - acc: 0.735 - ETA: 0s - loss: 0.5516 - acc: 0.754 - 0s 232us/step - loss: 0.5531 - acc: 0.7485 - val_loss: 0.5303 - val_acc: 0.8092\n",
      "Epoch 42/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6220 - acc: 0.656 - ETA: 0s - loss: 0.5405 - acc: 0.795 - ETA: 0s - loss: 0.5501 - acc: 0.764 - 0s 216us/step - loss: 0.5495 - acc: 0.7660 - val_loss: 0.5224 - val_acc: 0.8208\n",
      "Epoch 43/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5299 - acc: 0.781 - ETA: 0s - loss: 0.5476 - acc: 0.755 - ETA: 0s - loss: 0.5438 - acc: 0.772 - 0s 214us/step - loss: 0.5421 - acc: 0.7733 - val_loss: 0.5146 - val_acc: 0.8208\n",
      "Epoch 44/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.6197 - acc: 0.687 - ETA: 0s - loss: 0.5544 - acc: 0.770 - ETA: 0s - loss: 0.5351 - acc: 0.795 - 0s 238us/step - loss: 0.5338 - acc: 0.7951 - val_loss: 0.5066 - val_acc: 0.8208\n",
      "Epoch 45/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4766 - acc: 0.875 - ETA: 0s - loss: 0.5196 - acc: 0.793 - ETA: 0s - loss: 0.5290 - acc: 0.765 - 0s 222us/step - loss: 0.5289 - acc: 0.7718 - val_loss: 0.4990 - val_acc: 0.8266\n",
      "Epoch 46/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4714 - acc: 0.843 - ETA: 0s - loss: 0.5182 - acc: 0.786 - ETA: 0s - loss: 0.5201 - acc: 0.782 - 0s 221us/step - loss: 0.5181 - acc: 0.7863 - val_loss: 0.4921 - val_acc: 0.8324\n",
      "Epoch 47/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5673 - acc: 0.750 - ETA: 0s - loss: 0.5175 - acc: 0.790 - ETA: 0s - loss: 0.5308 - acc: 0.774 - 0s 228us/step - loss: 0.5174 - acc: 0.7907 - val_loss: 0.4848 - val_acc: 0.8382\n",
      "Epoch 48/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5142 - acc: 0.781 - ETA: 0s - loss: 0.5219 - acc: 0.800 - ETA: 0s - loss: 0.5158 - acc: 0.792 - 0s 231us/step - loss: 0.5107 - acc: 0.8023 - val_loss: 0.4781 - val_acc: 0.8382\n",
      "Epoch 49/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4793 - acc: 0.812 - ETA: 0s - loss: 0.5109 - acc: 0.796 - ETA: 0s - loss: 0.5141 - acc: 0.789 - 0s 212us/step - loss: 0.5079 - acc: 0.7965 - val_loss: 0.4714 - val_acc: 0.8439\n",
      "Epoch 50/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4754 - acc: 0.812 - ETA: 0s - loss: 0.4972 - acc: 0.795 - ETA: 0s - loss: 0.4993 - acc: 0.806 - 0s 220us/step - loss: 0.5001 - acc: 0.8038 - val_loss: 0.4649 - val_acc: 0.8382\n",
      "Epoch 51/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5824 - acc: 0.656 - ETA: 0s - loss: 0.5084 - acc: 0.798 - ETA: 0s - loss: 0.4844 - acc: 0.810 - 0s 222us/step - loss: 0.4926 - acc: 0.8052 - val_loss: 0.4585 - val_acc: 0.8382\n",
      "Epoch 52/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4538 - acc: 0.812 - ETA: 0s - loss: 0.4985 - acc: 0.792 - ETA: 0s - loss: 0.4883 - acc: 0.803 - 0s 218us/step - loss: 0.4867 - acc: 0.8067 - val_loss: 0.4527 - val_acc: 0.8439\n",
      "Epoch 53/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4989 - acc: 0.750 - ETA: 0s - loss: 0.4724 - acc: 0.825 - ETA: 0s - loss: 0.4793 - acc: 0.808 - 0s 236us/step - loss: 0.4889 - acc: 0.7994 - val_loss: 0.4467 - val_acc: 0.8497\n",
      "Epoch 54/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4228 - acc: 0.937 - ETA: 0s - loss: 0.4695 - acc: 0.821 - ETA: 0s - loss: 0.4736 - acc: 0.811 - 0s 218us/step - loss: 0.4767 - acc: 0.8081 - val_loss: 0.4422 - val_acc: 0.8497\n",
      "Epoch 55/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5421 - acc: 0.750 - ETA: 0s - loss: 0.4964 - acc: 0.798 - ETA: 0s - loss: 0.4816 - acc: 0.809 - 0s 218us/step - loss: 0.4778 - acc: 0.8125 - val_loss: 0.4371 - val_acc: 0.8497\n",
      "Epoch 56/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4697 - acc: 0.843 - ETA: 0s - loss: 0.4876 - acc: 0.786 - ETA: 0s - loss: 0.4735 - acc: 0.807 - 0s 211us/step - loss: 0.4677 - acc: 0.8169 - val_loss: 0.4324 - val_acc: 0.8555\n",
      "Epoch 57/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5214 - acc: 0.812 - ETA: 0s - loss: 0.4674 - acc: 0.812 - ETA: 0s - loss: 0.4687 - acc: 0.814 - 0s 210us/step - loss: 0.4682 - acc: 0.8140 - val_loss: 0.4265 - val_acc: 0.8439\n",
      "Epoch 58/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3821 - acc: 0.906 - ETA: 0s - loss: 0.4638 - acc: 0.817 - 0s 195us/step - loss: 0.4634 - acc: 0.8154 - val_loss: 0.4231 - val_acc: 0.8555\n",
      "Epoch 59/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4500 - acc: 0.875 - ETA: 0s - loss: 0.4771 - acc: 0.816 - ETA: 0s - loss: 0.4589 - acc: 0.825 - 0s 224us/step - loss: 0.4579 - acc: 0.8183 - val_loss: 0.4177 - val_acc: 0.8439\n",
      "Epoch 60/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4630 - acc: 0.781 - ETA: 0s - loss: 0.4822 - acc: 0.798 - ETA: 0s - loss: 0.4803 - acc: 0.798 - 0s 226us/step - loss: 0.4643 - acc: 0.8110 - val_loss: 0.4149 - val_acc: 0.8555\n",
      "Epoch 61/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4078 - acc: 0.843 - ETA: 0s - loss: 0.4609 - acc: 0.812 - ETA: 0s - loss: 0.4593 - acc: 0.810 - 0s 227us/step - loss: 0.4520 - acc: 0.8198 - val_loss: 0.4109 - val_acc: 0.8555\n",
      "Epoch 62/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4385 - acc: 0.875 - ETA: 0s - loss: 0.4672 - acc: 0.818 - ETA: 0s - loss: 0.4586 - acc: 0.818 - 0s 205us/step - loss: 0.4548 - acc: 0.8212 - val_loss: 0.4074 - val_acc: 0.8497\n",
      "Epoch 63/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4594 - acc: 0.812 - ETA: 0s - loss: 0.4535 - acc: 0.809 - ETA: 0s - loss: 0.4443 - acc: 0.818 - 0s 214us/step - loss: 0.4478 - acc: 0.8198 - val_loss: 0.4040 - val_acc: 0.8497\n",
      "Epoch 64/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3905 - acc: 0.875 - ETA: 0s - loss: 0.4423 - acc: 0.831 - ETA: 0s - loss: 0.4437 - acc: 0.817 - 0s 219us/step - loss: 0.4408 - acc: 0.8198 - val_loss: 0.3998 - val_acc: 0.8555\n",
      "Epoch 65/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4445 - acc: 0.781 - ETA: 0s - loss: 0.4614 - acc: 0.798 - ETA: 0s - loss: 0.4375 - acc: 0.821 - 0s 232us/step - loss: 0.4378 - acc: 0.8270 - val_loss: 0.3959 - val_acc: 0.8555\n",
      "Epoch 66/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3754 - acc: 0.781 - ETA: 0s - loss: 0.4413 - acc: 0.812 - ETA: 0s - loss: 0.4376 - acc: 0.812 - 0s 229us/step - loss: 0.4389 - acc: 0.8169 - val_loss: 0.3928 - val_acc: 0.8555\n",
      "Epoch 67/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4376 - acc: 0.781 - ETA: 0s - loss: 0.4202 - acc: 0.836 - ETA: 0s - loss: 0.4179 - acc: 0.841 - 0s 232us/step - loss: 0.4335 - acc: 0.8270 - val_loss: 0.3908 - val_acc: 0.8497\n",
      "Epoch 68/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4549 - acc: 0.843 - ETA: 0s - loss: 0.4467 - acc: 0.819 - ETA: 0s - loss: 0.4357 - acc: 0.827 - 0s 237us/step - loss: 0.4356 - acc: 0.8270 - val_loss: 0.3895 - val_acc: 0.8555\n",
      "Epoch 69/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4945 - acc: 0.750 - ETA: 0s - loss: 0.4183 - acc: 0.830 - ETA: 0s - loss: 0.4164 - acc: 0.837 - 0s 221us/step - loss: 0.4265 - acc: 0.8299 - val_loss: 0.3860 - val_acc: 0.8497\n",
      "Epoch 70/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4556 - acc: 0.781 - ETA: 0s - loss: 0.4432 - acc: 0.826 - ETA: 0s - loss: 0.4294 - acc: 0.830 - 0s 215us/step - loss: 0.4292 - acc: 0.8299 - val_loss: 0.3853 - val_acc: 0.8555\n",
      "Epoch 71/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4398 - acc: 0.750 - ETA: 0s - loss: 0.4183 - acc: 0.829 - ETA: 0s - loss: 0.4276 - acc: 0.825 - 0s 199us/step - loss: 0.4243 - acc: 0.8256 - val_loss: 0.3801 - val_acc: 0.8497\n",
      "Epoch 72/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4480 - acc: 0.812 - ETA: 0s - loss: 0.4514 - acc: 0.795 - ETA: 0s - loss: 0.4222 - acc: 0.822 - 0s 213us/step - loss: 0.4194 - acc: 0.8270 - val_loss: 0.3777 - val_acc: 0.8497\n",
      "Epoch 73/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4478 - acc: 0.781 - ETA: 0s - loss: 0.3998 - acc: 0.831 - ETA: 0s - loss: 0.4115 - acc: 0.826 - 0s 220us/step - loss: 0.4216 - acc: 0.8183 - val_loss: 0.3762 - val_acc: 0.8555\n",
      "Epoch 74/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4449 - acc: 0.812 - ETA: 0s - loss: 0.4423 - acc: 0.795 - ETA: 0s - loss: 0.4177 - acc: 0.821 - 0s 215us/step - loss: 0.4178 - acc: 0.8270 - val_loss: 0.3749 - val_acc: 0.8555\n",
      "Epoch 75/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5100 - acc: 0.718 - ETA: 0s - loss: 0.3988 - acc: 0.860 - ETA: 0s - loss: 0.4077 - acc: 0.834 - 0s 200us/step - loss: 0.4121 - acc: 0.8314 - val_loss: 0.3721 - val_acc: 0.8555\n",
      "Epoch 76/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4312 - acc: 0.875 - ETA: 0s - loss: 0.4424 - acc: 0.806 - ETA: 0s - loss: 0.4234 - acc: 0.830 - 0s 209us/step - loss: 0.4203 - acc: 0.8343 - val_loss: 0.3719 - val_acc: 0.8555\n",
      "Epoch 77/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4502 - acc: 0.812 - ETA: 0s - loss: 0.4244 - acc: 0.822 - ETA: 0s - loss: 0.4173 - acc: 0.823 - 0s 221us/step - loss: 0.4164 - acc: 0.8256 - val_loss: 0.3678 - val_acc: 0.8555\n",
      "Epoch 78/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3938 - acc: 0.812 - ETA: 0s - loss: 0.4137 - acc: 0.828 - ETA: 0s - loss: 0.4063 - acc: 0.832 - 0s 222us/step - loss: 0.4112 - acc: 0.8314 - val_loss: 0.3672 - val_acc: 0.8555\n",
      "Epoch 79/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4631 - acc: 0.781 - ETA: 0s - loss: 0.3935 - acc: 0.838 - ETA: 0s - loss: 0.4121 - acc: 0.831 - 0s 212us/step - loss: 0.4115 - acc: 0.8343 - val_loss: 0.3648 - val_acc: 0.8555\n",
      "Epoch 80/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4433 - acc: 0.812 - ETA: 0s - loss: 0.4409 - acc: 0.806 - ETA: 0s - loss: 0.4101 - acc: 0.836 - 0s 214us/step - loss: 0.4120 - acc: 0.8343 - val_loss: 0.3646 - val_acc: 0.8555\n",
      "Epoch 81/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3673 - acc: 0.843 - ETA: 0s - loss: 0.4019 - acc: 0.840 - ETA: 0s - loss: 0.3966 - acc: 0.835 - 0s 221us/step - loss: 0.4028 - acc: 0.8328 - val_loss: 0.3603 - val_acc: 0.8555\n",
      "Epoch 82/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4145 - acc: 0.812 - ETA: 0s - loss: 0.4281 - acc: 0.812 - ETA: 0s - loss: 0.4141 - acc: 0.826 - 0s 233us/step - loss: 0.4049 - acc: 0.8328 - val_loss: 0.3618 - val_acc: 0.8555\n",
      "Epoch 83/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4821 - acc: 0.875 - ETA: 0s - loss: 0.4416 - acc: 0.816 - ETA: 0s - loss: 0.4144 - acc: 0.825 - 0s 223us/step - loss: 0.4054 - acc: 0.8328 - val_loss: 0.3609 - val_acc: 0.8555\n",
      "Epoch 84/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3868 - acc: 0.843 - ETA: 0s - loss: 0.4066 - acc: 0.837 - ETA: 0s - loss: 0.4164 - acc: 0.833 - 0s 209us/step - loss: 0.4049 - acc: 0.8401 - val_loss: 0.3575 - val_acc: 0.8555\n",
      "Epoch 85/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3539 - acc: 0.843 - ETA: 0s - loss: 0.4047 - acc: 0.836 - ETA: 0s - loss: 0.4200 - acc: 0.833 - 0s 226us/step - loss: 0.4126 - acc: 0.8343 - val_loss: 0.3571 - val_acc: 0.8613\n",
      "Epoch 86/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4571 - acc: 0.781 - ETA: 0s - loss: 0.3992 - acc: 0.821 - ETA: 0s - loss: 0.4131 - acc: 0.826 - 0s 216us/step - loss: 0.4052 - acc: 0.8328 - val_loss: 0.3553 - val_acc: 0.8613\n",
      "Epoch 87/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4244 - acc: 0.812 - ETA: 0s - loss: 0.4097 - acc: 0.822 - ETA: 0s - loss: 0.4034 - acc: 0.823 - 0s 230us/step - loss: 0.4000 - acc: 0.8314 - val_loss: 0.3543 - val_acc: 0.8613\n",
      "Epoch 88/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3488 - acc: 0.843 - ETA: 0s - loss: 0.4150 - acc: 0.832 - ETA: 0s - loss: 0.4045 - acc: 0.831 - 0s 238us/step - loss: 0.3980 - acc: 0.8358 - val_loss: 0.3524 - val_acc: 0.8613\n",
      "Epoch 89/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4026 - acc: 0.875 - ETA: 0s - loss: 0.3760 - acc: 0.853 - ETA: 0s - loss: 0.3817 - acc: 0.843 - 0s 230us/step - loss: 0.3963 - acc: 0.8372 - val_loss: 0.3500 - val_acc: 0.8613\n",
      "Epoch 90/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5597 - acc: 0.781 - ETA: 0s - loss: 0.3680 - acc: 0.854 - ETA: 0s - loss: 0.3935 - acc: 0.841 - 0s 240us/step - loss: 0.3965 - acc: 0.8387 - val_loss: 0.3500 - val_acc: 0.8613\n",
      "Epoch 91/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5331 - acc: 0.875 - ETA: 0s - loss: 0.4056 - acc: 0.829 - ETA: 0s - loss: 0.4020 - acc: 0.834 - 0s 232us/step - loss: 0.3915 - acc: 0.8372 - val_loss: 0.3492 - val_acc: 0.8555\n",
      "Epoch 92/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5500 - acc: 0.812 - ETA: 0s - loss: 0.3939 - acc: 0.840 - ETA: 0s - loss: 0.4079 - acc: 0.830 - 0s 228us/step - loss: 0.3900 - acc: 0.8416 - val_loss: 0.3468 - val_acc: 0.8613\n",
      "Epoch 93/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3983 - acc: 0.875 - ETA: 0s - loss: 0.3546 - acc: 0.863 - ETA: 0s - loss: 0.3709 - acc: 0.846 - ETA: 0s - loss: 0.3909 - acc: 0.832 - 0s 303us/step - loss: 0.3864 - acc: 0.8372 - val_loss: 0.3472 - val_acc: 0.8555\n",
      "Epoch 94/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3453 - acc: 0.875 - ETA: 0s - loss: 0.3780 - acc: 0.847 - ETA: 0s - loss: 0.4107 - acc: 0.827 - 0s 254us/step - loss: 0.3837 - acc: 0.8401 - val_loss: 0.3484 - val_acc: 0.8613\n",
      "Epoch 95/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4132 - acc: 0.781 - ETA: 0s - loss: 0.3698 - acc: 0.859 - 0s 213us/step - loss: 0.3791 - acc: 0.8459 - val_loss: 0.3442 - val_acc: 0.8555\n",
      "Epoch 96/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3311 - acc: 0.843 - ETA: 0s - loss: 0.3711 - acc: 0.832 - ETA: 0s - loss: 0.3933 - acc: 0.836 - 0s 215us/step - loss: 0.3912 - acc: 0.8372 - val_loss: 0.3418 - val_acc: 0.8613\n",
      "Epoch 97/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3860 - acc: 0.812 - ETA: 0s - loss: 0.3692 - acc: 0.857 - ETA: 0s - loss: 0.3882 - acc: 0.847 - 0s 237us/step - loss: 0.3884 - acc: 0.8488 - val_loss: 0.3453 - val_acc: 0.8613\n",
      "Epoch 98/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.4394 - acc: 0.812 - ETA: 0s - loss: 0.3748 - acc: 0.846 - ETA: 0s - loss: 0.3880 - acc: 0.843 - 0s 214us/step - loss: 0.3903 - acc: 0.8401 - val_loss: 0.3423 - val_acc: 0.8613\n",
      "Epoch 99/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.3620 - acc: 0.875 - ETA: 0s - loss: 0.4038 - acc: 0.835 - ETA: 0s - loss: 0.3870 - acc: 0.840 - 0s 213us/step - loss: 0.3855 - acc: 0.8416 - val_loss: 0.3422 - val_acc: 0.8613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.5213 - acc: 0.781 - ETA: 0s - loss: 0.3707 - acc: 0.843 - ETA: 0s - loss: 0.3733 - acc: 0.845 - 0s 222us/step - loss: 0.3823 - acc: 0.8445 - val_loss: 0.3418 - val_acc: 0.8613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de7d9810f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dim = len(train_vecs_religion[0])\n",
    "num_data = len(train_vecs_religion)\n",
    "num_data_test = len(test_vecs_religion)\n",
    "\n",
    "train_vecs_religion = train_vecs_religion.reshape((num_data, timesteps, data_dim))\n",
    "y_train_religion = y_train_religion.reshape((num_data, num_class))\n",
    "test_vecs_religion = test_vecs_religion.reshape((num_data_test, timesteps, data_dim))\n",
    "y_test_religion = y_test_religion.reshape((num_data_test, num_class))\n",
    "\n",
    "model_ad_religion = Sequential()\n",
    "model_ad_religion.add(Bidirectional(LSTM(hidden_size, input_shape=(timesteps, data_dim)), merge_mode='concat'))\n",
    "model_ad_religion.add(Dropout(0.5))\n",
    "model_ad_religion.add(Dense(1, activation='sigmoid'))\n",
    "model_ad_religion.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_ad_religion.fit(train_vecs_religion, y_train_religion, epochs=num_epochs, validation_data=[test_vecs_religion, y_test_religion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.861271676300578 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.91      0.89       103\n",
      "          1       0.86      0.79      0.82        70\n",
      "\n",
      "avg / total       0.86      0.86      0.86       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction['ad_religion'] = model_ad_religion.predict(test_vecs_religion)\n",
    "\n",
    "for i in range(len(prediction['ad_religion'])):\n",
    "    prediction['ad_religion'][i][0] = round(prediction['ad_religion'][i][0])\n",
    "\n",
    "accuracy['ad_religion'] = accuracy_score(y_test_religion, prediction['ad_religion'])\n",
    "print(\"Accuracy: \", accuracy['ad_religion'], \"\\n\")\n",
    "print(classification_report(y_test_religion, prediction['ad_religion'], labels = [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model_ad_physics, open('model_ad_physics.sav', 'wb'))\n",
    "pickle.dump(model_ad_race, open('model_ad_race.sav', 'wb'))\n",
    "pickle.dump(model_ad_religion, open('model_ad_religion.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ad_physics = pickle.load(open('model_ad_physics.sav', 'rb'))\n",
    "model_ad_race = pickle.load(open('model_ad_race.sav', 'rb'))\n",
    "model_ad_religion = pickle.load(open('model_ad_religion.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Detection Model for Predict New Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1:exist, 0:not given\n",
    "\n",
    "def predict_aspect(text):\n",
    "    text = normalizer(text)\n",
    "    tokens = nltk.WhitespaceTokenizer().tokenize(text)\n",
    "    vecs = build_Word_Vector(tokens, 200)\n",
    "    vecs = vecs.reshape((1, 1, 200))\n",
    "    aspect = {}\n",
    "    aspect['physics'] = int(round(model_ad_physics.predict(vecs)[0][0]))\n",
    "    aspect['race'] = int(round(model_ad_race.predict(vecs)[0][0]))\n",
    "    aspect['religion'] = int(round(model_ad_religion.predict(vecs)[0][0]))\n",
    "    return aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'physics': 1, 'race': 1, 'religion': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_aspect(\"nigga bitch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
