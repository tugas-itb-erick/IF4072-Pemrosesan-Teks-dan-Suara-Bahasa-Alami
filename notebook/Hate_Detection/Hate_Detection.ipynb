{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>physics</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't get why negroes always traveling to wh...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All of you fattys need to stop trying to make ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmao how funny that true does know where the c...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@art_is_forever when did she publicly thank him?</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Post a picture of Khloe already!!!!! Come on!!!!</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  physics  race  religion\n",
       "0  I don't get why negroes always traveling to wh...       -1     1        -1\n",
       "1  All of you fattys need to stop trying to make ...        1    -1        -1\n",
       "2  lmao how funny that true does know where the c...       -1    -1        -1\n",
       "3   @art_is_forever when did she publicly thank him?       -1    -1        -1\n",
       "4   Post a picture of Khloe already!!!!! Come on!!!!       -1    -1        -1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "csv = 'hate_speech_dataset.csv'\n",
    "data = pd.read_csv(csv,index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.sentence\n",
    "y_physics = data.physics\n",
    "y_race = data.race\n",
    "y_religion = data.religion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, nltk, string\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def expand_contractions(text) :\n",
    "    pattern = re.compile(\"({})\".format(\"|\".join(CONTRACTION_MAP.keys())),flags = re.DOTALL| re.IGNORECASE)\n",
    "    \n",
    "    def replace_text(t):\n",
    "        txt = t.group(0)\n",
    "        if txt.lower() in CONTRACTION_MAP.keys():\n",
    "            return CONTRACTION_MAP[txt.lower()]\n",
    "        \n",
    "    expand_text = pattern.sub(replace_text,text)\n",
    "    return expand_text\n",
    "\n",
    "def remove_repeated_characters(word):\n",
    "    pattern = re.compile(r\"(\\w*)(\\w)\\2(\\w*)\")\n",
    "    substitution_pattern = r\"\\1\\2\\3\"\n",
    "    while True:\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        new_word = pattern.sub(substitution_pattern,word)\n",
    "        if new_word != word:\n",
    "            word = new_word\n",
    "            continue\n",
    "        else:\n",
    "            return new_word\n",
    "\n",
    "def spelling_checker(word):\n",
    "    checker = suggest(word)\n",
    "    return checker[0][0]\n",
    "\n",
    "def clean_symbol(text):\n",
    "    cleanr = re.compile('&#[0-9]+;')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def clean_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "    \n",
    "def normalizer(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text.lower(), flags=re.MULTILINE) #remove url\n",
    "    text = re.sub('@[^\\s]+','',text) #remove username\n",
    "    text = clean_emoji(text)\n",
    "    text = clean_symbol(text)\n",
    "    expand = expand_contractions(text)\n",
    "    pattern = re.compile(\"[{}]\".format(re.escape(string.punctuation)))\n",
    "    filter_char =  filter(None,[pattern.sub('' ,expand)])\n",
    "    text_filter_char =  \" \".join(filter_char)\n",
    "    tokens = nltk.WhitespaceTokenizer().tokenize(text_filter_char)\n",
    "\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    stems = [stemmer.stem(t) for t in lemmas]\n",
    "    filtered_result = list(filter(lambda l: l not in stop_words, stems))\n",
    "    concate = ' '.join(filtered_result)\n",
    "    return concate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    get whi negro alway travel white countri take ...\n",
       "1                   fatti need stop tri make fat thing\n",
       "2                  lmao funni true doe know camara lol\n",
       "3                                       publicli thank\n",
       "4                       post pictur khloe alreadi come\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.apply(normalizer)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_physics_clean = []\n",
    "X_race_clean = []\n",
    "X_religion_clean = []\n",
    "\n",
    "X_physics_no_hate = []\n",
    "X_race_no_hate = []\n",
    "X_religion_no_hate = []\n",
    "\n",
    "X_physics_hate = []\n",
    "X_race_hate = []\n",
    "X_religion_hate = []\n",
    "\n",
    "y_physics_clean = []\n",
    "y_race_clean = []\n",
    "y_religion_clean = []\n",
    "\n",
    "y_physics_no_hate = []\n",
    "y_race_no_hate = []\n",
    "y_religion_no_hate = []\n",
    "\n",
    "y_physics_hate = []\n",
    "y_race_hate = []\n",
    "y_religion_hate = []\n",
    "\n",
    "for i in range(len(y_physics)):\n",
    "    if y_physics[i] != -1:\n",
    "        if y_physics[i] == 0:\n",
    "            X_physics_no_hate.append(X[i])\n",
    "            y_physics_no_hate.append(y_physics[i])\n",
    "        else:\n",
    "            X_physics_hate.append(X[i])\n",
    "            y_physics_hate.append(y_physics[i])\n",
    "\n",
    "for i in range(len(y_race)):\n",
    "    if y_race[i] != -1:\n",
    "        if y_race[i] == 0:\n",
    "            X_race_no_hate.append(X[i])\n",
    "            y_race_no_hate.append(y_race[i])\n",
    "        else:\n",
    "            X_race_hate.append(X[i])\n",
    "            y_race_hate.append(y_race[i])\n",
    "\n",
    "for i in range(len(y_religion)):\n",
    "    if y_religion[i] != -1:\n",
    "        if y_religion[i] == 0:\n",
    "            X_religion_no_hate.append(X[i])\n",
    "            y_religion_no_hate.append(y_religion[i])\n",
    "        else:\n",
    "            X_religion_hate.append(X[i])\n",
    "            y_religion_hate.append(y_religion[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling Imbalance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics 754 1131\n",
      "race 112 168\n",
      "1885\n",
      "280\n",
      "344\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "import numpy as np\n",
    "\n",
    "def is_balance(no_hate, hate):\n",
    "    if no_hate == hate:\n",
    "        return True\n",
    "    elif no_hate > hate:\n",
    "        return (no_hate <= round(3/2 * hate))\n",
    "    else: # hate > no_hate\n",
    "        return (hate <= round(3/2 * no_hate))\n",
    "\n",
    "def get_distribution(no_hate, hate):\n",
    "    if no_hate == hate:\n",
    "        return no_hate, hate\n",
    "    elif no_hate > hate:\n",
    "        return round(3/2 * hate), hate\n",
    "    else: # hate > no_hate\n",
    "        return no_hate, round(3/2 * no_hate)\n",
    "\n",
    "# undesampling physics aspect (if needed)\n",
    "if is_balance(len(y_physics_no_hate), len(y_physics_hate)):\n",
    "    X_physics_clean.extend(X_physics_no_hate)\n",
    "    y_physics_clean.extend(y_physics_no_hate)\n",
    "    X_physics_clean.extend(X_physics_hate)\n",
    "    y_physics_clean.extend(y_physics_hate)\n",
    "else:\n",
    "    no_hate, hate = get_distribution(len(y_physics_no_hate), len(y_physics_hate))\n",
    "    print(\"physics\" + \" \" + str(no_hate) + \" \" + str(hate))\n",
    "    X_physics_clean.extend(X_physics_no_hate[:no_hate])\n",
    "    y_physics_clean.extend(y_physics_no_hate[:no_hate])\n",
    "    X_physics_clean.extend(X_physics_hate[:hate])\n",
    "    y_physics_clean.extend(y_physics_hate[:hate])\n",
    "\n",
    "# undesampling race aspect (if needed)\n",
    "if is_balance(len(y_race_no_hate), len(y_race_hate)):\n",
    "    X_race_clean.extend(X_race_no_hate)\n",
    "    y_race_clean.extend(y_race_no_hate)\n",
    "    X_race_clean.extend(X_race_hate)\n",
    "    y_race_clean.extend(y_race_hate)\n",
    "else:\n",
    "    no_hate, hate = get_distribution(len(y_race_no_hate), len(y_race_hate))\n",
    "    print(\"race\" + \" \" + str(no_hate) + \" \" + str(hate))\n",
    "    X_race_clean.extend(X_race_no_hate[:no_hate])\n",
    "    y_race_clean.extend(y_race_no_hate[:no_hate])\n",
    "    X_race_clean.extend(X_race_hate[:hate])\n",
    "    y_race_clean.extend(y_race_hate[:hate])\n",
    "\n",
    "# undesampling religion aspect (if needed)\n",
    "if is_balance(len(y_religion_no_hate), len(y_religion_hate)):\n",
    "    X_religion_clean.extend(X_religion_no_hate)\n",
    "    y_religion_clean.extend(y_religion_no_hate)\n",
    "    X_religion_clean.extend(X_religion_hate)\n",
    "    y_religion_clean.extend(y_religion_hate)\n",
    "else:\n",
    "    no_hate, hate = get_distribution(len(y_religion_no_hate), len(y_religion_hate))\n",
    "    print(\"religion\" + \" \" + str(no_hate) + \" \" + str(hate))\n",
    "    X_religion_clean.extend(X_religion_no_hate[:no_hate])\n",
    "    y_religion_clean.extend(y_religion_no_hate[:no_hate])\n",
    "    X_religion_clean.extend(X_religion_hate[:hate])\n",
    "    y_religion_clean.extend(y_religion_hate[:hate])\n",
    "\n",
    "X_physics_clean = np.asarray(X_physics_clean)\n",
    "X_race_clean = np.asarray(X_race_clean)\n",
    "X_religion_clean = np.asarray(X_religion_clean)\n",
    "\n",
    "y_physics_clean = np.asarray(y_physics_clean)\n",
    "y_race_clean = np.asarray(y_race_clean)\n",
    "y_religion_clean = np.asarray(y_religion_clean)\n",
    "\n",
    "# random order physics dataset\n",
    "random_idxs = random.choice(len(y_physics_clean), len(y_physics_clean), replace=False)\n",
    "X_physics_clean = X_physics_clean[random_idxs]\n",
    "y_physics_clean = y_physics_clean[random_idxs]\n",
    "\n",
    "# random order race dataset\n",
    "random_idxs = random.choice(len(y_race_clean), len(y_race_clean), replace=False)\n",
    "X_race_clean = X_race_clean[random_idxs]\n",
    "y_race_clean = y_race_clean[random_idxs]\n",
    "\n",
    "# random order religion dataset\n",
    "random_idxs = random.choice(len(y_religion_clean), len(y_religion_clean), replace=False)\n",
    "X_religion_clean = X_religion_clean[random_idxs]\n",
    "y_religion_clean = y_religion_clean[random_idxs]\n",
    "\n",
    "print(len(y_physics_clean))\n",
    "print(len(y_race_clean))\n",
    "print(len(y_religion_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_physics = pd.DataFrame({ 'sentence': X_physics_clean, 'physics': y_physics_clean }, columns = ['sentence', 'physics'])\n",
    "df_race = pd.DataFrame({ 'sentence': X_race_clean, 'race': y_race_clean }, columns = ['sentence', 'race'])\n",
    "df_religion = pd.DataFrame({ 'sentence': X_religion_clean, 'religion': y_religion_clean }, columns = ['sentence', 'religion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_physics, x_test_physics, y_train_physics, y_test_physics = train_test_split(df_physics['sentence'], df_physics['physics'], \n",
    "                                                                                    test_size = 0.2, random_state = 10)\n",
    "\n",
    "x_train_race, x_test_race, y_train_race, y_test_race = train_test_split(df_race['sentence'], df_race['race'], \n",
    "                                                                        test_size = 0.2, random_state = 10)\n",
    "\n",
    "x_train_religion, x_test_religion, y_train_religion, y_test_religion = train_test_split(df_religion['sentence'], df_religion['religion'], \n",
    "                                                                                        test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def labelize_text(text,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(text.index, text):\n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "\n",
    "all_x_w2v = labelize_text(X, 'ALL')\n",
    "\n",
    "# physics\n",
    "x_train_physics = labelize_text(x_train_physics, 'TRAIN')\n",
    "x_test_physics = labelize_text(x_test_physics, 'TEST')\n",
    "\n",
    "# race\n",
    "x_train_race = labelize_text(x_train_race, 'TRAIN')\n",
    "x_test_race = labelize_text(x_test_race, 'TEST')\n",
    "\n",
    "# religion\n",
    "x_train_religion = labelize_text(x_train_religion, 'TRAIN')\n",
    "x_test_religion = labelize_text(x_test_religion, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 4174/4174 [00:00<00:00, 454822.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4174/4174 [00:00<00:00, 1733368.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7942, 26188)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "import numpy as np\n",
    "\n",
    "model_w2v = Word2Vec(size=200, min_count=20)\n",
    "model_w2v.build_vocab([x.words for x in tqdm(all_x_w2v)])\n",
    "model_w2v.train([x.words for x in tqdm(all_x_w2v)], total_examples=len(all_x_w2v), epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Document Vector using Average Word Vector With TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "matrix = vectorizer.fit_transform([x.words for x in all_x_w2v])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "def build_Word_Vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: \n",
    "            \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "1508it [00:00, 7546.53it/s]\n",
      "377it [00:00, 10961.21it/s]\n",
      "224it [00:00, 6312.61it/s]\n",
      "56it [00:00, 5238.55it/s]\n",
      "275it [00:00, 6961.69it/s]\n",
      "69it [00:00, 3416.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# physics\n",
    "train_vecs_physics = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_train_physics))])\n",
    "test_vecs_physics = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_test_physics))])\n",
    "\n",
    "# race\n",
    "train_vecs_race = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_train_race))])\n",
    "test_vecs_race = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_test_race))])\n",
    "\n",
    "# religion\n",
    "train_vecs_religion = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_train_religion))])\n",
    "test_vecs_religion = np.concatenate([build_Word_Vector(z, 200) for z in tqdm(map(lambda x: x.words, x_test_religion))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM Hate Detection Model for Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  del sys.path[0]\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1508 samples, validate on 377 samples\n",
      "Epoch 1/100\n",
      "1508/1508 [==============================] - ETA: 2:01 - loss: 0.6932 - acc: 0.437 - ETA: 11s - loss: 0.6921 - acc: 0.611 - ETA: 4s - loss: 0.6917 - acc: 0.6011 - ETA: 2s - loss: 0.6911 - acc: 0.595 - ETA: 1s - loss: 0.6904 - acc: 0.598 - ETA: 0s - loss: 0.6905 - acc: 0.589 - 3s 2ms/step - loss: 0.6896 - acc: 0.5968 - val_loss: 0.6872 - val_acc: 0.5862\n",
      "Epoch 2/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6842 - acc: 0.625 - ETA: 0s - loss: 0.6873 - acc: 0.576 - ETA: 0s - loss: 0.6861 - acc: 0.588 - ETA: 0s - loss: 0.6854 - acc: 0.592 - ETA: 0s - loss: 0.6844 - acc: 0.598 - ETA: 0s - loss: 0.6836 - acc: 0.601 - 0s 220us/step - loss: 0.6830 - acc: 0.6034 - val_loss: 0.6824 - val_acc: 0.5862\n",
      "Epoch 3/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6803 - acc: 0.593 - ETA: 0s - loss: 0.6758 - acc: 0.628 - ETA: 0s - loss: 0.6776 - acc: 0.613 - ETA: 0s - loss: 0.6759 - acc: 0.619 - ETA: 0s - loss: 0.6781 - acc: 0.604 - ETA: 0s - loss: 0.6782 - acc: 0.601 - 0s 222us/step - loss: 0.6779 - acc: 0.6034 - val_loss: 0.6786 - val_acc: 0.5862\n",
      "Epoch 4/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6428 - acc: 0.750 - ETA: 0s - loss: 0.6673 - acc: 0.640 - ETA: 0s - loss: 0.6698 - acc: 0.621 - ETA: 0s - loss: 0.6717 - acc: 0.610 - ETA: 0s - loss: 0.6737 - acc: 0.601 - ETA: 0s - loss: 0.6734 - acc: 0.600 - 0s 213us/step - loss: 0.6722 - acc: 0.6034 - val_loss: 0.6759 - val_acc: 0.5862\n",
      "Epoch 5/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6510 - acc: 0.687 - ETA: 0s - loss: 0.6646 - acc: 0.628 - ETA: 0s - loss: 0.6675 - acc: 0.615 - ETA: 0s - loss: 0.6718 - acc: 0.598 - ETA: 0s - loss: 0.6724 - acc: 0.594 - ETA: 0s - loss: 0.6693 - acc: 0.604 - 0s 218us/step - loss: 0.6691 - acc: 0.6034 - val_loss: 0.6742 - val_acc: 0.5862\n",
      "Epoch 6/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6658 - acc: 0.625 - ETA: 0s - loss: 0.6568 - acc: 0.643 - ETA: 0s - loss: 0.6664 - acc: 0.612 - ETA: 0s - loss: 0.6683 - acc: 0.605 - ETA: 0s - loss: 0.6707 - acc: 0.597 - ETA: 0s - loss: 0.6698 - acc: 0.599 - 0s 213us/step - loss: 0.6687 - acc: 0.6034 - val_loss: 0.6732 - val_acc: 0.5862\n",
      "Epoch 7/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6750 - acc: 0.562 - ETA: 0s - loss: 0.6794 - acc: 0.575 - ETA: 0s - loss: 0.6714 - acc: 0.588 - ETA: 0s - loss: 0.6701 - acc: 0.592 - ETA: 0s - loss: 0.6714 - acc: 0.588 - ETA: 0s - loss: 0.6664 - acc: 0.602 - 0s 214us/step - loss: 0.6661 - acc: 0.6034 - val_loss: 0.6724 - val_acc: 0.5862\n",
      "Epoch 8/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6968 - acc: 0.500 - ETA: 0s - loss: 0.6551 - acc: 0.631 - ETA: 0s - loss: 0.6593 - acc: 0.612 - ETA: 0s - loss: 0.6648 - acc: 0.602 - ETA: 0s - loss: 0.6657 - acc: 0.601 - ETA: 0s - loss: 0.6631 - acc: 0.607 - 0s 215us/step - loss: 0.6649 - acc: 0.6034 - val_loss: 0.6717 - val_acc: 0.5862\n",
      "Epoch 9/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5990 - acc: 0.750 - ETA: 0s - loss: 0.6675 - acc: 0.602 - ETA: 0s - loss: 0.6744 - acc: 0.580 - ETA: 0s - loss: 0.6713 - acc: 0.590 - ETA: 0s - loss: 0.6659 - acc: 0.600 - ETA: 0s - loss: 0.6660 - acc: 0.600 - 0s 226us/step - loss: 0.6642 - acc: 0.6034 - val_loss: 0.6708 - val_acc: 0.5862\n",
      "Epoch 10/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6456 - acc: 0.656 - ETA: 0s - loss: 0.6631 - acc: 0.603 - ETA: 0s - loss: 0.6667 - acc: 0.588 - ETA: 0s - loss: 0.6616 - acc: 0.601 - ETA: 0s - loss: 0.6612 - acc: 0.605 - ETA: 0s - loss: 0.6618 - acc: 0.603 - 0s 224us/step - loss: 0.6622 - acc: 0.6034 - val_loss: 0.6700 - val_acc: 0.5862\n",
      "Epoch 11/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6463 - acc: 0.593 - ETA: 0s - loss: 0.6462 - acc: 0.632 - ETA: 0s - loss: 0.6578 - acc: 0.611 - ETA: 0s - loss: 0.6607 - acc: 0.604 - ETA: 0s - loss: 0.6605 - acc: 0.604 - ETA: 0s - loss: 0.6602 - acc: 0.603 - 0s 202us/step - loss: 0.6602 - acc: 0.6034 - val_loss: 0.6690 - val_acc: 0.5862\n",
      "Epoch 12/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6262 - acc: 0.656 - ETA: 0s - loss: 0.6570 - acc: 0.612 - ETA: 0s - loss: 0.6432 - acc: 0.640 - ETA: 0s - loss: 0.6543 - acc: 0.614 - ETA: 0s - loss: 0.6583 - acc: 0.608 - ETA: 0s - loss: 0.6599 - acc: 0.603 - 0s 217us/step - loss: 0.6597 - acc: 0.6034 - val_loss: 0.6679 - val_acc: 0.5862\n",
      "Epoch 13/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5896 - acc: 0.750 - ETA: 0s - loss: 0.6626 - acc: 0.593 - ETA: 0s - loss: 0.6558 - acc: 0.613 - ETA: 0s - loss: 0.6600 - acc: 0.601 - ETA: 0s - loss: 0.6575 - acc: 0.608 - ETA: 0s - loss: 0.6599 - acc: 0.603 - 0s 206us/step - loss: 0.6592 - acc: 0.6034 - val_loss: 0.6666 - val_acc: 0.5862\n",
      "Epoch 14/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.7015 - acc: 0.531 - ETA: 0s - loss: 0.6793 - acc: 0.553 - ETA: 0s - loss: 0.6649 - acc: 0.582 - ETA: 0s - loss: 0.6607 - acc: 0.596 - ETA: 0s - loss: 0.6619 - acc: 0.595 - ETA: 0s - loss: 0.6597 - acc: 0.601 - 0s 221us/step - loss: 0.6588 - acc: 0.6034 - val_loss: 0.6653 - val_acc: 0.5862\n",
      "Epoch 15/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5778 - acc: 0.781 - ETA: 0s - loss: 0.6555 - acc: 0.606 - ETA: 0s - loss: 0.6581 - acc: 0.599 - ETA: 0s - loss: 0.6554 - acc: 0.605 - ETA: 0s - loss: 0.6525 - acc: 0.613 - ETA: 0s - loss: 0.6523 - acc: 0.611 - 0s 220us/step - loss: 0.6569 - acc: 0.6034 - val_loss: 0.6642 - val_acc: 0.5862\n",
      "Epoch 16/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5813 - acc: 0.781 - ETA: 0s - loss: 0.6564 - acc: 0.606 - ETA: 0s - loss: 0.6540 - acc: 0.605 - ETA: 0s - loss: 0.6587 - acc: 0.600 - ETA: 0s - loss: 0.6569 - acc: 0.604 - ETA: 0s - loss: 0.6538 - acc: 0.607 - 0s 210us/step - loss: 0.6551 - acc: 0.6034 - val_loss: 0.6628 - val_acc: 0.5862\n",
      "Epoch 17/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6773 - acc: 0.593 - ETA: 0s - loss: 0.6630 - acc: 0.586 - ETA: 0s - loss: 0.6553 - acc: 0.599 - ETA: 0s - loss: 0.6473 - acc: 0.613 - ETA: 0s - loss: 0.6505 - acc: 0.606 - ETA: 0s - loss: 0.6498 - acc: 0.605 - 0s 222us/step - loss: 0.6506 - acc: 0.6034 - val_loss: 0.6614 - val_acc: 0.5862\n",
      "Epoch 18/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6074 - acc: 0.718 - ETA: 0s - loss: 0.6430 - acc: 0.638 - ETA: 0s - loss: 0.6401 - acc: 0.630 - ETA: 0s - loss: 0.6400 - acc: 0.625 - ETA: 0s - loss: 0.6440 - acc: 0.611 - ETA: 0s - loss: 0.6498 - acc: 0.602 - 0s 218us/step - loss: 0.6498 - acc: 0.6041 - val_loss: 0.6598 - val_acc: 0.5862\n",
      "Epoch 19/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6471 - acc: 0.562 - ETA: 0s - loss: 0.6572 - acc: 0.590 - ETA: 0s - loss: 0.6537 - acc: 0.593 - ETA: 0s - loss: 0.6532 - acc: 0.597 - ETA: 0s - loss: 0.6493 - acc: 0.607 - ETA: 0s - loss: 0.6482 - acc: 0.607 - 0s 210us/step - loss: 0.6485 - acc: 0.6088 - val_loss: 0.6580 - val_acc: 0.5889\n",
      "Epoch 20/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6766 - acc: 0.531 - ETA: 0s - loss: 0.6497 - acc: 0.600 - ETA: 0s - loss: 0.6385 - acc: 0.630 - ETA: 0s - loss: 0.6446 - acc: 0.617 - ETA: 0s - loss: 0.6492 - acc: 0.608 - ETA: 0s - loss: 0.6474 - acc: 0.612 - 0s 220us/step - loss: 0.6487 - acc: 0.6114 - val_loss: 0.6560 - val_acc: 0.5889\n",
      "Epoch 21/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6899 - acc: 0.468 - ETA: 0s - loss: 0.6360 - acc: 0.615 - ETA: 0s - loss: 0.6478 - acc: 0.602 - ETA: 0s - loss: 0.6399 - acc: 0.619 - ETA: 0s - loss: 0.6371 - acc: 0.624 - ETA: 0s - loss: 0.6417 - acc: 0.613 - 0s 221us/step - loss: 0.6420 - acc: 0.6134 - val_loss: 0.6542 - val_acc: 0.5889\n",
      "Epoch 22/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6460 - acc: 0.687 - ETA: 0s - loss: 0.6485 - acc: 0.613 - ETA: 0s - loss: 0.6457 - acc: 0.608 - ETA: 0s - loss: 0.6445 - acc: 0.611 - ETA: 0s - loss: 0.6442 - acc: 0.611 - ETA: 0s - loss: 0.6444 - acc: 0.610 - 0s 217us/step - loss: 0.6432 - acc: 0.6167 - val_loss: 0.6518 - val_acc: 0.5995\n",
      "Epoch 23/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6536 - acc: 0.625 - ETA: 0s - loss: 0.6372 - acc: 0.603 - ETA: 0s - loss: 0.6409 - acc: 0.599 - ETA: 0s - loss: 0.6426 - acc: 0.599 - ETA: 0s - loss: 0.6384 - acc: 0.610 - ETA: 0s - loss: 0.6363 - acc: 0.620 - 0s 221us/step - loss: 0.6373 - acc: 0.6200 - val_loss: 0.6498 - val_acc: 0.5995\n",
      "Epoch 24/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6454 - acc: 0.656 - ETA: 0s - loss: 0.6327 - acc: 0.646 - ETA: 0s - loss: 0.6371 - acc: 0.621 - ETA: 0s - loss: 0.6395 - acc: 0.614 - ETA: 0s - loss: 0.6380 - acc: 0.615 - ETA: 0s - loss: 0.6351 - acc: 0.624 - 0s 220us/step - loss: 0.6371 - acc: 0.6187 - val_loss: 0.6476 - val_acc: 0.6074\n",
      "Epoch 25/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6495 - acc: 0.593 - ETA: 0s - loss: 0.6326 - acc: 0.618 - ETA: 0s - loss: 0.6335 - acc: 0.623 - ETA: 0s - loss: 0.6347 - acc: 0.628 - ETA: 0s - loss: 0.6326 - acc: 0.631 - ETA: 0s - loss: 0.6324 - acc: 0.625 - 0s 218us/step - loss: 0.6315 - acc: 0.6280 - val_loss: 0.6449 - val_acc: 0.6127\n",
      "Epoch 26/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6660 - acc: 0.500 - ETA: 0s - loss: 0.6221 - acc: 0.640 - ETA: 0s - loss: 0.6158 - acc: 0.652 - ETA: 0s - loss: 0.6260 - acc: 0.636 - ETA: 0s - loss: 0.6308 - acc: 0.631 - ETA: 0s - loss: 0.6298 - acc: 0.635 - 0s 212us/step - loss: 0.6298 - acc: 0.6333 - val_loss: 0.6417 - val_acc: 0.6154\n",
      "Epoch 27/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.7108 - acc: 0.500 - ETA: 0s - loss: 0.6286 - acc: 0.647 - ETA: 0s - loss: 0.6267 - acc: 0.648 - ETA: 0s - loss: 0.6233 - acc: 0.650 - ETA: 0s - loss: 0.6295 - acc: 0.638 - ETA: 0s - loss: 0.6255 - acc: 0.644 - 0s 216us/step - loss: 0.6281 - acc: 0.6379 - val_loss: 0.6387 - val_acc: 0.6180\n",
      "Epoch 28/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6246 - acc: 0.625 - ETA: 0s - loss: 0.6281 - acc: 0.638 - ETA: 0s - loss: 0.6300 - acc: 0.621 - ETA: 0s - loss: 0.6262 - acc: 0.630 - ETA: 0s - loss: 0.6255 - acc: 0.635 - ETA: 0s - loss: 0.6275 - acc: 0.643 - 0s 220us/step - loss: 0.6258 - acc: 0.6492 - val_loss: 0.6354 - val_acc: 0.6313\n",
      "Epoch 29/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6215 - acc: 0.593 - ETA: 0s - loss: 0.6349 - acc: 0.619 - ETA: 0s - loss: 0.6318 - acc: 0.639 - ETA: 0s - loss: 0.6338 - acc: 0.636 - ETA: 0s - loss: 0.6261 - acc: 0.655 - ETA: 0s - loss: 0.6229 - acc: 0.660 - 0s 221us/step - loss: 0.6210 - acc: 0.6645 - val_loss: 0.6325 - val_acc: 0.6366\n",
      "Epoch 30/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6450 - acc: 0.531 - ETA: 0s - loss: 0.6030 - acc: 0.684 - ETA: 0s - loss: 0.6139 - acc: 0.662 - ETA: 0s - loss: 0.6126 - acc: 0.662 - ETA: 0s - loss: 0.6170 - acc: 0.660 - ETA: 0s - loss: 0.6148 - acc: 0.667 - 0s 217us/step - loss: 0.6180 - acc: 0.6631 - val_loss: 0.6293 - val_acc: 0.6446\n",
      "Epoch 31/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6525 - acc: 0.593 - ETA: 0s - loss: 0.6148 - acc: 0.673 - ETA: 0s - loss: 0.6088 - acc: 0.671 - ETA: 0s - loss: 0.6100 - acc: 0.672 - ETA: 0s - loss: 0.6100 - acc: 0.675 - ETA: 0s - loss: 0.6133 - acc: 0.670 - 0s 217us/step - loss: 0.6124 - acc: 0.6737 - val_loss: 0.6258 - val_acc: 0.6552\n",
      "Epoch 32/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5383 - acc: 0.875 - ETA: 0s - loss: 0.6097 - acc: 0.687 - ETA: 0s - loss: 0.5998 - acc: 0.687 - ETA: 0s - loss: 0.6012 - acc: 0.692 - ETA: 0s - loss: 0.6085 - acc: 0.688 - ETA: 0s - loss: 0.6084 - acc: 0.689 - 0s 225us/step - loss: 0.6092 - acc: 0.6883 - val_loss: 0.6226 - val_acc: 0.6631\n",
      "Epoch 33/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5784 - acc: 0.718 - ETA: 0s - loss: 0.6101 - acc: 0.663 - ETA: 0s - loss: 0.6172 - acc: 0.678 - ETA: 0s - loss: 0.6127 - acc: 0.699 - ETA: 0s - loss: 0.6102 - acc: 0.701 - ETA: 0s - loss: 0.6063 - acc: 0.706 - 0s 228us/step - loss: 0.6032 - acc: 0.7089 - val_loss: 0.6193 - val_acc: 0.6684\n",
      "Epoch 34/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5545 - acc: 0.750 - ETA: 0s - loss: 0.5973 - acc: 0.710 - ETA: 0s - loss: 0.6076 - acc: 0.697 - ETA: 0s - loss: 0.6063 - acc: 0.700 - ETA: 0s - loss: 0.6063 - acc: 0.700 - ETA: 0s - loss: 0.5997 - acc: 0.705 - 0s 208us/step - loss: 0.6018 - acc: 0.7042 - val_loss: 0.6162 - val_acc: 0.6737\n",
      "Epoch 35/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6312 - acc: 0.562 - ETA: 0s - loss: 0.5854 - acc: 0.734 - ETA: 0s - loss: 0.5824 - acc: 0.723 - ETA: 0s - loss: 0.5870 - acc: 0.716 - ETA: 0s - loss: 0.5970 - acc: 0.704 - ETA: 0s - loss: 0.5960 - acc: 0.712 - 0s 204us/step - loss: 0.5962 - acc: 0.7115 - val_loss: 0.6125 - val_acc: 0.6817\n",
      "Epoch 36/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6136 - acc: 0.718 - ETA: 0s - loss: 0.5994 - acc: 0.730 - ETA: 0s - loss: 0.6017 - acc: 0.712 - ETA: 0s - loss: 0.5986 - acc: 0.721 - ETA: 0s - loss: 0.5981 - acc: 0.722 - ETA: 0s - loss: 0.5954 - acc: 0.728 - 0s 212us/step - loss: 0.5930 - acc: 0.7314 - val_loss: 0.6095 - val_acc: 0.6870\n",
      "Epoch 37/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6620 - acc: 0.593 - ETA: 0s - loss: 0.6107 - acc: 0.687 - ETA: 0s - loss: 0.6107 - acc: 0.685 - ETA: 0s - loss: 0.6085 - acc: 0.698 - ETA: 0s - loss: 0.5946 - acc: 0.714 - ETA: 0s - loss: 0.5870 - acc: 0.726 - 0s 217us/step - loss: 0.5869 - acc: 0.7281 - val_loss: 0.6070 - val_acc: 0.6817\n",
      "Epoch 38/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6516 - acc: 0.687 - ETA: 0s - loss: 0.5917 - acc: 0.737 - ETA: 0s - loss: 0.5816 - acc: 0.743 - ETA: 0s - loss: 0.5905 - acc: 0.730 - ETA: 0s - loss: 0.5894 - acc: 0.736 - ETA: 0s - loss: 0.5882 - acc: 0.732 - 0s 221us/step - loss: 0.5872 - acc: 0.7361 - val_loss: 0.6035 - val_acc: 0.7003\n",
      "Epoch 39/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5861 - acc: 0.718 - ETA: 0s - loss: 0.6056 - acc: 0.711 - ETA: 0s - loss: 0.5956 - acc: 0.724 - ETA: 0s - loss: 0.5904 - acc: 0.733 - ETA: 0s - loss: 0.5855 - acc: 0.739 - ETA: 0s - loss: 0.5848 - acc: 0.739 - 0s 231us/step - loss: 0.5803 - acc: 0.7493 - val_loss: 0.6002 - val_acc: 0.7029\n",
      "Epoch 40/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4989 - acc: 0.812 - ETA: 0s - loss: 0.5623 - acc: 0.768 - ETA: 0s - loss: 0.5786 - acc: 0.751 - ETA: 0s - loss: 0.5799 - acc: 0.741 - ETA: 0s - loss: 0.5865 - acc: 0.735 - ETA: 0s - loss: 0.5808 - acc: 0.741 - 0s 221us/step - loss: 0.5792 - acc: 0.7467 - val_loss: 0.5977 - val_acc: 0.7135\n",
      "Epoch 41/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5550 - acc: 0.750 - ETA: 0s - loss: 0.5705 - acc: 0.743 - ETA: 0s - loss: 0.5559 - acc: 0.769 - ETA: 0s - loss: 0.5786 - acc: 0.737 - ETA: 0s - loss: 0.5724 - acc: 0.747 - ETA: 0s - loss: 0.5729 - acc: 0.745 - 0s 226us/step - loss: 0.5778 - acc: 0.7414 - val_loss: 0.5953 - val_acc: 0.7135\n",
      "Epoch 42/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5236 - acc: 0.843 - ETA: 0s - loss: 0.5527 - acc: 0.753 - ETA: 0s - loss: 0.5672 - acc: 0.755 - ETA: 0s - loss: 0.5749 - acc: 0.747 - ETA: 0s - loss: 0.5762 - acc: 0.745 - ETA: 0s - loss: 0.5756 - acc: 0.746 - 0s 220us/step - loss: 0.5747 - acc: 0.7480 - val_loss: 0.5929 - val_acc: 0.7109\n",
      "Epoch 43/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5262 - acc: 0.843 - ETA: 0s - loss: 0.5818 - acc: 0.743 - ETA: 0s - loss: 0.5740 - acc: 0.742 - ETA: 0s - loss: 0.5729 - acc: 0.745 - ETA: 0s - loss: 0.5684 - acc: 0.751 - ETA: 0s - loss: 0.5772 - acc: 0.740 - 0s 223us/step - loss: 0.5720 - acc: 0.7467 - val_loss: 0.5906 - val_acc: 0.7135\n",
      "Epoch 44/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5835 - acc: 0.687 - ETA: 0s - loss: 0.5646 - acc: 0.741 - ETA: 0s - loss: 0.5746 - acc: 0.740 - ETA: 0s - loss: 0.5590 - acc: 0.756 - ETA: 0s - loss: 0.5586 - acc: 0.763 - ETA: 0s - loss: 0.5699 - acc: 0.749 - 0s 218us/step - loss: 0.5696 - acc: 0.7493 - val_loss: 0.5886 - val_acc: 0.7135\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5263 - acc: 0.812 - ETA: 0s - loss: 0.5803 - acc: 0.732 - ETA: 0s - loss: 0.5670 - acc: 0.772 - ETA: 0s - loss: 0.5705 - acc: 0.758 - ETA: 0s - loss: 0.5633 - acc: 0.767 - ETA: 0s - loss: 0.5668 - acc: 0.764 - 0s 220us/step - loss: 0.5692 - acc: 0.7593 - val_loss: 0.5866 - val_acc: 0.7188\n",
      "Epoch 46/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4699 - acc: 0.906 - ETA: 0s - loss: 0.5243 - acc: 0.803 - ETA: 0s - loss: 0.5515 - acc: 0.759 - ETA: 0s - loss: 0.5669 - acc: 0.759 - ETA: 0s - loss: 0.5680 - acc: 0.759 - ETA: 0s - loss: 0.5667 - acc: 0.758 - 0s 223us/step - loss: 0.5652 - acc: 0.7580 - val_loss: 0.5849 - val_acc: 0.7241\n",
      "Epoch 47/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6484 - acc: 0.656 - ETA: 0s - loss: 0.5788 - acc: 0.728 - ETA: 0s - loss: 0.5793 - acc: 0.751 - ETA: 0s - loss: 0.5726 - acc: 0.758 - ETA: 0s - loss: 0.5617 - acc: 0.761 - ETA: 0s - loss: 0.5611 - acc: 0.763 - 0s 222us/step - loss: 0.5607 - acc: 0.7613 - val_loss: 0.5834 - val_acc: 0.7215\n",
      "Epoch 48/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4454 - acc: 0.906 - ETA: 0s - loss: 0.5621 - acc: 0.711 - ETA: 0s - loss: 0.5581 - acc: 0.729 - ETA: 0s - loss: 0.5559 - acc: 0.752 - ETA: 0s - loss: 0.5554 - acc: 0.760 - ETA: 0s - loss: 0.5546 - acc: 0.760 - ETA: 0s - loss: 0.5619 - acc: 0.753 - 0s 239us/step - loss: 0.5626 - acc: 0.7527 - val_loss: 0.5819 - val_acc: 0.7241\n",
      "Epoch 49/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4771 - acc: 0.750 - ETA: 0s - loss: 0.5565 - acc: 0.756 - ETA: 0s - loss: 0.5513 - acc: 0.764 - ETA: 0s - loss: 0.5506 - acc: 0.767 - ETA: 0s - loss: 0.5498 - acc: 0.770 - ETA: 0s - loss: 0.5552 - acc: 0.767 - 0s 215us/step - loss: 0.5652 - acc: 0.7566 - val_loss: 0.5810 - val_acc: 0.7241\n",
      "Epoch 50/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4522 - acc: 0.875 - ETA: 0s - loss: 0.5573 - acc: 0.750 - ETA: 0s - loss: 0.5465 - acc: 0.760 - ETA: 0s - loss: 0.5505 - acc: 0.763 - ETA: 0s - loss: 0.5524 - acc: 0.761 - ETA: 0s - loss: 0.5581 - acc: 0.755 - 0s 218us/step - loss: 0.5586 - acc: 0.7580 - val_loss: 0.5793 - val_acc: 0.7480\n",
      "Epoch 51/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4767 - acc: 0.812 - ETA: 0s - loss: 0.5286 - acc: 0.784 - ETA: 0s - loss: 0.5400 - acc: 0.769 - ETA: 0s - loss: 0.5526 - acc: 0.760 - ETA: 0s - loss: 0.5503 - acc: 0.762 - ETA: 0s - loss: 0.5587 - acc: 0.750 - 0s 213us/step - loss: 0.5557 - acc: 0.7546 - val_loss: 0.5781 - val_acc: 0.7374\n",
      "Epoch 52/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5629 - acc: 0.812 - ETA: 0s - loss: 0.5237 - acc: 0.768 - ETA: 0s - loss: 0.5483 - acc: 0.750 - ETA: 0s - loss: 0.5532 - acc: 0.755 - ETA: 0s - loss: 0.5598 - acc: 0.746 - ETA: 0s - loss: 0.5561 - acc: 0.756 - 0s 225us/step - loss: 0.5544 - acc: 0.7566 - val_loss: 0.5772 - val_acc: 0.7401\n",
      "Epoch 53/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5592 - acc: 0.718 - ETA: 0s - loss: 0.5489 - acc: 0.753 - ETA: 0s - loss: 0.5342 - acc: 0.772 - ETA: 0s - loss: 0.5563 - acc: 0.748 - ETA: 0s - loss: 0.5522 - acc: 0.752 - ETA: 0s - loss: 0.5523 - acc: 0.756 - ETA: 0s - loss: 0.5536 - acc: 0.756 - 0s 245us/step - loss: 0.5526 - acc: 0.7593 - val_loss: 0.5760 - val_acc: 0.7427\n",
      "Epoch 54/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.7617 - acc: 0.562 - ETA: 0s - loss: 0.5975 - acc: 0.725 - ETA: 0s - loss: 0.5661 - acc: 0.753 - ETA: 0s - loss: 0.5651 - acc: 0.753 - ETA: 0s - loss: 0.5610 - acc: 0.746 - ETA: 0s - loss: 0.5620 - acc: 0.747 - 0s 221us/step - loss: 0.5508 - acc: 0.7566 - val_loss: 0.5751 - val_acc: 0.7401\n",
      "Epoch 55/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6026 - acc: 0.750 - ETA: 0s - loss: 0.5722 - acc: 0.765 - ETA: 0s - loss: 0.5660 - acc: 0.763 - ETA: 0s - loss: 0.5675 - acc: 0.752 - ETA: 0s - loss: 0.5519 - acc: 0.761 - ETA: 0s - loss: 0.5472 - acc: 0.761 - 0s 223us/step - loss: 0.5505 - acc: 0.7613 - val_loss: 0.5742 - val_acc: 0.7401\n",
      "Epoch 56/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.7128 - acc: 0.625 - ETA: 0s - loss: 0.5287 - acc: 0.762 - ETA: 0s - loss: 0.5445 - acc: 0.763 - ETA: 0s - loss: 0.5460 - acc: 0.756 - ETA: 0s - loss: 0.5510 - acc: 0.747 - ETA: 0s - loss: 0.5437 - acc: 0.750 - 0s 219us/step - loss: 0.5440 - acc: 0.7527 - val_loss: 0.5731 - val_acc: 0.7401\n",
      "Epoch 57/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4468 - acc: 0.875 - ETA: 0s - loss: 0.5305 - acc: 0.764 - ETA: 0s - loss: 0.5221 - acc: 0.771 - ETA: 0s - loss: 0.5446 - acc: 0.751 - ETA: 0s - loss: 0.5422 - acc: 0.753 - ETA: 0s - loss: 0.5422 - acc: 0.760 - 0s 218us/step - loss: 0.5441 - acc: 0.7626 - val_loss: 0.5724 - val_acc: 0.7427\n",
      "Epoch 58/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6146 - acc: 0.687 - ETA: 0s - loss: 0.5951 - acc: 0.696 - ETA: 0s - loss: 0.5510 - acc: 0.750 - ETA: 0s - loss: 0.5503 - acc: 0.745 - ETA: 0s - loss: 0.5536 - acc: 0.749 - ETA: 0s - loss: 0.5459 - acc: 0.756 - 0s 219us/step - loss: 0.5455 - acc: 0.7586 - val_loss: 0.5717 - val_acc: 0.7401\n",
      "Epoch 59/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5871 - acc: 0.687 - ETA: 0s - loss: 0.5452 - acc: 0.750 - ETA: 0s - loss: 0.5388 - acc: 0.769 - ETA: 0s - loss: 0.5427 - acc: 0.760 - ETA: 0s - loss: 0.5439 - acc: 0.761 - ETA: 0s - loss: 0.5376 - acc: 0.763 - 0s 220us/step - loss: 0.5385 - acc: 0.7606 - val_loss: 0.5715 - val_acc: 0.7374\n",
      "Epoch 60/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6034 - acc: 0.687 - ETA: 0s - loss: 0.5467 - acc: 0.743 - ETA: 0s - loss: 0.5290 - acc: 0.772 - ETA: 0s - loss: 0.5326 - acc: 0.770 - ETA: 0s - loss: 0.5401 - acc: 0.763 - ETA: 0s - loss: 0.5458 - acc: 0.757 - 0s 219us/step - loss: 0.5468 - acc: 0.7580 - val_loss: 0.5703 - val_acc: 0.7454\n",
      "Epoch 61/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5903 - acc: 0.593 - ETA: 0s - loss: 0.5206 - acc: 0.778 - ETA: 0s - loss: 0.5259 - acc: 0.770 - ETA: 0s - loss: 0.5429 - acc: 0.759 - ETA: 0s - loss: 0.5463 - acc: 0.756 - ETA: 0s - loss: 0.5507 - acc: 0.750 - 0s 220us/step - loss: 0.5471 - acc: 0.7546 - val_loss: 0.5699 - val_acc: 0.7480\n",
      "Epoch 62/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5567 - acc: 0.750 - ETA: 0s - loss: 0.5183 - acc: 0.781 - ETA: 0s - loss: 0.5119 - acc: 0.774 - ETA: 0s - loss: 0.5232 - acc: 0.766 - ETA: 0s - loss: 0.5262 - acc: 0.767 - ETA: 0s - loss: 0.5334 - acc: 0.764 - 0s 209us/step - loss: 0.5422 - acc: 0.7599 - val_loss: 0.5692 - val_acc: 0.7454\n",
      "Epoch 63/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5052 - acc: 0.781 - ETA: 0s - loss: 0.5564 - acc: 0.731 - ETA: 0s - loss: 0.5542 - acc: 0.748 - ETA: 0s - loss: 0.5418 - acc: 0.755 - ETA: 0s - loss: 0.5370 - acc: 0.766 - ETA: 0s - loss: 0.5437 - acc: 0.762 - 0s 209us/step - loss: 0.5436 - acc: 0.7599 - val_loss: 0.5687 - val_acc: 0.7454\n",
      "Epoch 64/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.3747 - acc: 0.875 - ETA: 0s - loss: 0.5296 - acc: 0.778 - ETA: 0s - loss: 0.5390 - acc: 0.770 - ETA: 0s - loss: 0.5277 - acc: 0.767 - ETA: 0s - loss: 0.5390 - acc: 0.759 - ETA: 0s - loss: 0.5389 - acc: 0.760 - 0s 209us/step - loss: 0.5395 - acc: 0.7593 - val_loss: 0.5680 - val_acc: 0.7401\n",
      "Epoch 65/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5535 - acc: 0.750 - ETA: 0s - loss: 0.5851 - acc: 0.721 - ETA: 0s - loss: 0.5677 - acc: 0.729 - ETA: 0s - loss: 0.5570 - acc: 0.734 - ETA: 0s - loss: 0.5504 - acc: 0.738 - ETA: 0s - loss: 0.5387 - acc: 0.750 - 0s 224us/step - loss: 0.5364 - acc: 0.7527 - val_loss: 0.5674 - val_acc: 0.7454\n",
      "Epoch 66/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4768 - acc: 0.843 - ETA: 0s - loss: 0.5253 - acc: 0.756 - ETA: 0s - loss: 0.5262 - acc: 0.769 - ETA: 0s - loss: 0.5237 - acc: 0.772 - ETA: 0s - loss: 0.5218 - acc: 0.773 - ETA: 0s - loss: 0.5299 - acc: 0.768 - 0s 218us/step - loss: 0.5379 - acc: 0.7619 - val_loss: 0.5672 - val_acc: 0.7480\n",
      "Epoch 67/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5690 - acc: 0.750 - ETA: 0s - loss: 0.5299 - acc: 0.784 - ETA: 0s - loss: 0.5389 - acc: 0.761 - ETA: 0s - loss: 0.5291 - acc: 0.766 - ETA: 0s - loss: 0.5266 - acc: 0.771 - ETA: 0s - loss: 0.5302 - acc: 0.764 - 0s 219us/step - loss: 0.5363 - acc: 0.7619 - val_loss: 0.5665 - val_acc: 0.7454\n",
      "Epoch 68/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5076 - acc: 0.781 - ETA: 0s - loss: 0.5167 - acc: 0.784 - ETA: 0s - loss: 0.5295 - acc: 0.767 - ETA: 0s - loss: 0.5339 - acc: 0.760 - ETA: 0s - loss: 0.5349 - acc: 0.758 - ETA: 0s - loss: 0.5321 - acc: 0.758 - 0s 221us/step - loss: 0.5371 - acc: 0.7560 - val_loss: 0.5663 - val_acc: 0.7401\n",
      "Epoch 69/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4713 - acc: 0.812 - ETA: 0s - loss: 0.5285 - acc: 0.777 - ETA: 0s - loss: 0.5403 - acc: 0.755 - ETA: 0s - loss: 0.5372 - acc: 0.757 - ETA: 0s - loss: 0.5377 - acc: 0.764 - ETA: 0s - loss: 0.5399 - acc: 0.759 - 0s 223us/step - loss: 0.5416 - acc: 0.7573 - val_loss: 0.5660 - val_acc: 0.7427\n",
      "Epoch 70/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5218 - acc: 0.812 - ETA: 0s - loss: 0.5165 - acc: 0.761 - ETA: 0s - loss: 0.5061 - acc: 0.770 - ETA: 0s - loss: 0.5214 - acc: 0.762 - ETA: 0s - loss: 0.5236 - acc: 0.763 - ETA: 0s - loss: 0.5301 - acc: 0.760 - 0s 213us/step - loss: 0.5317 - acc: 0.7593 - val_loss: 0.5655 - val_acc: 0.7454\n",
      "Epoch 71/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6806 - acc: 0.593 - ETA: 0s - loss: 0.5789 - acc: 0.704 - ETA: 0s - loss: 0.5631 - acc: 0.728 - ETA: 0s - loss: 0.5448 - acc: 0.751 - ETA: 0s - loss: 0.5367 - acc: 0.756 - ETA: 0s - loss: 0.5314 - acc: 0.763 - 0s 219us/step - loss: 0.5296 - acc: 0.7619 - val_loss: 0.5657 - val_acc: 0.7480\n",
      "Epoch 72/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4521 - acc: 0.812 - ETA: 0s - loss: 0.5128 - acc: 0.768 - ETA: 0s - loss: 0.5326 - acc: 0.760 - ETA: 0s - loss: 0.5481 - acc: 0.748 - ETA: 0s - loss: 0.5499 - acc: 0.742 - ETA: 0s - loss: 0.5403 - acc: 0.747 - 0s 216us/step - loss: 0.5377 - acc: 0.7513 - val_loss: 0.5650 - val_acc: 0.7401\n",
      "Epoch 73/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4567 - acc: 0.750 - ETA: 0s - loss: 0.5022 - acc: 0.775 - ETA: 0s - loss: 0.5250 - acc: 0.756 - ETA: 0s - loss: 0.5289 - acc: 0.754 - ETA: 0s - loss: 0.5403 - acc: 0.755 - ETA: 0s - loss: 0.5338 - acc: 0.760 - 0s 223us/step - loss: 0.5341 - acc: 0.7593 - val_loss: 0.5648 - val_acc: 0.7427\n",
      "Epoch 74/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.3784 - acc: 0.937 - ETA: 0s - loss: 0.5221 - acc: 0.778 - ETA: 0s - loss: 0.5253 - acc: 0.771 - ETA: 0s - loss: 0.5282 - acc: 0.773 - ETA: 0s - loss: 0.5282 - acc: 0.769 - ETA: 0s - loss: 0.5280 - acc: 0.762 - 0s 215us/step - loss: 0.5308 - acc: 0.7606 - val_loss: 0.5646 - val_acc: 0.7454\n",
      "Epoch 75/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4844 - acc: 0.781 - ETA: 0s - loss: 0.5356 - acc: 0.725 - ETA: 0s - loss: 0.5278 - acc: 0.759 - ETA: 0s - loss: 0.5316 - acc: 0.744 - ETA: 0s - loss: 0.5296 - acc: 0.756 - ETA: 0s - loss: 0.5320 - acc: 0.753 - 0s 227us/step - loss: 0.5312 - acc: 0.7566 - val_loss: 0.5644 - val_acc: 0.7401\n",
      "Epoch 76/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4653 - acc: 0.781 - ETA: 0s - loss: 0.5606 - acc: 0.743 - ETA: 0s - loss: 0.5301 - acc: 0.772 - ETA: 0s - loss: 0.5252 - acc: 0.772 - ETA: 0s - loss: 0.5266 - acc: 0.768 - ETA: 0s - loss: 0.5317 - acc: 0.758 - 0s 221us/step - loss: 0.5306 - acc: 0.7599 - val_loss: 0.5642 - val_acc: 0.7401\n",
      "Epoch 77/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5087 - acc: 0.718 - ETA: 0s - loss: 0.5331 - acc: 0.765 - ETA: 0s - loss: 0.5295 - acc: 0.770 - ETA: 0s - loss: 0.5349 - acc: 0.768 - ETA: 0s - loss: 0.5309 - acc: 0.777 - ETA: 0s - loss: 0.5362 - acc: 0.761 - 0s 222us/step - loss: 0.5322 - acc: 0.7639 - val_loss: 0.5640 - val_acc: 0.7401\n",
      "Epoch 78/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4697 - acc: 0.781 - ETA: 0s - loss: 0.5614 - acc: 0.737 - ETA: 0s - loss: 0.5627 - acc: 0.735 - ETA: 0s - loss: 0.5494 - acc: 0.739 - ETA: 0s - loss: 0.5355 - acc: 0.753 - ETA: 0s - loss: 0.5368 - acc: 0.750 - 0s 206us/step - loss: 0.5317 - acc: 0.7540 - val_loss: 0.5640 - val_acc: 0.7427\n",
      "Epoch 79/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5606 - acc: 0.656 - ETA: 0s - loss: 0.5331 - acc: 0.763 - ETA: 0s - loss: 0.5253 - acc: 0.769 - ETA: 0s - loss: 0.5206 - acc: 0.767 - ETA: 0s - loss: 0.5269 - acc: 0.764 - ETA: 0s - loss: 0.5269 - acc: 0.760 - 0s 204us/step - loss: 0.5273 - acc: 0.7599 - val_loss: 0.5641 - val_acc: 0.7427\n",
      "Epoch 80/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4698 - acc: 0.750 - ETA: 0s - loss: 0.5539 - acc: 0.718 - ETA: 0s - loss: 0.5422 - acc: 0.739 - ETA: 0s - loss: 0.5396 - acc: 0.736 - ETA: 0s - loss: 0.5287 - acc: 0.750 - ETA: 0s - loss: 0.5243 - acc: 0.756 - 0s 220us/step - loss: 0.5232 - acc: 0.7573 - val_loss: 0.5636 - val_acc: 0.7480\n",
      "Epoch 81/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5966 - acc: 0.750 - ETA: 0s - loss: 0.5198 - acc: 0.771 - ETA: 0s - loss: 0.5256 - acc: 0.753 - ETA: 0s - loss: 0.5284 - acc: 0.763 - ETA: 0s - loss: 0.5285 - acc: 0.761 - ETA: 0s - loss: 0.5335 - acc: 0.757 - 0s 219us/step - loss: 0.5295 - acc: 0.7606 - val_loss: 0.5635 - val_acc: 0.7427\n",
      "Epoch 82/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6424 - acc: 0.656 - ETA: 0s - loss: 0.5336 - acc: 0.762 - ETA: 0s - loss: 0.5151 - acc: 0.769 - ETA: 0s - loss: 0.5010 - acc: 0.787 - ETA: 0s - loss: 0.5187 - acc: 0.775 - ETA: 0s - loss: 0.5266 - acc: 0.766 - ETA: 0s - loss: 0.5264 - acc: 0.762 - 0s 236us/step - loss: 0.5270 - acc: 0.7626 - val_loss: 0.5632 - val_acc: 0.7454\n",
      "Epoch 83/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4564 - acc: 0.750 - ETA: 0s - loss: 0.4962 - acc: 0.781 - ETA: 0s - loss: 0.5410 - acc: 0.746 - ETA: 0s - loss: 0.5324 - acc: 0.763 - ETA: 0s - loss: 0.5344 - acc: 0.758 - ETA: 0s - loss: 0.5322 - acc: 0.758 - 0s 217us/step - loss: 0.5303 - acc: 0.7593 - val_loss: 0.5630 - val_acc: 0.7427\n",
      "Epoch 84/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5547 - acc: 0.656 - ETA: 0s - loss: 0.5201 - acc: 0.739 - ETA: 0s - loss: 0.5211 - acc: 0.759 - ETA: 0s - loss: 0.5148 - acc: 0.770 - ETA: 0s - loss: 0.5153 - acc: 0.767 - ETA: 0s - loss: 0.5183 - acc: 0.766 - 0s 221us/step - loss: 0.5250 - acc: 0.7606 - val_loss: 0.5627 - val_acc: 0.7507\n",
      "Epoch 85/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4944 - acc: 0.812 - ETA: 0s - loss: 0.5495 - acc: 0.725 - ETA: 0s - loss: 0.5435 - acc: 0.734 - ETA: 0s - loss: 0.5258 - acc: 0.747 - ETA: 0s - loss: 0.5221 - acc: 0.754 - ETA: 0s - loss: 0.5154 - acc: 0.757 - 0s 218us/step - loss: 0.5195 - acc: 0.7573 - val_loss: 0.5631 - val_acc: 0.7454\n",
      "Epoch 86/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4944 - acc: 0.718 - ETA: 0s - loss: 0.5234 - acc: 0.753 - ETA: 0s - loss: 0.5100 - acc: 0.779 - ETA: 0s - loss: 0.5105 - acc: 0.771 - ETA: 0s - loss: 0.5197 - acc: 0.762 - ETA: 0s - loss: 0.5206 - acc: 0.760 - 0s 225us/step - loss: 0.5248 - acc: 0.7566 - val_loss: 0.5627 - val_acc: 0.7401\n",
      "Epoch 87/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5187 - acc: 0.843 - ETA: 0s - loss: 0.4986 - acc: 0.818 - ETA: 0s - loss: 0.5235 - acc: 0.786 - ETA: 0s - loss: 0.5193 - acc: 0.781 - ETA: 0s - loss: 0.5241 - acc: 0.772 - ETA: 0s - loss: 0.5333 - acc: 0.764 - 0s 220us/step - loss: 0.5276 - acc: 0.7666 - val_loss: 0.5626 - val_acc: 0.7321\n",
      "Epoch 88/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6079 - acc: 0.687 - ETA: 0s - loss: 0.5063 - acc: 0.783 - ETA: 0s - loss: 0.5184 - acc: 0.762 - ETA: 0s - loss: 0.5247 - acc: 0.755 - 0s 137us/step - loss: 0.5267 - acc: 0.7566 - val_loss: 0.5624 - val_acc: 0.7321\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4949 - acc: 0.781 - ETA: 0s - loss: 0.5256 - acc: 0.776 - ETA: 0s - loss: 0.5244 - acc: 0.772 - ETA: 0s - loss: 0.5184 - acc: 0.775 - ETA: 0s - loss: 0.5193 - acc: 0.768 - 0s 169us/step - loss: 0.5211 - acc: 0.7653 - val_loss: 0.5625 - val_acc: 0.7401\n",
      "Epoch 90/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5580 - acc: 0.843 - ETA: 0s - loss: 0.5048 - acc: 0.775 - ETA: 0s - loss: 0.5135 - acc: 0.767 - ETA: 0s - loss: 0.5152 - acc: 0.764 - ETA: 0s - loss: 0.5284 - acc: 0.755 - ETA: 0s - loss: 0.5271 - acc: 0.754 - 0s 218us/step - loss: 0.5242 - acc: 0.7566 - val_loss: 0.5623 - val_acc: 0.7321\n",
      "Epoch 91/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4420 - acc: 0.843 - ETA: 0s - loss: 0.5125 - acc: 0.771 - ETA: 0s - loss: 0.5246 - acc: 0.753 - ETA: 0s - loss: 0.5121 - acc: 0.770 - ETA: 0s - loss: 0.5054 - acc: 0.774 - ETA: 0s - loss: 0.5192 - acc: 0.762 - 0s 214us/step - loss: 0.5173 - acc: 0.7646 - val_loss: 0.5624 - val_acc: 0.7347\n",
      "Epoch 92/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.6293 - acc: 0.781 - ETA: 0s - loss: 0.5571 - acc: 0.729 - ETA: 0s - loss: 0.5349 - acc: 0.732 - ETA: 0s - loss: 0.5225 - acc: 0.743 - ETA: 0s - loss: 0.5215 - acc: 0.753 - ETA: 0s - loss: 0.5264 - acc: 0.754 - 0s 226us/step - loss: 0.5261 - acc: 0.7546 - val_loss: 0.5623 - val_acc: 0.7321\n",
      "Epoch 93/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4653 - acc: 0.812 - ETA: 0s - loss: 0.5074 - acc: 0.765 - ETA: 0s - loss: 0.5245 - acc: 0.753 - ETA: 0s - loss: 0.5214 - acc: 0.758 - ETA: 0s - loss: 0.5245 - acc: 0.760 - ETA: 0s - loss: 0.5210 - acc: 0.758 - 0s 220us/step - loss: 0.5218 - acc: 0.7580 - val_loss: 0.5623 - val_acc: 0.7321\n",
      "Epoch 94/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4790 - acc: 0.843 - ETA: 0s - loss: 0.5041 - acc: 0.767 - ETA: 0s - loss: 0.5044 - acc: 0.772 - ETA: 0s - loss: 0.5180 - acc: 0.760 - ETA: 0s - loss: 0.5246 - acc: 0.758 - ETA: 0s - loss: 0.5229 - acc: 0.760 - 0s 220us/step - loss: 0.5218 - acc: 0.7580 - val_loss: 0.5622 - val_acc: 0.7374\n",
      "Epoch 95/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5542 - acc: 0.718 - ETA: 0s - loss: 0.5399 - acc: 0.762 - ETA: 0s - loss: 0.5318 - acc: 0.746 - ETA: 0s - loss: 0.5232 - acc: 0.757 - ETA: 0s - loss: 0.5202 - acc: 0.759 - ETA: 0s - loss: 0.5207 - acc: 0.758 - 0s 213us/step - loss: 0.5170 - acc: 0.7606 - val_loss: 0.5625 - val_acc: 0.7321\n",
      "Epoch 96/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4694 - acc: 0.843 - ETA: 0s - loss: 0.4844 - acc: 0.798 - ETA: 0s - loss: 0.4913 - acc: 0.770 - ETA: 0s - loss: 0.4930 - acc: 0.775 - ETA: 0s - loss: 0.4954 - acc: 0.776 - ETA: 0s - loss: 0.5086 - acc: 0.763 - 0s 223us/step - loss: 0.5157 - acc: 0.7553 - val_loss: 0.5625 - val_acc: 0.7321\n",
      "Epoch 97/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.3791 - acc: 0.843 - ETA: 0s - loss: 0.4758 - acc: 0.787 - ETA: 0s - loss: 0.4787 - acc: 0.770 - ETA: 0s - loss: 0.4972 - acc: 0.761 - ETA: 0s - loss: 0.5102 - acc: 0.752 - ETA: 0s - loss: 0.5167 - acc: 0.747 - 0s 222us/step - loss: 0.5163 - acc: 0.7493 - val_loss: 0.5623 - val_acc: 0.7294\n",
      "Epoch 98/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.5287 - acc: 0.781 - ETA: 0s - loss: 0.5073 - acc: 0.775 - ETA: 0s - loss: 0.5250 - acc: 0.758 - ETA: 0s - loss: 0.5208 - acc: 0.753 - ETA: 0s - loss: 0.5196 - acc: 0.750 - ETA: 0s - loss: 0.5141 - acc: 0.755 - 0s 215us/step - loss: 0.5133 - acc: 0.7586 - val_loss: 0.5633 - val_acc: 0.7454\n",
      "Epoch 99/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4412 - acc: 0.843 - ETA: 0s - loss: 0.5325 - acc: 0.753 - ETA: 0s - loss: 0.5258 - acc: 0.750 - ETA: 0s - loss: 0.5234 - acc: 0.751 - ETA: 0s - loss: 0.5264 - acc: 0.745 - ETA: 0s - loss: 0.5169 - acc: 0.754 - 0s 219us/step - loss: 0.5150 - acc: 0.7593 - val_loss: 0.5624 - val_acc: 0.7294\n",
      "Epoch 100/100\n",
      "1508/1508 [==============================] - ETA: 0s - loss: 0.4418 - acc: 0.843 - ETA: 0s - loss: 0.5292 - acc: 0.767 - ETA: 0s - loss: 0.4937 - acc: 0.779 - ETA: 0s - loss: 0.5043 - acc: 0.772 - ETA: 0s - loss: 0.5117 - acc: 0.769 - ETA: 0s - loss: 0.5120 - acc: 0.764 - 0s 223us/step - loss: 0.5166 - acc: 0.7586 - val_loss: 0.5623 - val_acc: 0.7321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d6a5498b70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 100\n",
    "hidden_size = 10\n",
    "timesteps = 1\n",
    "num_class = 1\n",
    "data_dim = len(train_vecs_physics[0])\n",
    "num_data = len(train_vecs_physics)\n",
    "num_data_test = len(test_vecs_physics)\n",
    "\n",
    "train_vecs_physics = train_vecs_physics.reshape((num_data, timesteps, data_dim))\n",
    "y_train_physics = y_train_physics.reshape((num_data, num_class))\n",
    "test_vecs_physics = test_vecs_physics.reshape((num_data_test, timesteps, data_dim))\n",
    "y_test_physics = y_test_physics.reshape((num_data_test, num_class))\n",
    "\n",
    "model_hd_physics = Sequential()\n",
    "model_hd_physics.add(Bidirectional(LSTM(hidden_size, input_shape=(timesteps, data_dim)), merge_mode='ave'))\n",
    "model_hd_physics.add(Dropout(0.5))\n",
    "model_hd_physics.add(Dense(1, activation='sigmoid'))\n",
    "model_hd_physics.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_hd_physics.fit(train_vecs_physics, y_train_physics, epochs=num_epochs, validation_data=[test_vecs_physics, y_test_physics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7320954907161804 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.54      0.62       156\n",
      "          1       0.73      0.87      0.79       221\n",
      "\n",
      "avg / total       0.73      0.73      0.72       377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "prediction = {}\n",
    "prediction['hd_physics'] = model_hd_physics.predict(test_vecs_physics)\n",
    "\n",
    "for i in range(len(prediction['hd_physics'])):\n",
    "    prediction['hd_physics'][i][0] = round(prediction['hd_physics'][i][0])\n",
    "\n",
    "accuracy = {}\n",
    "accuracy['hd_physics'] = accuracy_score(y_test_physics, prediction['hd_physics'])\n",
    "print(\"Accuracy: \", accuracy['hd_physics'], \"\\n\")\n",
    "print(classification_report(y_test_physics, prediction['hd_physics'], labels = [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Hate Detection Model for Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8214285714285714 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.53      0.64        17\n",
      "          1       0.82      0.95      0.88        39\n",
      "\n",
      "avg / total       0.82      0.82      0.81        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = {}\n",
    "accuracy = {}\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model_hd_race = MLPClassifier(hidden_layer_sizes=(10,10), alpha=0.005, activation='identity', learning_rate='adaptive', learning_rate_init=0.05, solver='adam')\n",
    "model_hd_race.fit(train_vecs_race, y_train_race)\n",
    "\n",
    "prediction['hd_race'] = model_hd_race.predict(test_vecs_race)\n",
    "accuracy['hd_race'] = accuracy_score(y_test_race, prediction['hd_race'])\n",
    "print(\"Accuracy: \", accuracy['hd_race'], \"\\n\")\n",
    "print(classification_report(y_test_race, prediction['hd_race'], labels = [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model_hd_race, 'mlp_hd_race.model')\n",
    "model_hd_race = joblib.load('mlp_hd_race.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Hate Detection Model for Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.927536231884058 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.91      0.92        33\n",
      "          1       0.92      0.94      0.93        36\n",
      "\n",
      "avg / total       0.93      0.93      0.93        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = {}\n",
    "accuracy = {}\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model_hd_religion = MLPClassifier(hidden_layer_sizes=(10,10), alpha=0.005, activation='identity', learning_rate='adaptive', learning_rate_init=0.05, solver='adam')\n",
    "model_hd_religion.fit(train_vecs_religion, y_train_religion)\n",
    "\n",
    "prediction['hd_religion'] = model_hd_religion.predict(test_vecs_religion)\n",
    "accuracy['hd_religion'] = accuracy_score(y_test_religion, prediction['hd_religion'])\n",
    "print(\"Accuracy: \", accuracy['hd_religion'], \"\\n\")\n",
    "print(classification_report(y_test_religion, prediction['hd_religion'], labels = [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlp_hd_religion.model']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import models\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "model_hd_physics.save('bilstm_hd_physics.model')\n",
    "joblib.dump(model_hd_race, 'mlp_hd_race.model')\n",
    "joblib.dump(model_hd_religion, 'mlp_hd_religion.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "model_hd_physics = models.load_model('bilstm_hd_physics.model')\n",
    "model_hd_race = joblib.load('mlp_hd_race.model')\n",
    "model_hd_religion = joblib.load('mlp_hd_religion.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Detection Model for Predict New Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1:hate, 0:no hate\n",
    "\n",
    "def predict_aspect(text):\n",
    "    text = normalizer(text)\n",
    "    tokens = nltk.WhitespaceTokenizer().tokenize(text)\n",
    "    vecs = build_Word_Vector(tokens, 200)\n",
    "    vecs_reshape = vecs.reshape((1, 1, 200))\n",
    "    aspect = {}\n",
    "    aspect['physics'] = int(round(model_hd_physics.predict(vecs_reshape)[0][0]))\n",
    "    aspect['race'] = int(round(model_hd_race.predict(vecs)[0]))\n",
    "    aspect['religion'] = int(round(model_hd_religion.predict(vecs)[0]))\n",
    "    return aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'physics': 1, 'race': 0, 'religion': 0}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_aspect(\"ugly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
